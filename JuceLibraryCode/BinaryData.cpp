/* ==================================== JUCER_BINARY_RESOURCE ====================================

   This is an auto-generated file: Any edits you make may be overwritten!

*/

#include <cstring>

namespace BinaryData
{

//================== TrsmUnrolls.inc ==================
static const unsigned char temp_binary_data_0[] =
{ 47,47,32,84,104,105,115,32,102,105,108,101,32,105,115,32,112,97,114,116,32,111,102,32,69,105,103,101,110,44,32,97,32,108,105,103,104,116,119,101,105,103,104,116,32,67,43,43,32,116,101,109,112,108,97,116,101,32,108,105,98,114,97,114,121,10,47,47,32,102,
111,114,32,108,105,110,101,97,114,32,97,108,103,101,98,114,97,46,10,47,47,10,47,47,32,67,111,112,121,114,105,103,104,116,32,40,67,41,32,50,48,50,50,32,73,110,116,101,108,32,67,111,114,112,111,114,97,116,105,111,110,10,47,47,10,47,47,32,84,104,105,115,
32,83,111,117,114,99,101,32,67,111,100,101,32,70,111,114,109,32,105,115,32,115,117,98,106,101,99,116,32,116,111,32,116,104,101,32,116,101,114,109,115,32,111,102,32,116,104,101,32,77,111,122,105,108,108,97,10,47,47,32,80,117,98,108,105,99,32,76,105,99,
101,110,115,101,32,118,46,32,50,46,48,46,32,73,102,32,97,32,99,111,112,121,32,111,102,32,116,104,101,32,77,80,76,32,119,97,115,32,110,111,116,32,100,105,115,116,114,105,98,117,116,101,100,10,47,47,32,119,105,116,104,32,116,104,105,115,32,102,105,108,
101,44,32,89,111,117,32,99,97,110,32,111,98,116,97,105,110,32,111,110,101,32,97,116,32,104,116,116,112,58,47,47,109,111,122,105,108,108,97,46,111,114,103,47,77,80,76,47,50,46,48,47,46,10,10,35,105,102,110,100,101,102,32,69,73,71,69,78,95,67,79,82,69,
95,65,82,67,72,95,65,86,88,53,49,50,95,84,82,83,77,95,85,78,82,79,76,76,83,95,72,10,35,100,101,102,105,110,101,32,69,73,71,69,78,95,67,79,82,69,95,65,82,67,72,95,65,86,88,53,49,50,95,84,82,83,77,95,85,78,82,79,76,76,83,95,72,10,10,116,101,109,112,108,
97,116,101,32,60,98,111,111,108,32,105,115,65,82,111,119,77,97,106,111,114,32,61,32,116,114,117,101,62,10,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,105,110,116,54,52,95,116,32,105,100,65,40,105,110,116,54,52,95,116,32,105,44,32,105,
110,116,54,52,95,116,32,106,44,32,105,110,116,54,52,95,116,32,76,68,65,41,32,123,10,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,105,115,65,82,111,119,77,97,106,111,114,41,32,114,101,116,117,114,110,32,105,32,42,32,76,68,65,32,43,32,
106,59,10,32,32,101,108,115,101,32,114,101,116,117,114,110,32,105,32,43,32,106,32,42,32,76,68,65,59,10,125,10,10,47,42,42,10,32,42,32,84,104,105,115,32,110,97,109,101,115,112,97,99,101,32,99,111,110,116,97,105,110,115,32,118,97,114,105,111,117,115,32,
99,108,97,115,115,101,115,32,117,115,101,100,32,116,111,32,103,101,110,101,114,97,116,101,32,99,111,109,112,105,108,101,45,116,105,109,101,32,117,110,114,111,108,108,115,32,119,104,105,99,104,32,97,114,101,10,32,42,32,117,115,101,100,32,116,104,114,111,
117,103,104,111,117,116,32,116,104,101,32,116,114,115,109,47,103,101,109,109,32,107,101,114,110,101,108,115,46,32,84,104,101,32,117,110,114,111,108,108,115,32,97,114,101,32,99,104,97,114,97,99,116,101,114,105,122,101,100,32,97,115,32,102,111,114,45,108,
111,111,112,115,32,40,49,45,68,41,44,32,110,101,115,116,101,100,10,32,42,32,102,111,114,45,108,111,111,112,115,32,40,50,45,68,41,44,32,111,114,32,116,114,105,112,108,101,32,110,101,115,116,101,100,32,102,111,114,45,108,111,111,112,115,32,40,51,45,68,
41,46,32,85,110,114,111,108,108,115,32,97,114,101,32,103,101,110,101,114,97,116,101,100,32,117,115,105,110,103,32,116,101,109,112,108,97,116,101,32,114,101,99,117,114,115,105,111,110,10,32,42,10,32,42,32,69,120,97,109,112,108,101,44,32,116,104,101,32,
50,45,68,32,102,111,114,45,108,111,111,112,32,105,115,32,117,110,114,111,108,108,101,100,32,114,101,99,117,114,115,105,118,101,108,121,32,98,121,32,102,105,114,115,116,32,102,108,97,116,116,101,110,105,110,103,32,116,111,32,97,32,49,45,68,32,108,111,
111,112,46,10,32,42,10,32,42,32,102,111,114,40,115,116,97,114,116,73,32,61,32,48,59,32,115,116,97,114,116,73,32,60,32,101,110,100,73,59,32,115,116,97,114,116,73,43,43,41,32,32,32,32,32,32,32,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,67,32,61,
32,48,59,32,115,116,97,114,116,67,32,60,32,101,110,100,73,42,101,110,100,74,59,32,115,116,97,114,116,67,43,43,41,10,32,42,32,32,32,102,111,114,40,115,116,97,114,116,74,32,61,32,48,59,32,115,116,97,114,116,74,32,60,32,101,110,100,74,59,32,115,116,97,114,
116,74,43,43,41,32,32,45,45,45,45,62,32,32,32,32,32,32,115,116,97,114,116,73,32,61,32,40,115,116,97,114,116,67,41,47,40,101,110,100,74,41,10,32,42,32,32,32,32,32,102,117,110,99,40,115,116,97,114,116,73,44,115,116,97,114,116,74,41,32,32,32,32,32,32,32,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,115,116,97,114,116,74,32,61,32,40,115,116,97,114,116,67,41,37,40,101,110,100,74,41,10,32,42,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,102,117,110,99,40,46,46,46,41,10,32,42,10,32,42,32,84,104,101,32,49,45,68,32,108,111,111,112,32,99,97,110,32,98,101,32,117,110,114,111,108,108,101,100,32,114,101,99,117,
114,115,105,118,101,108,121,32,98,121,32,117,115,105,110,103,32,101,110,97,98,108,101,95,105,102,32,97,110,100,32,100,101,102,105,110,105,110,103,32,97,110,32,97,117,120,105,108,108,97,114,121,32,102,117,110,99,116,105,111,110,10,32,42,32,119,105,116,
104,32,97,32,116,101,109,112,108,97,116,101,32,112,97,114,97,109,101,116,101,114,32,117,115,101,100,32,97,115,32,97,32,99,111,117,110,116,101,114,46,10,32,42,10,32,42,32,116,101,109,112,108,97,116,101,32,60,101,110,100,73,44,32,101,110,100,74,44,32,99,
111,117,110,116,101,114,62,10,32,42,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,60,61,32,48,41,62,32,32,60,45,45,45,45,32,116,97,105,108,32,99,97,115,101,46,10,32,42,32,97,117,120,95,102,117,110,99,
32,123,125,10,32,42,10,32,42,32,116,101,109,112,108,97,116,101,32,60,101,110,100,73,44,32,101,110,100,74,44,32,99,111,117,110,116,101,114,62,10,32,42,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,62,
32,48,41,62,32,32,32,60,45,45,45,45,32,97,99,116,117,97,108,32,102,111,114,45,108,111,111,112,10,32,42,32,97,117,120,95,102,117,110,99,32,123,10,32,42,32,32,32,115,116,97,114,116,67,32,61,32,101,110,100,73,42,101,110,100,74,32,45,32,99,111,117,110,116,
101,114,10,32,42,32,32,32,115,116,97,114,116,73,32,61,32,40,115,116,97,114,116,67,41,47,40,101,110,100,74,41,10,32,42,32,32,32,115,116,97,114,116,74,32,61,32,40,115,116,97,114,116,67,41,37,40,101,110,100,74,41,10,32,42,32,32,32,102,117,110,99,40,115,
116,97,114,116,73,44,32,115,116,97,114,116,74,41,10,32,42,32,32,32,97,117,120,95,102,117,110,99,60,101,110,100,73,44,32,101,110,100,74,44,32,99,111,117,110,116,101,114,45,49,62,40,41,10,32,42,32,125,10,32,42,10,32,42,32,78,111,116,101,58,32,65,100,100,
105,116,105,111,110,97,108,32,119,114,97,112,112,101,114,32,102,117,110,99,116,105,111,110,115,32,97,114,101,32,112,114,111,118,105,100,101,100,32,102,111,114,32,97,117,120,95,102,117,110,99,32,119,104,105,99,104,32,104,105,100,101,115,32,116,104,101,
32,99,111,117,110,116,101,114,32,116,101,109,112,108,97,116,101,10,32,42,32,112,97,114,97,109,101,116,101,114,32,115,105,110,99,101,32,99,111,117,110,116,101,114,32,117,115,117,97,108,108,121,32,100,101,112,101,110,100,115,32,111,110,32,101,110,100,73,
44,32,101,110,100,74,44,32,101,116,99,46,46,46,10,32,42,10,32,42,32,67,111,110,118,101,110,116,105,111,110,115,58,10,32,42,32,49,41,32,101,110,100,88,58,32,115,112,101,99,105,102,105,101,115,32,116,104,101,32,116,101,114,109,105,110,97,108,32,118,97,
108,117,101,32,102,111,114,32,116,104,101,32,102,111,114,45,108,111,111,112,44,32,40,101,120,58,32,102,111,114,40,115,116,97,114,116,88,32,61,32,48,59,32,115,116,97,114,116,88,32,60,32,101,110,100,88,59,32,115,116,97,114,116,88,43,43,41,41,10,32,42,10,
32,42,32,50,41,32,114,101,109,44,32,114,101,109,77,44,32,114,101,109,75,32,116,101,109,112,108,97,116,101,32,112,97,114,97,109,101,116,101,114,115,32,97,114,101,32,117,115,101,100,32,102,111,114,32,100,101,99,105,100,105,110,103,32,119,104,101,116,104,
101,114,32,116,111,32,117,115,101,32,109,97,115,107,101,100,32,111,112,101,114,97,116,105,111,110,115,32,102,111,114,10,32,42,32,32,32,32,104,97,110,100,108,105,110,103,32,114,101,109,97,105,110,105,110,103,32,116,97,105,108,115,32,40,119,104,101,110,
32,115,105,122,101,115,32,97,114,101,32,110,111,116,32,109,117,108,116,105,112,108,101,115,32,111,102,32,80,97,99,107,101,116,83,105,122,101,32,111,114,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,41,10,32,42,47,10,110,97,109,101,
115,112,97,99,101,32,117,110,114,111,108,108,115,32,123,10,10,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,78,62,10,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,97,117,116,111,32,114,101,109,77,97,115,107,40,105,110,
116,54,52,95,116,32,109,41,32,123,10,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,78,32,61,61,32,49,54,41,32,123,32,114,101,116,117,114,110,32,48,120,70,70,70,70,32,62,62,32,40,49,54,32,45,32,109,41,59,32,125,10,32,32,101,108,115,101,
32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,78,32,61,61,32,56,41,32,123,10,32,32,32,32,114,101,116,117,114,110,32,48,120,70,70,32,62,62,32,40,56,32,45,32,109,41,59,10,32,32,125,10,32,32,101,108,115,101,32,69,73,71,69,78,95,73,70,95,67,
79,78,83,84,69,88,80,82,40,78,32,61,61,32,52,41,32,123,10,32,32,32,32,114,101,116,117,114,110,32,48,120,48,70,32,62,62,32,40,52,32,45,32,109,41,59,10,32,32,125,10,32,32,114,101,116,117,114,110,32,48,59,10,125,10,10,116,101,109,112,108,97,116,101,32,60,
116,121,112,101,110,97,109,101,32,80,97,99,107,101,116,62,10,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,105,100,32,116,114,97,110,115,56,120,56,98,108,111,99,107,115,40,80,97,99,107,101,116,66,108,111,99,107,60,80,97,99,107,101,
116,44,32,56,62,32,38,107,101,114,110,101,108,41,59,10,10,116,101,109,112,108,97,116,101,32,60,62,10,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,105,100,32,116,114,97,110,115,56,120,56,98,108,111,99,107,115,40,80,97,99,107,101,
116,66,108,111,99,107,60,80,97,99,107,101,116,49,54,102,44,32,56,62,32,38,107,101,114,110,101,108,41,32,123,10,32,32,95,95,109,53,49,50,32,84,48,32,61,32,95,109,109,53,49,50,95,117,110,112,97,99,107,108,111,95,112,115,40,107,101,114,110,101,108,46,112,
97,99,107,101,116,91,48,93,44,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,49,93,41,59,10,32,32,95,95,109,53,49,50,32,84,49,32,61,32,95,109,109,53,49,50,95,117,110,112,97,99,107,104,105,95,112,115,40,107,101,114,110,101,108,46,112,97,99,107,
101,116,91,48,93,44,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,49,93,41,59,10,32,32,95,95,109,53,49,50,32,84,50,32,61,32,95,109,109,53,49,50,95,117,110,112,97,99,107,108,111,95,112,115,40,107,101,114,110,101,108,46,112,97,99,107,101,116,91,
50,93,44,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,51,93,41,59,10,32,32,95,95,109,53,49,50,32,84,51,32,61,32,95,109,109,53,49,50,95,117,110,112,97,99,107,104,105,95,112,115,40,107,101,114,110,101,108,46,112,97,99,107,101,116,91,50,93,44,
32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,51,93,41,59,10,32,32,95,95,109,53,49,50,32,84,52,32,61,32,95,109,109,53,49,50,95,117,110,112,97,99,107,108,111,95,112,115,40,107,101,114,110,101,108,46,112,97,99,107,101,116,91,52,93,44,32,107,101,
114,110,101,108,46,112,97,99,107,101,116,91,53,93,41,59,10,32,32,95,95,109,53,49,50,32,84,53,32,61,32,95,109,109,53,49,50,95,117,110,112,97,99,107,104,105,95,112,115,40,107,101,114,110,101,108,46,112,97,99,107,101,116,91,52,93,44,32,107,101,114,110,101,
108,46,112,97,99,107,101,116,91,53,93,41,59,10,32,32,95,95,109,53,49,50,32,84,54,32,61,32,95,109,109,53,49,50,95,117,110,112,97,99,107,108,111,95,112,115,40,107,101,114,110,101,108,46,112,97,99,107,101,116,91,54,93,44,32,107,101,114,110,101,108,46,112,
97,99,107,101,116,91,55,93,41,59,10,32,32,95,95,109,53,49,50,32,84,55,32,61,32,95,109,109,53,49,50,95,117,110,112,97,99,107,104,105,95,112,115,40,107,101,114,110,101,108,46,112,97,99,107,101,116,91,54,93,44,32,107,101,114,110,101,108,46,112,97,99,107,
101,116,91,55,93,41,59,10,10,32,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,48,93,32,61,32,95,109,109,53,49,50,95,99,97,115,116,112,100,95,112,115,40,95,109,109,53,49,50,95,117,110,112,97,99,107,108,111,95,112,100,40,95,109,109,53,49,50,95,
99,97,115,116,112,115,95,112,100,40,84,48,41,44,32,95,109,109,53,49,50,95,99,97,115,116,112,115,95,112,100,40,84,50,41,41,41,59,10,32,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,49,93,32,61,32,95,109,109,53,49,50,95,99,97,115,116,112,100,95,
112,115,40,95,109,109,53,49,50,95,117,110,112,97,99,107,104,105,95,112,100,40,95,109,109,53,49,50,95,99,97,115,116,112,115,95,112,100,40,84,48,41,44,32,95,109,109,53,49,50,95,99,97,115,116,112,115,95,112,100,40,84,50,41,41,41,59,10,32,32,107,101,114,
110,101,108,46,112,97,99,107,101,116,91,50,93,32,61,32,95,109,109,53,49,50,95,99,97,115,116,112,100,95,112,115,40,95,109,109,53,49,50,95,117,110,112,97,99,107,108,111,95,112,100,40,95,109,109,53,49,50,95,99,97,115,116,112,115,95,112,100,40,84,49,41,44,
32,95,109,109,53,49,50,95,99,97,115,116,112,115,95,112,100,40,84,51,41,41,41,59,10,32,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,51,93,32,61,32,95,109,109,53,49,50,95,99,97,115,116,112,100,95,112,115,40,95,109,109,53,49,50,95,117,110,112,
97,99,107,104,105,95,112,100,40,95,109,109,53,49,50,95,99,97,115,116,112,115,95,112,100,40,84,49,41,44,32,95,109,109,53,49,50,95,99,97,115,116,112,115,95,112,100,40,84,51,41,41,41,59,10,32,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,52,93,
32,61,32,95,109,109,53,49,50,95,99,97,115,116,112,100,95,112,115,40,95,109,109,53,49,50,95,117,110,112,97,99,107,108,111,95,112,100,40,95,109,109,53,49,50,95,99,97,115,116,112,115,95,112,100,40,84,52,41,44,32,95,109,109,53,49,50,95,99,97,115,116,112,
115,95,112,100,40,84,54,41,41,41,59,10,32,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,53,93,32,61,32,95,109,109,53,49,50,95,99,97,115,116,112,100,95,112,115,40,95,109,109,53,49,50,95,117,110,112,97,99,107,104,105,95,112,100,40,95,109,109,53,
49,50,95,99,97,115,116,112,115,95,112,100,40,84,52,41,44,32,95,109,109,53,49,50,95,99,97,115,116,112,115,95,112,100,40,84,54,41,41,41,59,10,32,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,54,93,32,61,32,95,109,109,53,49,50,95,99,97,115,116,
112,100,95,112,115,40,95,109,109,53,49,50,95,117,110,112,97,99,107,108,111,95,112,100,40,95,109,109,53,49,50,95,99,97,115,116,112,115,95,112,100,40,84,53,41,44,32,95,109,109,53,49,50,95,99,97,115,116,112,115,95,112,100,40,84,55,41,41,41,59,10,32,32,107,
101,114,110,101,108,46,112,97,99,107,101,116,91,55,93,32,61,32,95,109,109,53,49,50,95,99,97,115,116,112,100,95,112,115,40,95,109,109,53,49,50,95,117,110,112,97,99,107,104,105,95,112,100,40,95,109,109,53,49,50,95,99,97,115,116,112,115,95,112,100,40,84,
53,41,44,32,95,109,109,53,49,50,95,99,97,115,116,112,115,95,112,100,40,84,55,41,41,41,59,10,10,32,32,84,48,32,61,32,95,109,109,53,49,50,95,99,97,115,116,112,100,95,112,115,40,95,109,109,53,49,50,95,112,101,114,109,117,116,101,120,95,112,100,40,95,109,
109,53,49,50,95,99,97,115,116,112,115,95,112,100,40,107,101,114,110,101,108,46,112,97,99,107,101,116,91,52,93,41,44,32,48,120,52,69,41,41,59,10,32,32,84,48,32,61,32,95,109,109,53,49,50,95,109,97,115,107,95,98,108,101,110,100,95,112,115,40,48,120,70,48,
70,48,44,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,48,93,44,32,84,48,41,59,10,32,32,84,52,32,61,32,95,109,109,53,49,50,95,99,97,115,116,112,100,95,112,115,40,95,109,109,53,49,50,95,112,101,114,109,117,116,101,120,95,112,100,40,95,109,109,
53,49,50,95,99,97,115,116,112,115,95,112,100,40,107,101,114,110,101,108,46,112,97,99,107,101,116,91,48,93,41,44,32,48,120,52,69,41,41,59,10,32,32,84,52,32,61,32,95,109,109,53,49,50,95,109,97,115,107,95,98,108,101,110,100,95,112,115,40,48,120,70,48,70,
48,44,32,84,52,44,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,52,93,41,59,10,32,32,84,49,32,61,32,95,109,109,53,49,50,95,99,97,115,116,112,100,95,112,115,40,95,109,109,53,49,50,95,112,101,114,109,117,116,101,120,95,112,100,40,95,109,109,53,
49,50,95,99,97,115,116,112,115,95,112,100,40,107,101,114,110,101,108,46,112,97,99,107,101,116,91,53,93,41,44,32,48,120,52,69,41,41,59,10,32,32,84,49,32,61,32,95,109,109,53,49,50,95,109,97,115,107,95,98,108,101,110,100,95,112,115,40,48,120,70,48,70,48,
44,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,49,93,44,32,84,49,41,59,10,32,32,84,53,32,61,32,95,109,109,53,49,50,95,99,97,115,116,112,100,95,112,115,40,95,109,109,53,49,50,95,112,101,114,109,117,116,101,120,95,112,100,40,95,109,109,53,49,
50,95,99,97,115,116,112,115,95,112,100,40,107,101,114,110,101,108,46,112,97,99,107,101,116,91,49,93,41,44,32,48,120,52,69,41,41,59,10,32,32,84,53,32,61,32,95,109,109,53,49,50,95,109,97,115,107,95,98,108,101,110,100,95,112,115,40,48,120,70,48,70,48,44,
32,84,53,44,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,53,93,41,59,10,32,32,84,50,32,61,32,95,109,109,53,49,50,95,99,97,115,116,112,100,95,112,115,40,95,109,109,53,49,50,95,112,101,114,109,117,116,101,120,95,112,100,40,95,109,109,53,49,50,
95,99,97,115,116,112,115,95,112,100,40,107,101,114,110,101,108,46,112,97,99,107,101,116,91,54,93,41,44,32,48,120,52,69,41,41,59,10,32,32,84,50,32,61,32,95,109,109,53,49,50,95,109,97,115,107,95,98,108,101,110,100,95,112,115,40,48,120,70,48,70,48,44,32,
107,101,114,110,101,108,46,112,97,99,107,101,116,91,50,93,44,32,84,50,41,59,10,32,32,84,54,32,61,32,95,109,109,53,49,50,95,99,97,115,116,112,100,95,112,115,40,95,109,109,53,49,50,95,112,101,114,109,117,116,101,120,95,112,100,40,95,109,109,53,49,50,95,
99,97,115,116,112,115,95,112,100,40,107,101,114,110,101,108,46,112,97,99,107,101,116,91,50,93,41,44,32,48,120,52,69,41,41,59,10,32,32,84,54,32,61,32,95,109,109,53,49,50,95,109,97,115,107,95,98,108,101,110,100,95,112,115,40,48,120,70,48,70,48,44,32,84,
54,44,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,54,93,41,59,10,32,32,84,51,32,61,32,95,109,109,53,49,50,95,99,97,115,116,112,100,95,112,115,40,95,109,109,53,49,50,95,112,101,114,109,117,116,101,120,95,112,100,40,95,109,109,53,49,50,95,99,
97,115,116,112,115,95,112,100,40,107,101,114,110,101,108,46,112,97,99,107,101,116,91,55,93,41,44,32,48,120,52,69,41,41,59,10,32,32,84,51,32,61,32,95,109,109,53,49,50,95,109,97,115,107,95,98,108,101,110,100,95,112,115,40,48,120,70,48,70,48,44,32,107,101,
114,110,101,108,46,112,97,99,107,101,116,91,51,93,44,32,84,51,41,59,10,32,32,84,55,32,61,32,95,109,109,53,49,50,95,99,97,115,116,112,100,95,112,115,40,95,109,109,53,49,50,95,112,101,114,109,117,116,101,120,95,112,100,40,95,109,109,53,49,50,95,99,97,115,
116,112,115,95,112,100,40,107,101,114,110,101,108,46,112,97,99,107,101,116,91,51,93,41,44,32,48,120,52,69,41,41,59,10,32,32,84,55,32,61,32,95,109,109,53,49,50,95,109,97,115,107,95,98,108,101,110,100,95,112,115,40,48,120,70,48,70,48,44,32,84,55,44,32,
107,101,114,110,101,108,46,112,97,99,107,101,116,91,55,93,41,59,10,10,32,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,48,93,32,61,32,84,48,59,10,32,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,49,93,32,61,32,84,49,59,10,32,32,107,
101,114,110,101,108,46,112,97,99,107,101,116,91,50,93,32,61,32,84,50,59,10,32,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,51,93,32,61,32,84,51,59,10,32,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,52,93,32,61,32,84,52,59,10,32,32,
107,101,114,110,101,108,46,112,97,99,107,101,116,91,53,93,32,61,32,84,53,59,10,32,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,54,93,32,61,32,84,54,59,10,32,32,107,101,114,110,101,108,46,112,97,99,107,101,116,91,55,93,32,61,32,84,55,59,10,125,
10,10,116,101,109,112,108,97,116,101,32,60,62,10,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,105,100,32,116,114,97,110,115,56,120,56,98,108,111,99,107,115,40,80,97,99,107,101,116,66,108,111,99,107,60,80,97,99,107,101,116,56,100,
44,32,56,62,32,38,107,101,114,110,101,108,41,32,123,10,32,32,112,116,114,97,110,115,112,111,115,101,40,107,101,114,110,101,108,41,59,10,125,10,10,47,42,42,42,10,32,42,32,85,110,114,111,108,108,115,32,102,111,114,32,116,114,97,110,112,111,115,101,100,
32,67,32,115,116,111,114,101,115,10,32,42,47,10,116,101,109,112,108,97,116,101,32,60,116,121,112,101,110,97,109,101,32,83,99,97,108,97,114,62,10,99,108,97,115,115,32,116,114,97,110,115,32,123,10,32,112,117,98,108,105,99,58,10,32,32,117,115,105,110,103,
32,118,101,99,32,61,32,116,121,112,101,110,97,109,101,32,115,116,100,58,58,99,111,110,100,105,116,105,111,110,97,108,60,115,116,100,58,58,105,115,95,115,97,109,101,60,83,99,97,108,97,114,44,32,102,108,111,97,116,62,58,58,118,97,108,117,101,44,32,118,
101,99,70,117,108,108,70,108,111,97,116,44,32,118,101,99,70,117,108,108,68,111,117,98,108,101,62,58,58,116,121,112,101,59,10,32,32,117,115,105,110,103,32,118,101,99,72,97,108,102,32,61,32,116,121,112,101,110,97,109,101,32,115,116,100,58,58,99,111,110,
100,105,116,105,111,110,97,108,60,115,116,100,58,58,105,115,95,115,97,109,101,60,83,99,97,108,97,114,44,32,102,108,111,97,116,62,58,58,118,97,108,117,101,44,32,118,101,99,72,97,108,102,70,108,111,97,116,44,32,118,101,99,70,117,108,108,68,111,117,98,108,
101,62,58,58,116,121,112,101,59,10,32,32,115,116,97,116,105,99,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,80,97,99,107,101,116,83,105,122,101,32,61,32,112,97,99,107,101,116,95,116,114,97,105,116,115,60,83,99,97,108,97,114,62,
58,58,115,105,122,101,59,10,10,32,32,47,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,10,32,32,32,42,32,65,117,120,105,108,108,97,114,121,32,70,117,110,99,116,105,111,110,115,32,102,111,114,58,
10,32,32,32,42,32,32,45,32,115,116,111,114,101,67,10,32,32,32,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,10,32,32,32,42,47,10,10,32,32,47,42,42,10,32,32,32,42,32,97,117,120,95,115,116,111,
114,101,67,10,32,32,32,42,10,32,32,32,42,32,49,45,68,32,117,110,114,111,108,108,10,32,32,32,42,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,78,32,61,32,48,59,32,115,116,97,114,116,78,32,60,32,101,110,100,78,59,32,115,116,97,114,116,78,43,43,41,
10,32,32,32,42,10,32,32,32,42,32,40,101,110,100,78,32,60,61,32,80,97,99,107,101,116,83,105,122,101,41,32,105,115,32,114,101,113,117,105,114,101,100,32,116,111,32,104,97,110,100,108,101,32,116,104,101,32,102,112,51,50,32,99,97,115,101,44,32,115,101,101,
32,99,111,109,109,101,110,116,115,32,105,110,32,116,114,97,110,115,83,116,111,114,101,67,10,32,32,32,42,10,32,32,32,42,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,52,95,116,32,99,
111,117,110,116,101,114,44,32,105,110,116,54,52,95,116,32,117,110,114,111,108,108,78,44,32,105,110,116,54,52,95,116,32,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,44,32,98,111,111,108,32,114,101,109,77,62,10,32,32,115,116,97,116,105,
99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,62,32,48,32,38,38,32,101,110,100,78,32,60,61,32,80,97,99,107,101,116,83,105,122,101,41,62,
32,97,117,120,95,115,116,111,114,101,67,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,67,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,67,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,82,67,72,95,68,
69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,44,32,105,110,116,54,52,95,116,32,114,101,109,77,95,32,61,32,48,41,32,123,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,
32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,61,32,101,110,100,78,32,45,32,99,111,117,110,116,101,114,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,115,116,97,114,116,78,32,61,32,99,111,117,110,116,
101,114,82,101,118,101,114,115,101,59,10,10,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,115,116,97,114,116,78,32,60,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,41,32,123,10,32,32,32,32,32,32,69,73,71,69,
78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,114,101,109,77,41,32,123,10,32,32,32,32,32,32,32,32,112,115,116,111,114,101,117,60,83,99,97,108,97,114,62,40,10,32,32,32,32,32,32,32,32,32,32,32,32,67,95,97,114,114,32,43,32,76,68,67,32,42,32,115,116,97,114,
116,78,44,10,32,32,32,32,32,32,32,32,32,32,32,32,112,97,100,100,40,112,108,111,97,100,117,60,118,101,99,72,97,108,102,62,40,40,99,111,110,115,116,32,83,99,97,108,97,114,32,42,41,67,95,97,114,114,32,43,32,76,68,67,32,42,32,115,116,97,114,116,78,44,32,
114,101,109,77,97,115,107,60,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,40,114,101,109,77,95,41,41,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,112,114,101,105,110,116,101,114,112,114,101,116,60,118,101,99,72,97,108,
102,62,40,122,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,40,117,110,114,111,108,108,78,32,47,32,80,97,99,107,101,116,83,105,122,101,41,32,42,32,115,116,97,114,116,78,93,41,44,10,32,32,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,114,101,109,77,97,115,107,60,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,40,114,101,109,77,95,41,41,44,10,32,32,32,32,32,32,32,32,32,32,32,32,114,101,109,77,97,115,107,60,69,73,71,69,
78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,40,114,101,109,77,95,41,41,59,10,32,32,32,32,32,32,125,10,32,32,32,32,32,32,101,108,115,101,32,123,10,32,32,32,32,32,32,32,32,112,115,116,111,114,101,117,60,83,99,97,108,97,114,62,40,67,95,97,114,
114,32,43,32,76,68,67,32,42,32,115,116,97,114,116,78,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,112,97,100,100,40,112,108,111,97,100,117,60,118,101,99,72,97,108,102,62,40,40,99,111,110,115,116,32,83,99,97,108,97,114,
32,42,41,67,95,97,114,114,32,43,32,76,68,67,32,42,32,115,116,97,114,116,78,41,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,112,114,101,105,110,116,101,114,112,114,101,116,60,118,101,99,72,97,108,102,62,
40,122,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,40,117,110,114,111,108,108,78,32,47,32,80,97,99,107,101,116,83,105,122,101,41,32,42,32,115,116,97,114,116,78,93,41,41,41,59,10,32,32,32,
32,32,32,125,10,32,32,32,32,125,10,32,32,32,32,101,108,115,101,32,123,32,32,47,47,32,84,104,105,115,32,98,108,111,99,107,32,105,115,32,111,110,108,121,32,110,101,101,100,101,100,32,102,111,114,32,102,112,51,50,32,99,97,115,101,10,32,32,32,32,32,32,47,
47,32,82,101,105,110,116,101,114,112,114,101,116,32,97,115,32,95,95,109,53,49,50,32,102,111,114,32,95,109,109,53,49,50,95,115,104,117,102,102,108,101,95,102,51,50,120,52,10,32,32,32,32,32,32,118,101,99,70,117,108,108,70,108,111,97,116,32,122,109,109,
50,118,101,99,70,117,108,108,70,108,111,97,116,32,61,32,112,114,101,105,110,116,101,114,112,114,101,116,60,118,101,99,70,117,108,108,70,108,111,97,116,62,40,10,32,32,32,32,32,32,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,
73,110,100,101,120,79,102,102,115,101,116,32,43,32,40,117,110,114,111,108,108,78,32,47,32,80,97,99,107,101,116,83,105,122,101,41,32,42,32,40,115,116,97,114,116,78,32,45,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,41,93,41,59,10,
32,32,32,32,32,32,47,47,32,83,119,97,112,32,108,111,119,101,114,32,97,110,100,32,117,112,112,101,114,32,104,97,108,102,32,111,102,32,97,118,120,32,114,101,103,105,115,116,101,114,46,10,32,32,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,112,97,
99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,40,117,110,114,111,108,108,78,32,47,32,80,97,99,107,101,116,83,105,122,101,41,32,42,32,40,115,116,97,114,116,78,32,45,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,
41,93,32,61,10,32,32,32,32,32,32,32,32,32,32,112,114,101,105,110,116,101,114,112,114,101,116,60,118,101,99,62,40,95,109,109,53,49,50,95,115,104,117,102,102,108,101,95,102,51,50,120,52,40,122,109,109,50,118,101,99,70,117,108,108,70,108,111,97,116,44,32,
122,109,109,50,118,101,99,70,117,108,108,70,108,111,97,116,44,32,48,98,48,49,48,48,49,49,49,48,41,41,59,10,10,32,32,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,114,101,109,77,41,32,123,10,32,32,32,32,32,32,32,32,112,115,116,111,
114,101,117,60,83,99,97,108,97,114,62,40,10,32,32,32,32,32,32,32,32,32,32,32,32,67,95,97,114,114,32,43,32,76,68,67,32,42,32,115,116,97,114,116,78,44,10,32,32,32,32,32,32,32,32,32,32,32,32,112,97,100,100,40,112,108,111,97,100,117,60,118,101,99,72,97,108,
102,62,40,40,99,111,110,115,116,32,83,99,97,108,97,114,32,42,41,67,95,97,114,114,32,43,32,76,68,67,32,42,32,115,116,97,114,116,78,44,32,114,101,109,77,97,115,107,60,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,40,114,101,109,77,95,
41,41,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,112,114,101,105,110,116,101,114,112,114,101,116,60,118,101,99,72,97,108,102,62,40,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,
112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,40,117,110,114,111,108,108,78,32,47,32,80,97,99,107,101,116,83,105,122,101,41,32,42,32,40,115,116,97,114,116,78,32,45,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,
79,87,41,93,41,41,44,10,32,32,32,32,32,32,32,32,32,32,32,32,114,101,109,77,97,115,107,60,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,40,114,101,109,77,95,41,41,59,10,32,32,32,32,32,32,125,10,32,32,32,32,32,32,101,108,115,101,32,
123,10,32,32,32,32,32,32,32,32,112,115,116,111,114,101,117,60,83,99,97,108,97,114,62,40,10,32,32,32,32,32,32,32,32,32,32,32,32,67,95,97,114,114,32,43,32,76,68,67,32,42,32,115,116,97,114,116,78,44,10,32,32,32,32,32,32,32,32,32,32,32,32,112,97,100,100,
40,112,108,111,97,100,117,60,118,101,99,72,97,108,102,62,40,40,99,111,110,115,116,32,83,99,97,108,97,114,32,42,41,67,95,97,114,114,32,43,32,76,68,67,32,42,32,115,116,97,114,116,78,41,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,112,114,101,
105,110,116,101,114,112,114,101,116,60,118,101,99,72,97,108,102,62,40,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,40,
117,110,114,111,108,108,78,32,47,32,80,97,99,107,101,116,83,105,122,101,41,32,42,32,40,115,116,97,114,116,78,32,45,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,41,93,41,41,41,59,10,32,32,32,32,32,32,125,10,32,32,32,32,125,10,32,32,
32,32,97,117,120,95,115,116,111,114,101,67,60,101,110,100,78,44,32,99,111,117,110,116,101,114,32,45,32,49,44,32,117,110,114,111,108,108,78,44,32,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,44,32,114,101,109,77,62,40,67,95,97,114,114,
44,32,76,68,67,44,32,122,109,109,44,32,114,101,109,77,95,41,59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,105,110,116,54,
52,95,116,32,117,110,114,111,108,108,78,44,32,105,110,116,54,52,95,116,32,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,44,32,98,111,111,108,32,114,101,109,77,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,
95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,33,40,99,111,117,110,116,101,114,32,62,32,48,32,38,38,32,101,110,100,78,32,60,61,32,80,97,99,107,101,116,83,105,122,101,41,62,32,97,117,120,95,115,116,111,114,101,67,
40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,67,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,67,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,
95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,44,32,105,110,116,54,52,95,116,32,114,101,109,77,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,67,95,97,114,114,41,59,10,32,32,32,
32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,76,68,67,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,122,109,109,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,
66,76,69,40,114,101,109,77,95,41,59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,52,95,116,32,117,110,114,111,108,108,78,44,32,105,110,116,54,52,95,116,32,112,97,99,107,
101,116,73,110,100,101,120,79,102,102,115,101,116,44,32,98,111,111,108,32,114,101,109,77,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,105,100,32,115,116,111,114,101,67,40,83,99,97,108,97,114,
32,42,67,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,67,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,
32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,
32,32,32,32,32,105,110,116,54,52,95,116,32,114,101,109,77,95,32,61,32,48,41,32,123,10,32,32,32,32,97,117,120,95,115,116,111,114,101,67,60,101,110,100,78,44,32,101,110,100,78,44,32,117,110,114,111,108,108,78,44,32,112,97,99,107,101,116,73,110,100,101,
120,79,102,102,115,101,116,44,32,114,101,109,77,62,40,67,95,97,114,114,44,32,76,68,67,44,32,122,109,109,44,32,114,101,109,77,95,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,84,114,97,110,115,112,111,115,101,115,32,76,120,117,110,114,111,
108,108,78,32,114,111,119,32,109,97,106,111,114,32,98,108,111,99,107,32,111,102,32,109,97,116,114,105,99,101,115,32,115,116,111,114,101,100,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,65,67,67,32,122,109,109,32,114,101,103,105,115,116,101,
114,115,32,116,111,10,32,32,32,42,32,34,117,110,114,111,108,108,78,34,120,76,32,121,109,109,32,114,101,103,105,115,116,101,114,115,32,116,111,32,98,101,32,115,116,111,114,101,100,32,99,111,108,45,109,97,106,111,114,32,105,110,116,111,32,67,46,10,32,32,
32,42,10,32,32,32,42,32,32,70,111,114,32,56,120,52,56,44,32,116,104,101,32,56,120,52,56,32,98,108,111,99,107,32,40,114,111,119,45,109,97,106,111,114,41,32,105,115,32,115,116,111,114,101,100,32,105,110,32,122,109,109,32,97,115,32,102,111,108,108,111,119,
115,58,10,32,32,32,42,10,32,32,32,42,32,32,114,111,119,48,58,32,122,109,109,48,32,122,109,109,49,32,122,109,109,50,10,32,32,32,42,32,32,114,111,119,49,58,32,122,109,109,51,32,122,109,109,52,32,122,109,109,53,10,32,32,32,42,32,32,32,32,46,10,32,32,32,
42,32,32,32,32,46,10,32,32,32,42,32,32,114,111,119,55,58,32,122,109,109,50,49,32,122,109,109,50,50,32,122,109,109,50,51,10,32,32,32,42,10,32,32,32,42,32,32,70,111,114,32,56,120,51,50,44,32,116,104,101,32,56,120,51,50,32,98,108,111,99,107,32,40,114,111,
119,45,109,97,106,111,114,41,32,105,115,32,115,116,111,114,101,100,32,105,110,32,122,109,109,32,97,115,32,102,111,108,108,111,119,115,58,10,32,32,32,42,10,32,32,32,42,32,32,114,111,119,48,58,32,122,109,109,48,32,122,109,109,49,10,32,32,32,42,32,32,114,
111,119,49,58,32,122,109,109,50,32,122,109,109,51,10,32,32,32,42,32,32,32,32,46,10,32,32,32,42,32,32,32,32,46,10,32,32,32,42,32,32,114,111,119,55,58,32,122,109,109,49,52,32,122,109,109,49,53,10,32,32,32,42,10,32,32,32,42,10,32,32,32,42,32,73,110,32,103,
101,110,101,114,97,108,32,119,101,32,119,105,108,108,32,104,97,118,101,32,123,49,44,50,44,51,125,32,103,114,111,117,112,115,32,111,102,32,97,118,120,32,114,101,103,105,115,116,101,114,115,32,101,97,99,104,32,111,102,32,115,105,122,101,10,32,32,32,42,
32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,46,32,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,105,115,32,117,115,101,100,32,116,111,32,115,101,108,101,99,116,32,119,104,105,99,104,32,34,98,108,111,99,107,34,
32,111,102,10,32,32,32,42,32,97,118,120,32,114,101,103,105,115,116,101,114,115,32,97,114,101,32,98,101,105,110,103,32,116,114,97,110,115,112,111,115,101,100,46,10,32,32,32,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,
117,110,114,111,108,108,78,44,32,105,110,116,54,52,95,116,32,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,105,100,32,116,114,97,
110,115,112,111,115,101,40,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,41,32,123,10,32,32,32,32,47,47,
32,78,111,116,101,58,32,116,104,105,115,32,97,115,115,117,109,101,115,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,32,61,32,56,46,32,85,110,114,111,108,108,115,32,115,104,111,117,108,100,32,98,101,32,97,100,106,117,115,116,101,100,
10,32,32,32,32,47,47,32,97,99,99,111,114,100,105,110,103,108,121,32,105,102,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,32,105,115,32,115,109,97,108,108,101,114,46,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,
54,52,95,116,32,122,109,109,83,116,114,105,100,101,32,61,32,117,110,114,111,108,108,78,32,47,32,80,97,99,107,101,116,83,105,122,101,59,10,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,
78,85,77,95,82,79,87,62,32,114,59,10,32,32,32,32,114,46,112,97,99,107,101,116,91,48,93,32,61,32,122,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,122,109,109,83,116,114,105,100,101,32,42,
32,48,93,59,10,32,32,32,32,114,46,112,97,99,107,101,116,91,49,93,32,61,32,122,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,122,109,109,83,116,114,105,100,101,32,42,32,49,93,59,10,32,32,32,
32,114,46,112,97,99,107,101,116,91,50,93,32,61,32,122,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,122,109,109,83,116,114,105,100,101,32,42,32,50,93,59,10,32,32,32,32,114,46,112,97,99,107,
101,116,91,51,93,32,61,32,122,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,122,109,109,83,116,114,105,100,101,32,42,32,51,93,59,10,32,32,32,32,114,46,112,97,99,107,101,116,91,52,93,32,61,
32,122,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,122,109,109,83,116,114,105,100,101,32,42,32,52,93,59,10,32,32,32,32,114,46,112,97,99,107,101,116,91,53,93,32,61,32,122,109,109,46,112,
97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,122,109,109,83,116,114,105,100,101,32,42,32,53,93,59,10,32,32,32,32,114,46,112,97,99,107,101,116,91,54,93,32,61,32,122,109,109,46,112,97,99,107,101,116,91,112,
97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,122,109,109,83,116,114,105,100,101,32,42,32,54,93,59,10,32,32,32,32,114,46,112,97,99,107,101,116,91,55,93,32,61,32,122,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,
100,101,120,79,102,102,115,101,116,32,43,32,122,109,109,83,116,114,105,100,101,32,42,32,55,93,59,10,32,32,32,32,116,114,97,110,115,56,120,56,98,108,111,99,107,115,40,114,41,59,10,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,
73,110,100,101,120,79,102,102,115,101,116,32,43,32,122,109,109,83,116,114,105,100,101,32,42,32,48,93,32,61,32,114,46,112,97,99,107,101,116,91,48,93,59,10,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,
102,102,115,101,116,32,43,32,122,109,109,83,116,114,105,100,101,32,42,32,49,93,32,61,32,114,46,112,97,99,107,101,116,91,49,93,59,10,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,
43,32,122,109,109,83,116,114,105,100,101,32,42,32,50,93,32,61,32,114,46,112,97,99,107,101,116,91,50,93,59,10,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,122,109,109,83,116,
114,105,100,101,32,42,32,51,93,32,61,32,114,46,112,97,99,107,101,116,91,51,93,59,10,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,122,109,109,83,116,114,105,100,101,32,42,
32,52,93,32,61,32,114,46,112,97,99,107,101,116,91,52,93,59,10,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,122,109,109,83,116,114,105,100,101,32,42,32,53,93,32,61,32,114,
46,112,97,99,107,101,116,91,53,93,59,10,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,122,109,109,83,116,114,105,100,101,32,42,32,54,93,32,61,32,114,46,112,97,99,107,101,116,
91,54,93,59,10,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,122,109,109,83,116,114,105,100,101,32,42,32,55,93,32,61,32,114,46,112,97,99,107,101,116,91,55,93,59,10,32,32,125,
10,125,59,10,10,47,42,42,10,32,42,32,85,110,114,111,108,108,115,32,102,111,114,32,99,111,112,121,66,84,111,82,111,119,77,97,106,111,114,10,32,42,10,32,42,32,73,100,101,97,58,10,32,42,32,32,49,41,32,76,111,97,100,32,97,32,98,108,111,99,107,32,111,102,
32,114,105,103,104,116,45,104,97,110,100,32,115,105,100,101,115,32,116,111,32,114,101,103,105,115,116,101,114,115,32,40,117,115,105,110,103,32,108,111,97,100,66,41,46,10,32,42,32,32,50,41,32,67,111,110,118,101,114,116,32,116,104,101,32,98,108,111,99,
107,32,102,114,111,109,32,99,111,108,117,109,110,45,109,97,106,111,114,32,116,111,32,114,111,119,45,109,97,106,111,114,32,40,116,114,97,110,115,112,111,115,101,76,120,76,41,10,32,42,32,32,51,41,32,83,116,111,114,101,32,116,104,101,32,98,108,111,99,107,
115,32,102,114,111,109,32,114,101,103,105,115,116,101,114,32,101,105,116,104,101,114,32,116,111,32,97,32,116,101,109,112,32,97,114,114,97,121,32,40,116,111,84,101,109,112,32,61,61,32,116,114,117,101,41,44,32,111,114,32,98,97,99,107,32,116,111,32,66,32,
40,116,111,84,101,109,112,32,61,61,32,102,97,108,115,101,41,46,10,32,42,10,32,42,32,32,87,101,32,117,115,101,32,97,116,32,109,111,115,116,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,65,67,67,32,97,118,120,32,114,101,103,105,115,116,101,114,
115,32,116,111,32,115,116,111,114,101,32,116,104,101,32,98,108,111,99,107,115,32,111,102,32,66,46,32,84,104,101,32,114,101,109,97,105,110,105,110,103,32,114,101,103,105,115,116,101,114,115,32,97,114,101,10,32,42,32,32,117,115,101,100,32,97,115,32,116,
101,109,112,115,32,102,111,114,32,116,114,97,110,115,112,111,115,105,110,103,46,10,32,42,10,32,42,32,32,66,108,111,99,107,115,32,119,105,108,108,32,98,101,32,111,102,32,115,105,122,101,32,76,120,123,85,49,44,85,50,44,85,51,125,46,32,112,97,99,107,101,
116,73,110,100,101,120,79,102,102,115,101,116,32,105,115,32,117,115,101,100,32,116,111,32,105,110,100,101,120,32,98,101,116,119,101,101,110,32,116,104,101,115,101,32,115,117,98,98,108,111,99,107,115,10,32,42,32,32,70,111,114,32,102,112,51,50,44,32,80,
97,99,107,101,116,83,105,122,101,32,61,32,50,42,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,44,32,115,111,32,119,101,32,114,101,105,110,116,101,114,112,114,101,116,32,112,97,99,107,101,116,115,32,97,115,32,112,97,99,107,101,116,115,
32,104,97,108,102,32,116,104,101,32,115,105,122,101,32,40,122,109,109,32,45,62,32,121,109,109,41,46,10,32,42,47,10,116,101,109,112,108,97,116,101,32,60,116,121,112,101,110,97,109,101,32,83,99,97,108,97,114,62,10,99,108,97,115,115,32,116,114,97,110,115,
66,32,123,10,32,112,117,98,108,105,99,58,10,32,32,117,115,105,110,103,32,118,101,99,32,61,32,116,121,112,101,110,97,109,101,32,115,116,100,58,58,99,111,110,100,105,116,105,111,110,97,108,60,115,116,100,58,58,105,115,95,115,97,109,101,60,83,99,97,108,
97,114,44,32,102,108,111,97,116,62,58,58,118,97,108,117,101,44,32,118,101,99,70,117,108,108,70,108,111,97,116,44,32,118,101,99,70,117,108,108,68,111,117,98,108,101,62,58,58,116,121,112,101,59,10,32,32,117,115,105,110,103,32,118,101,99,72,97,108,102,32,
61,32,116,121,112,101,110,97,109,101,32,115,116,100,58,58,99,111,110,100,105,116,105,111,110,97,108,60,115,116,100,58,58,105,115,95,115,97,109,101,60,83,99,97,108,97,114,44,32,102,108,111,97,116,62,58,58,118,97,108,117,101,44,32,118,101,99,72,97,108,
102,70,108,111,97,116,44,32,118,101,99,70,117,108,108,68,111,117,98,108,101,62,58,58,116,121,112,101,59,10,32,32,115,116,97,116,105,99,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,80,97,99,107,101,116,83,105,122,101,32,61,32,112,
97,99,107,101,116,95,116,114,97,105,116,115,60,83,99,97,108,97,114,62,58,58,115,105,122,101,59,10,10,32,32,47,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,10,32,32,32,42,32,65,117,120,105,108,
108,97,114,121,32,70,117,110,99,116,105,111,110,115,32,102,111,114,58,10,32,32,32,42,32,32,45,32,108,111,97,100,66,10,32,32,32,42,32,32,45,32,115,116,111,114,101,66,10,32,32,32,42,32,32,45,32,108,111,97,100,66,66,108,111,99,107,10,32,32,32,42,32,32,45,
32,115,116,111,114,101,66,66,108,111,99,107,10,32,32,32,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,10,32,32,32,42,47,10,10,32,32,47,42,42,10,32,32,32,42,32,97,117,120,95,108,111,97,100,66,
10,32,32,32,42,10,32,32,32,42,32,49,45,68,32,117,110,114,111,108,108,10,32,32,32,42,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,78,32,61,32,48,59,32,115,116,97,114,116,78,32,60,32,101,110,100,78,59,32,115,116,97,114,116,78,43,43,41,10,32,32,32,
42,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,105,110,116,54,52,95,116,32,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,
116,44,32,98,111,111,108,32,114,101,109,77,44,32,105,110,116,54,52,95,116,32,114,101,109,78,95,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,
60,40,99,111,117,110,116,101,114,32,62,32,48,41,62,32,97,117,120,95,108,111,97,100,66,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,66,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,
99,72,97,108,102,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,121,109,109,44,10,32,32,32,32,32,32,105,110,116,54,52,95,116,32,114,101,109,77,95,32,61,32,48,41,32,123,
10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,61,32,101,110,100,78,32,45,32,99,111,117,110,116,101,114,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,
105,110,116,54,52,95,116,32,115,116,97,114,116,78,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,59,10,10,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,114,101,109,77,41,32,123,10,32,32,32,32,32,32,121,109,109,46,
112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,115,116,97,114,116,78,93,32,61,10,32,32,32,32,32,32,32,32,32,32,112,108,111,97,100,117,60,118,101,99,72,97,108,102,62,40,40,99,111,110,115,116,32,83,99,
97,108,97,114,32,42,41,38,66,95,97,114,114,91,115,116,97,114,116,78,32,42,32,76,68,66,93,44,32,114,101,109,77,97,115,107,60,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,40,114,101,109,77,95,41,41,59,10,32,32,32,32,125,10,32,32,32,
32,101,108,115,101,32,123,10,32,32,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,114,101,109,78,95,32,61,61,32,48,41,32,123,10,32,32,32,32,32,32,32,32,121,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,
120,79,102,102,115,101,116,32,43,32,115,116,97,114,116,78,93,32,61,32,112,108,111,97,100,117,60,118,101,99,72,97,108,102,62,40,40,99,111,110,115,116,32,83,99,97,108,97,114,32,42,41,38,66,95,97,114,114,91,115,116,97,114,116,78,32,42,32,76,68,66,93,41,
59,10,32,32,32,32,32,32,125,10,32,32,32,32,32,32,101,108,115,101,32,121,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,115,116,97,114,116,78,93,32,61,10,32,32,32,32,32,32,32,32,32,32,112,108,
111,97,100,117,60,118,101,99,72,97,108,102,62,40,40,99,111,110,115,116,32,83,99,97,108,97,114,32,42,41,38,66,95,97,114,114,91,115,116,97,114,116,78,32,42,32,76,68,66,93,44,32,114,101,109,77,97,115,107,60,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,
77,95,82,79,87,62,40,114,101,109,78,95,41,41,59,10,32,32,32,32,125,10,10,32,32,32,32,97,117,120,95,108,111,97,100,66,60,101,110,100,78,44,32,99,111,117,110,116,101,114,32,45,32,49,44,32,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,
44,32,114,101,109,77,44,32,114,101,109,78,95,62,40,66,95,97,114,114,44,32,76,68,66,44,32,121,109,109,44,32,114,101,109,77,95,41,59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,
116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,105,110,116,54,52,95,116,32,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,44,32,98,111,111,108,32,114,101,109,77,44,32,105,110,116,54,52,95,116,32,114,101,109,78,95,62,10,32,32,115,
116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,60,61,32,48,41,62,32,97,117,120,95,108,111,97,100,66,40,10,32,32,32,32,32,32,
83,99,97,108,97,114,32,42,66,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,72,97,108,102,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,
82,69,71,73,83,84,69,82,83,62,32,38,121,109,109,44,10,32,32,32,32,32,32,105,110,116,54,52,95,116,32,114,101,109,77,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,66,95,97,114,114,41,59,10,32,
32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,76,68,66,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,121,109,109,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,
73,65,66,76,69,40,114,101,109,77,95,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,97,117,120,95,115,116,111,114,101,66,10,32,32,32,42,10,32,32,32,42,32,49,45,68,32,117,110,114,111,108,108,10,32,32,32,42,32,32,32,32,32,32,102,111,114,40,115,
116,97,114,116,78,32,61,32,48,59,32,115,116,97,114,116,78,32,60,32,101,110,100,78,59,32,115,116,97,114,116,78,43,43,41,10,32,32,32,42,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,
52,95,116,32,99,111,117,110,116,101,114,44,32,105,110,116,54,52,95,116,32,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,44,32,98,111,111,108,32,114,101,109,75,44,32,98,111,111,108,32,114,101,109,77,62,10,32,32,115,116,97,116,105,99,
32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,62,32,48,41,62,32,97,117,120,95,115,116,111,114,101,66,40,10,32,32,32,32,32,32,83,99,97,108,97,
114,32,42,66,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,72,97,108,102,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,
84,69,82,83,62,32,38,121,109,109,44,32,105,110,116,54,52,95,116,32,114,101,109,95,32,61,32,48,41,32,123,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,61,32,101,
110,100,78,32,45,32,99,111,117,110,116,101,114,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,115,116,97,114,116,78,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,59,10,10,32,32,32,32,69,73,71,69,78,
95,73,70,95,67,79,78,83,84,69,88,80,82,40,114,101,109,75,32,124,124,32,114,101,109,77,41,32,123,10,32,32,32,32,32,32,112,115,116,111,114,101,117,60,83,99,97,108,97,114,62,40,38,66,95,97,114,114,91,115,116,97,114,116,78,32,42,32,76,68,66,93,44,32,121,
109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,115,116,97,114,116,78,93,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,114,101,109,77,97,115,107,60,69,73,71,69,78,
95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,40,114,101,109,95,41,41,59,10,32,32,32,32,125,10,32,32,32,32,101,108,115,101,32,123,10,32,32,32,32,32,32,112,115,116,111,114,101,117,60,83,99,97,108,97,114,62,40,38,66,95,97,114,114,91,115,116,97,114,
116,78,32,42,32,76,68,66,93,44,32,121,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,115,116,97,114,116,78,93,41,59,10,32,32,32,32,125,10,10,32,32,32,32,97,117,120,95,115,116,111,114,101,66,
60,101,110,100,78,44,32,99,111,117,110,116,101,114,32,45,32,49,44,32,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,44,32,114,101,109,75,44,32,114,101,109,77,62,40,66,95,97,114,114,44,32,76,68,66,44,32,121,109,109,44,32,114,101,109,95,
41,59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,105,110,116,54,52,95,116,32,112,97,99,107,101,116,73,110,100,101,120,79,
102,102,115,101,116,44,32,98,111,111,108,32,114,101,109,75,44,32,98,111,111,108,32,114,101,109,77,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,
60,40,99,111,117,110,116,101,114,32,60,61,32,48,41,62,32,97,117,120,95,115,116,111,114,101,66,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,66,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,80,97,99,107,101,116,66,108,111,99,107,60,
118,101,99,72,97,108,102,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,121,109,109,44,32,105,110,116,54,52,95,116,32,114,101,109,95,32,61,32,48,41,32,123,10,32,32,32,32,
69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,66,95,97,114,114,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,76,68,66,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,
65,66,76,69,40,121,109,109,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,114,101,109,95,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,97,117,120,95,108,111,97,100,66,66,108,111,99,107,10,32,32,32,42,
10,32,32,32,42,32,49,45,68,32,117,110,114,111,108,108,10,32,32,32,42,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,78,32,61,32,48,59,32,115,116,97,114,116,78,32,60,32,101,110,100,78,59,32,115,116,97,114,116,78,32,43,61,32,69,73,71,69,78,95,65,86,
88,95,77,65,88,95,78,85,77,95,82,79,87,41,10,32,32,32,42,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,98,111,111,108,32,116,111,84,101,
109,112,44,32,98,111,111,108,32,114,101,109,77,44,32,105,110,116,54,52,95,116,32,114,101,109,78,95,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,
116,60,40,99,111,117,110,116,101,114,32,62,32,48,41,62,32,97,117,120,95,108,111,97,100,66,66,108,111,99,107,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,66,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,83,99,97,108,97,114,32,42,
66,95,116,101,109,112,44,32,105,110,116,54,52,95,116,32,76,68,66,95,44,10,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,72,97,108,102,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,
82,69,71,73,83,84,69,82,83,62,32,38,121,109,109,44,32,105,110,116,54,52,95,116,32,114,101,109,77,95,32,61,32,48,41,32,123,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,82,101,118,101,114,115,
101,32,61,32,101,110,100,78,32,45,32,99,111,117,110,116,101,114,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,115,116,97,114,116,78,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,59,10,32,32,32,32,
116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,108,111,97,100,66,60,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,44,32,115,116,97,114,116,78,44,32,102,97,108,115,101,44,32,40,116,111,84,101,109,112,32,63,32,48,32,58,32,
114,101,109,78,95,41,62,40,38,66,95,116,101,109,112,91,115,116,97,114,116,78,93,44,32,76,68,66,95,44,32,121,109,109,41,59,10,32,32,32,32,97,117,120,95,108,111,97,100,66,66,108,111,99,107,60,101,110,100,78,44,32,99,111,117,110,116,101,114,32,45,32,69,
73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,44,32,116,111,84,101,109,112,44,32,114,101,109,77,44,32,114,101,109,78,95,62,40,66,95,97,114,114,44,32,76,68,66,44,32,66,95,116,101,109,112,44,32,76,68,66,95,44,32,121,109,109,44,32,114,101,
109,77,95,41,59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,98,111,111,108,32,116,111,84,101,109,112,44,32,98,111,111,108,
32,114,101,109,77,44,32,105,110,116,54,52,95,116,32,114,101,109,78,95,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,
101,114,32,60,61,32,48,41,62,32,97,117,120,95,108,111,97,100,66,66,108,111,99,107,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,66,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,83,99,97,108,97,114,32,42,66,95,116,101,109,112,44,32,
105,110,116,54,52,95,116,32,76,68,66,95,44,10,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,72,97,108,102,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,
62,32,38,121,109,109,44,32,105,110,116,54,52,95,116,32,114,101,109,77,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,66,95,97,114,114,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,
95,86,65,82,73,65,66,76,69,40,76,68,66,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,66,95,116,101,109,112,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,76,68,66,95,41,
59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,121,109,109,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,114,101,109,77,95,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,
32,42,32,97,117,120,95,115,116,111,114,101,66,66,108,111,99,107,10,32,32,32,42,10,32,32,32,42,32,49,45,68,32,117,110,114,111,108,108,10,32,32,32,42,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,78,32,61,32,48,59,32,115,116,97,114,116,78,32,60,32,
101,110,100,78,59,32,115,116,97,114,116,78,32,43,61,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,41,10,32,32,32,42,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,
52,95,116,32,99,111,117,110,116,101,114,44,32,98,111,111,108,32,116,111,84,101,109,112,44,32,98,111,111,108,32,114,101,109,77,44,32,105,110,116,54,52,95,116,32,114,101,109,75,95,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,
95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,62,32,48,41,62,32,97,117,120,95,115,116,111,114,101,66,66,108,111,99,107,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,66,95,97,
114,114,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,83,99,97,108,97,114,32,42,66,95,116,101,109,112,44,32,105,110,116,54,52,95,116,32,76,68,66,95,44,10,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,72,97,108,102,44,32,69,
73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,121,109,109,44,32,105,110,116,54,52,95,116,32,114,101,109,77,95,32,61,32,48,41,32,123,10,32,32,32,32,99,111,110,115,116,101,120,112,
114,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,61,32,101,110,100,78,32,45,32,99,111,117,110,116,101,114,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,115,116,97,114,116,
78,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,59,10,10,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,116,111,84,101,109,112,41,32,123,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,
101,32,115,116,111,114,101,66,60,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,44,32,115,116,97,114,116,78,44,32,114,101,109,75,95,32,33,61,32,48,44,32,102,97,108,115,101,62,40,38,66,95,116,101,109,112,91,115,116,97,114,116,78,93,44,
32,76,68,66,95,44,32,121,109,109,44,32,114,101,109,75,95,41,59,10,32,32,32,32,125,10,32,32,32,32,101,108,115,101,32,123,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,115,116,111,114,101,66,60,115,116,100,58,58,109,
105,110,40,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,44,32,101,110,100,78,41,44,32,115,116,97,114,116,78,44,32,102,97,108,115,101,44,32,114,101,109,77,62,40,38,66,95,97,114,114,91,48,32,43,32,115,116,97,114,116,78,32,42,32,76,68,
66,93,44,32,76,68,66,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,121,109,109,44,32,114,101,109,77,95,41,59,10,32,32,32,32,125,10,32,32,32,32,97,117,120,95,115,116,111,114,101,66,66,108,111,99,107,60,101,110,100,78,44,32,99,111,117,110,116,101,114,32,45,32,69,73,71,69,78,
95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,44,32,116,111,84,101,109,112,44,32,114,101,109,77,44,32,114,101,109,75,95,62,40,66,95,97,114,114,44,32,76,68,66,44,32,66,95,116,101,109,112,44,32,76,68,66,95,44,32,121,109,109,44,32,114,101,109,77,95,41,
59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,98,111,111,108,32,116,111,84,101,109,112,44,32,98,111,111,108,32,114,101,109,
77,44,32,105,110,116,54,52,95,116,32,114,101,109,75,95,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,60,61,
32,48,41,62,32,97,117,120,95,115,116,111,114,101,66,66,108,111,99,107,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,66,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,83,99,97,108,97,114,32,42,66,95,116,101,109,112,44,32,105,110,116,
54,52,95,116,32,76,68,66,95,44,10,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,72,97,108,102,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,121,
109,109,44,32,105,110,116,54,52,95,116,32,114,101,109,77,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,66,95,97,114,114,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,
65,66,76,69,40,76,68,66,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,66,95,116,101,109,112,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,76,68,66,95,41,59,10,32,32,32,
32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,121,109,109,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,114,101,109,77,95,41,59,10,32,32,125,10,10,32,32,47,42,42,42,42,42,42,42,42,42,42,
42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,10,32,32,32,42,32,87,114,97,112,112,101,114,115,32,102,111,114,32,97,117,120,95,88,88,88,88,32,116,111,32,104,105,
100,101,32,99,111,117,110,116,101,114,32,112,97,114,97,109,101,116,101,114,10,32,32,32,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,
42,47,10,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,52,95,116,32,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,44,32,98,111,111,108,32,114,101,109,77,44,32,105,110,116,
54,52,95,116,32,114,101,109,78,95,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,105,100,32,108,111,97,100,66,40,83,99,97,108,97,114,32,42,66,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,
68,66,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,72,97,108,102,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,
85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,121,109,109,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,105,110,116,54,52,95,116,32,114,101,
109,77,95,32,61,32,48,41,32,123,10,32,32,32,32,97,117,120,95,108,111,97,100,66,60,101,110,100,78,44,32,101,110,100,78,44,32,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,44,32,114,101,109,77,44,32,114,101,109,78,95,62,40,66,95,97,114,
114,44,32,76,68,66,44,32,121,109,109,44,32,114,101,109,77,95,41,59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,52,95,116,32,112,97,99,107,101,116,73,110,100,101,120,79,
102,102,115,101,116,44,32,98,111,111,108,32,114,101,109,75,44,32,98,111,111,108,32,114,101,109,77,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,105,100,32,115,116,111,114,101,66,40,83,99,97,108,
97,114,32,42,66,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,66,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,
99,72,97,108,102,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,121,109,109,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,
32,32,32,32,32,32,32,32,32,32,32,32,105,110,116,54,52,95,116,32,114,101,109,95,32,61,32,48,41,32,123,10,32,32,32,32,97,117,120,95,115,116,111,114,101,66,60,101,110,100,78,44,32,101,110,100,78,44,32,112,97,99,107,101,116,73,110,100,101,120,79,102,102,
115,101,116,44,32,114,101,109,75,44,32,114,101,109,77,62,40,66,95,97,114,114,44,32,76,68,66,44,32,121,109,109,44,32,114,101,109,95,41,59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,117,110,114,111,108,108,
78,44,32,98,111,111,108,32,116,111,84,101,109,112,44,32,98,111,111,108,32,114,101,109,77,44,32,105,110,116,54,52,95,116,32,114,101,109,78,95,32,61,32,48,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,
118,111,105,100,32,108,111,97,100,66,66,108,111,99,107,40,83,99,97,108,97,114,32,42,66,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,83,99,97,108,97,114,32,42,66,95,116,101,109,112,44,32,105,110,116,54,52,95,116,32,76,68,66,95,44,10,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,72,97,108,102,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,
65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,121,109,109,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,105,110,116,54,52,
95,116,32,114,101,109,77,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,116,111,84,101,109,112,41,32,123,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,108,111,97,100,66,60,117,110,114,
111,108,108,78,44,32,48,44,32,114,101,109,77,44,32,48,62,40,38,66,95,97,114,114,91,48,93,44,32,76,68,66,44,32,121,109,109,44,32,114,101,109,77,95,41,59,32,125,10,32,32,32,32,101,108,115,101,32,123,10,32,32,32,32,32,32,97,117,120,95,108,111,97,100,66,
66,108,111,99,107,60,117,110,114,111,108,108,78,44,32,117,110,114,111,108,108,78,44,32,116,111,84,101,109,112,44,32,114,101,109,77,44,32,114,101,109,78,95,62,40,66,95,97,114,114,44,32,76,68,66,44,32,66,95,116,101,109,112,44,32,76,68,66,95,44,32,121,109,
109,44,32,114,101,109,77,95,41,59,10,32,32,32,32,125,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,117,110,114,111,108,108,78,44,32,98,111,111,108,32,116,111,84,101,109,112,44,32,98,111,111,108,32,114,101,109,
77,44,32,105,110,116,54,52,95,116,32,114,101,109,75,95,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,105,100,32,115,116,111,114,101,66,66,108,111,99,107,40,83,99,97,108,97,114,32,42,66,95,97,
114,114,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,83,99,97,108,97,114,32,42,66,95,116,101,109,112,44,32,105,110,116,54,52,95,116,32,76,68,66,95,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,72,97,108,102,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,121,109,109,
44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,105,110,116,54,52,95,116,32,114,101,109,77,95,32,61,32,48,41,32,123,10,32,32,32,32,97,117,120,95,115,116,111,
114,101,66,66,108,111,99,107,60,117,110,114,111,108,108,78,44,32,117,110,114,111,108,108,78,44,32,116,111,84,101,109,112,44,32,114,101,109,77,44,32,114,101,109,75,95,62,40,66,95,97,114,114,44,32,76,68,66,44,32,66,95,116,101,109,112,44,32,76,68,66,95,
44,32,121,109,109,44,32,114,101,109,77,95,41,59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,
78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,105,100,32,116,114,97,110,115,112,111,115,101,76,120,76,40,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,72,97,108,102,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,
77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,121,109,109,41,32,123,10,32,32,32,32,47,47,32,78,111,116,101,58,32,116,104,105,115,32,97,115,115,117,109,101,115,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,32,61,32,56,
46,32,85,110,114,111,108,108,115,32,115,104,111,117,108,100,32,98,101,32,97,100,106,117,115,116,101,100,10,32,32,32,32,47,47,32,97,99,99,111,114,100,105,110,103,108,121,32,105,102,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,32,105,
115,32,115,109,97,108,108,101,114,46,10,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,72,97,108,102,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,32,114,59,10,32,32,32,32,114,46,112,97,99,107,101,116,91,48,
93,32,61,32,121,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,48,93,59,10,32,32,32,32,114,46,112,97,99,107,101,116,91,49,93,32,61,32,121,109,109,46,112,97,99,107,101,116,91,112,97,99,107,
101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,49,93,59,10,32,32,32,32,114,46,112,97,99,107,101,116,91,50,93,32,61,32,121,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,50,93,59,
10,32,32,32,32,114,46,112,97,99,107,101,116,91,51,93,32,61,32,121,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,51,93,59,10,32,32,32,32,114,46,112,97,99,107,101,116,91,52,93,32,61,32,121,
109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,52,93,59,10,32,32,32,32,114,46,112,97,99,107,101,116,91,53,93,32,61,32,121,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,
101,120,79,102,102,115,101,116,32,43,32,53,93,59,10,32,32,32,32,114,46,112,97,99,107,101,116,91,54,93,32,61,32,121,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,54,93,59,10,32,32,32,32,114,
46,112,97,99,107,101,116,91,55,93,32,61,32,121,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,55,93,59,10,32,32,32,32,112,116,114,97,110,115,112,111,115,101,40,114,41,59,10,32,32,32,32,121,
109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,48,93,32,61,32,114,46,112,97,99,107,101,116,91,48,93,59,10,32,32,32,32,121,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,
101,120,79,102,102,115,101,116,32,43,32,49,93,32,61,32,114,46,112,97,99,107,101,116,91,49,93,59,10,32,32,32,32,121,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,50,93,32,61,32,114,46,112,
97,99,107,101,116,91,50,93,59,10,32,32,32,32,121,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,51,93,32,61,32,114,46,112,97,99,107,101,116,91,51,93,59,10,32,32,32,32,121,109,109,46,112,97,
99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,52,93,32,61,32,114,46,112,97,99,107,101,116,91,52,93,59,10,32,32,32,32,121,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,
102,115,101,116,32,43,32,53,93,32,61,32,114,46,112,97,99,107,101,116,91,53,93,59,10,32,32,32,32,121,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,54,93,32,61,32,114,46,112,97,99,107,101,116,
91,54,93,59,10,32,32,32,32,121,109,109,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,79,102,102,115,101,116,32,43,32,55,93,32,61,32,114,46,112,97,99,107,101,116,91,55,93,59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,
32,60,105,110,116,54,52,95,116,32,117,110,114,111,108,108,78,44,32,98,111,111,108,32,116,111,84,101,109,112,44,32,98,111,111,108,32,114,101,109,77,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,
105,100,32,116,114,97,110,115,66,95,107,101,114,110,101,108,40,83,99,97,108,97,114,32,42,66,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,83,99,97,108,97,114,32,42,66,95,116,101,109,112,44,32,105,110,116,54,52,95,116,32,76,68,66,95,44,
10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,72,97,108,102,44,32,69,73,71,69,78,95,65,82,67,72,
95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,121,109,109,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,
32,105,110,116,54,52,95,116,32,114,101,109,77,95,32,61,32,48,41,32,123,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,85,51,32,61,32,80,97,99,107,101,116,83,105,122,101,32,42,32,51,59,10,32,32,32,32,99,111,110,115,116,
101,120,112,114,32,105,110,116,54,52,95,116,32,85,50,32,61,32,80,97,99,107,101,116,83,105,122,101,32,42,32,50,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,85,49,32,61,32,80,97,99,107,101,116,83,105,122,101,32,42,
32,49,59,10,32,32,32,32,47,42,42,10,32,32,32,32,32,42,32,32,85,110,114,111,108,108,115,32,110,101,101,100,101,100,32,102,111,114,32,101,97,99,104,32,99,97,115,101,58,10,32,32,32,32,32,42,32,32,32,45,32,65,86,88,53,49,50,32,102,112,51,50,32,52,56,32,51,
50,32,49,54,32,56,32,52,32,50,32,49,10,32,32,32,32,32,42,32,32,32,45,32,65,86,88,53,49,50,32,102,112,54,52,32,50,52,32,49,54,32,56,32,32,52,32,50,32,49,10,32,32,32,32,32,42,10,32,32,32,32,32,42,32,32,70,111,114,32,102,112,51,50,32,76,32,97,110,100,32,
85,49,32,97,114,101,32,49,58,50,32,115,111,32,102,111,114,32,85,51,47,85,50,32,99,97,115,101,115,32,116,104,101,32,108,111,97,100,115,47,115,116,111,114,101,115,32,110,101,101,100,32,116,111,32,98,101,32,115,112,108,105,116,32,117,112,46,10,32,32,32,
32,32,42,47,10,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,117,110,114,111,108,108,78,32,61,61,32,85,51,41,32,123,10,32,32,32,32,32,32,47,47,32,108,111,97,100,32,76,120,85,51,32,66,32,99,111,108,32,109,97,106,111,114,44,32,116,
114,97,110,115,112,111,115,101,32,76,120,85,51,32,114,111,119,32,109,97,106,111,114,10,32,32,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,109,97,120,85,66,108,111,99,107,32,61,32,115,116,100,58,58,109,105,110,40,51,32,
42,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,44,32,85,51,41,59,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,108,111,97,100,66,66,108,111,99,107,60,109,97,120,85,66,108,111,99,107,44,32,116,
111,84,101,109,112,44,32,114,101,109,77,62,40,66,95,97,114,114,44,32,76,68,66,44,32,66,95,116,101,109,112,44,32,76,68,66,95,44,32,121,109,109,44,32,114,101,109,77,95,41,59,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,
32,116,114,97,110,115,112,111,115,101,76,120,76,60,48,32,42,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,40,121,109,109,41,59,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,116,114,97,110,115,
112,111,115,101,76,120,76,60,49,32,42,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,40,121,109,109,41,59,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,116,114,97,110,115,112,111,115,101,76,120,
76,60,50,32,42,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,40,121,109,109,41,59,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,115,116,111,114,101,66,66,108,111,99,107,60,109,97,120,85,66,108,
111,99,107,44,32,116,111,84,101,109,112,44,32,114,101,109,77,44,32,48,62,40,66,95,97,114,114,44,32,76,68,66,44,32,66,95,116,101,109,112,44,32,76,68,66,95,44,32,121,109,109,44,32,114,101,109,77,95,41,59,10,10,32,32,32,32,32,32,69,73,71,69,78,95,73,70,
95,67,79,78,83,84,69,88,80,82,40,109,97,120,85,66,108,111,99,107,32,60,32,85,51,41,32,123,10,32,32,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,108,111,97,100,66,66,108,111,99,107,60,109,97,120,85,66,108,111,99,107,
44,32,116,111,84,101,109,112,44,32,114,101,109,77,62,40,38,66,95,97,114,114,91,109,97,120,85,66,108,111,99,107,32,42,32,76,68,66,93,44,32,76,68,66,44,32,38,66,95,116,101,109,112,91,109,97,120,85,66,108,111,99,107,93,44,32,76,68,66,95,44,10,32,32,32,32,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,121,109,109,44,32,114,101,109,77,95,41,59,10,32,32,32,32,32,32,32,32,116,114,97,
110,115,66,58,58,116,101,109,112,108,97,116,101,32,116,114,97,110,115,112,111,115,101,76,120,76,60,48,32,42,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,40,121,109,109,41,59,10,32,32,32,32,32,32,32,32,116,114,97,110,115,66,58,
58,116,101,109,112,108,97,116,101,32,116,114,97,110,115,112,111,115,101,76,120,76,60,49,32,42,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,40,121,109,109,41,59,10,32,32,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,
112,108,97,116,101,32,116,114,97,110,115,112,111,115,101,76,120,76,60,50,32,42,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,40,121,109,109,41,59,10,32,32,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,
101,32,115,116,111,114,101,66,66,108,111,99,107,60,109,97,120,85,66,108,111,99,107,44,32,116,111,84,101,109,112,44,32,114,101,109,77,44,32,48,62,40,38,66,95,97,114,114,91,109,97,120,85,66,108,111,99,107,32,42,32,76,68,66,93,44,32,76,68,66,44,32,38,66,
95,116,101,109,112,91,109,97,120,85,66,108,111,99,107,93,44,32,76,68,66,95,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,
32,32,32,32,32,32,32,32,121,109,109,44,32,114,101,109,77,95,41,59,10,32,32,32,32,32,32,125,10,32,32,32,32,125,10,32,32,32,32,101,108,115,101,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,117,110,114,111,108,108,78,32,61,61,32,85,50,41,32,
123,10,32,32,32,32,32,32,47,47,32,108,111,97,100,32,76,120,85,50,32,66,32,99,111,108,32,109,97,106,111,114,44,32,116,114,97,110,115,112,111,115,101,32,76,120,85,50,32,114,111,119,32,109,97,106,111,114,10,32,32,32,32,32,32,99,111,110,115,116,101,120,112,
114,32,105,110,116,54,52,95,116,32,109,97,120,85,66,108,111,99,107,32,61,32,115,116,100,58,58,109,105,110,40,51,32,42,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,44,32,85,50,41,59,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,
116,101,109,112,108,97,116,101,32,108,111,97,100,66,66,108,111,99,107,60,109,97,120,85,66,108,111,99,107,44,32,116,111,84,101,109,112,44,32,114,101,109,77,62,40,66,95,97,114,114,44,32,76,68,66,44,32,66,95,116,101,109,112,44,32,76,68,66,95,44,32,121,109,
109,44,32,114,101,109,77,95,41,59,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,116,114,97,110,115,112,111,115,101,76,120,76,60,48,32,42,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,40,121,
109,109,41,59,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,116,114,97,110,115,112,111,115,101,76,120,76,60,49,32,42,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,40,121,109,109,41,59,10,32,
32,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,109,97,120,85,66,108,111,99,107,32,60,32,85,50,41,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,116,114,97,110,115,112,111,115,101,76,120,76,60,50,32,42,32,69,
73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,40,121,109,109,41,59,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,115,116,111,114,101,66,66,108,111,99,107,60,109,97,120,85,66,108,111,99,107,44,32,116,
111,84,101,109,112,44,32,114,101,109,77,44,32,48,62,40,66,95,97,114,114,44,32,76,68,66,44,32,66,95,116,101,109,112,44,32,76,68,66,95,44,32,121,109,109,44,32,114,101,109,77,95,41,59,10,10,32,32,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,
88,80,82,40,109,97,120,85,66,108,111,99,107,32,60,32,85,50,41,32,123,10,32,32,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,108,111,97,100,66,66,108,111,99,107,60,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,
82,79,87,44,32,116,111,84,101,109,112,44,32,114,101,109,77,62,40,38,66,95,97,114,114,91,109,97,120,85,66,108,111,99,107,32,42,32,76,68,66,93,44,32,76,68,66,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,38,66,95,116,101,109,112,91,109,97,120,85,66,108,111,99,107,93,44,32,76,68,66,95,44,32,121,109,109,44,32,114,101,109,77,95,
41,59,10,32,32,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,116,114,97,110,115,112,111,115,101,76,120,76,60,48,62,40,121,109,109,41,59,10,32,32,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,
101,32,115,116,111,114,101,66,66,108,111,99,107,60,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,44,32,116,111,84,101,109,112,44,32,114,101,109,77,44,32,48,62,40,38,66,95,97,114,114,91,109,97,120,85,66,108,111,99,107,32,42,32,76,68,66,
93,44,32,76,68,66,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,
32,38,66,95,116,101,109,112,91,109,97,120,85,66,108,111,99,107,93,44,32,76,68,66,95,44,32,121,109,109,44,32,114,101,109,77,95,41,59,10,32,32,32,32,32,32,125,10,32,32,32,32,125,10,32,32,32,32,101,108,115,101,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,
69,88,80,82,40,117,110,114,111,108,108,78,32,61,61,32,85,49,41,32,123,10,32,32,32,32,32,32,47,47,32,108,111,97,100,32,76,120,85,49,32,66,32,99,111,108,32,109,97,106,111,114,44,32,116,114,97,110,115,112,111,115,101,32,76,120,85,49,32,114,111,119,32,109,
97,106,111,114,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,108,111,97,100,66,66,108,111,99,107,60,85,49,44,32,116,111,84,101,109,112,44,32,114,101,109,77,62,40,66,95,97,114,114,44,32,76,68,66,44,32,66,95,116,101,
109,112,44,32,76,68,66,95,44,32,121,109,109,44,32,114,101,109,77,95,41,59,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,116,114,97,110,115,112,111,115,101,76,120,76,60,48,62,40,121,109,109,41,59,10,32,32,32,32,32,
32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,32,60,32,85,49,41,32,123,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,116,114,97,110,115,112,111,115,101,76,
120,76,60,49,32,42,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,40,121,109,109,41,59,32,125,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,115,116,111,114,101,66,66,108,111,99,107,60,85,49,44,
32,116,111,84,101,109,112,44,32,114,101,109,77,44,32,48,62,40,66,95,97,114,114,44,32,76,68,66,44,32,66,95,116,101,109,112,44,32,76,68,66,95,44,32,121,109,109,44,32,114,101,109,77,95,41,59,10,32,32,32,32,125,10,32,32,32,32,101,108,115,101,32,69,73,71,
69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,117,110,114,111,108,108,78,32,61,61,32,56,32,38,38,32,85,49,32,62,32,56,41,32,123,10,32,32,32,32,32,32,47,47,32,108,111,97,100,32,76,120,52,32,66,32,99,111,108,32,109,97,106,111,114,44,32,116,114,97,110,
115,112,111,115,101,32,76,120,52,32,114,111,119,32,109,97,106,111,114,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,108,111,97,100,66,66,108,111,99,107,60,56,44,32,116,111,84,101,109,112,44,32,114,101,109,77,62,40,
66,95,97,114,114,44,32,76,68,66,44,32,66,95,116,101,109,112,44,32,76,68,66,95,44,32,121,109,109,44,32,114,101,109,77,95,41,59,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,116,114,97,110,115,112,111,115,101,76,120,
76,60,48,62,40,121,109,109,41,59,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,115,116,111,114,101,66,66,108,111,99,107,60,56,44,32,116,111,84,101,109,112,44,32,114,101,109,77,44,32,56,62,40,66,95,97,114,114,44,32,
76,68,66,44,32,66,95,116,101,109,112,44,32,76,68,66,95,44,32,121,109,109,44,32,114,101,109,77,95,41,59,10,32,32,32,32,125,10,32,32,32,32,101,108,115,101,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,117,110,114,111,108,108,78,32,61,61,32,
52,32,38,38,32,85,49,32,62,32,52,41,32,123,10,32,32,32,32,32,32,47,47,32,108,111,97,100,32,76,120,52,32,66,32,99,111,108,32,109,97,106,111,114,44,32,116,114,97,110,115,112,111,115,101,32,76,120,52,32,114,111,119,32,109,97,106,111,114,10,32,32,32,32,32,
32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,108,111,97,100,66,66,108,111,99,107,60,52,44,32,116,111,84,101,109,112,44,32,114,101,109,77,62,40,66,95,97,114,114,44,32,76,68,66,44,32,66,95,116,101,109,112,44,32,76,68,66,95,44,32,121,
109,109,44,32,114,101,109,77,95,41,59,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,116,114,97,110,115,112,111,115,101,76,120,76,60,48,62,40,121,109,109,41,59,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,
109,112,108,97,116,101,32,115,116,111,114,101,66,66,108,111,99,107,60,52,44,32,116,111,84,101,109,112,44,32,114,101,109,77,44,32,52,62,40,66,95,97,114,114,44,32,76,68,66,44,32,66,95,116,101,109,112,44,32,76,68,66,95,44,32,121,109,109,44,32,114,101,109,
77,95,41,59,10,32,32,32,32,125,10,32,32,32,32,101,108,115,101,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,117,110,114,111,108,108,78,32,61,61,32,50,41,32,123,10,32,32,32,32,32,32,47,47,32,108,111,97,100,32,76,120,50,32,66,32,99,111,108,
32,109,97,106,111,114,44,32,116,114,97,110,115,112,111,115,101,32,76,120,50,32,114,111,119,32,109,97,106,111,114,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,108,111,97,100,66,66,108,111,99,107,60,50,44,32,116,111,
84,101,109,112,44,32,114,101,109,77,44,32,50,62,40,66,95,97,114,114,44,32,76,68,66,44,32,66,95,116,101,109,112,44,32,76,68,66,95,44,32,121,109,109,44,32,114,101,109,77,95,41,59,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,
101,32,116,114,97,110,115,112,111,115,101,76,120,76,60,48,62,40,121,109,109,41,59,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,115,116,111,114,101,66,66,108,111,99,107,60,50,44,32,116,111,84,101,109,112,44,32,114,
101,109,77,44,32,50,62,40,66,95,97,114,114,44,32,76,68,66,44,32,66,95,116,101,109,112,44,32,76,68,66,95,44,32,121,109,109,44,32,114,101,109,77,95,41,59,10,32,32,32,32,125,10,32,32,32,32,101,108,115,101,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,
88,80,82,40,117,110,114,111,108,108,78,32,61,61,32,49,41,32,123,10,32,32,32,32,32,32,47,47,32,108,111,97,100,32,76,120,49,32,66,32,99,111,108,32,109,97,106,111,114,44,32,116,114,97,110,115,112,111,115,101,32,76,120,49,32,114,111,119,32,109,97,106,111,
114,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,108,111,97,100,66,66,108,111,99,107,60,49,44,32,116,111,84,101,109,112,44,32,114,101,109,77,44,32,49,62,40,66,95,97,114,114,44,32,76,68,66,44,32,66,95,116,101,109,
112,44,32,76,68,66,95,44,32,121,109,109,44,32,114,101,109,77,95,41,59,10,32,32,32,32,32,32,116,114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,116,114,97,110,115,112,111,115,101,76,120,76,60,48,62,40,121,109,109,41,59,10,32,32,32,32,32,32,116,
114,97,110,115,66,58,58,116,101,109,112,108,97,116,101,32,115,116,111,114,101,66,66,108,111,99,107,60,49,44,32,116,111,84,101,109,112,44,32,114,101,109,77,44,32,49,62,40,66,95,97,114,114,44,32,76,68,66,44,32,66,95,116,101,109,112,44,32,76,68,66,95,44,
32,121,109,109,44,32,114,101,109,77,95,41,59,10,32,32,32,32,125,10,32,32,125,10,125,59,10,10,47,42,42,10,32,42,32,85,110,114,111,108,108,115,32,102,111,114,32,116,114,105,83,111,108,118,101,75,101,114,110,101,108,10,32,42,10,32,42,32,73,100,101,97,58,
10,32,42,32,32,49,41,32,76,111,97,100,32,97,32,98,108,111,99,107,32,111,102,32,114,105,103,104,116,45,104,97,110,100,32,115,105,100,101,115,32,116,111,32,114,101,103,105,115,116,101,114,115,32,105,110,32,82,72,83,73,110,80,97,99,107,101,116,32,40,117,
115,105,110,103,32,108,111,97,100,82,72,83,41,46,10,32,42,32,32,50,41,32,68,111,32,116,114,105,97,110,103,117,108,97,114,32,115,111,108,118,101,32,119,105,116,104,32,82,72,83,73,110,80,97,99,107,101,116,32,97,110,100,32,97,32,115,109,97,108,108,32,98,
108,111,99,107,32,111,102,32,65,32,40,116,114,105,97,110,103,117,108,97,114,32,109,97,116,114,105,120,41,10,32,42,32,32,32,32,32,115,116,111,114,101,100,32,105,110,32,65,73,110,80,97,99,107,101,116,32,40,117,115,105,110,103,32,116,114,105,83,111,108,
118,101,77,105,99,114,111,75,101,114,110,101,108,41,46,10,32,42,32,32,51,41,32,83,116,111,114,101,32,102,105,110,97,108,32,114,101,115,117,108,116,115,32,40,105,110,32,97,118,120,32,114,101,103,105,115,116,101,114,115,41,32,98,97,99,107,32,105,110,116,
111,32,109,101,109,111,114,121,32,40,117,115,105,110,103,32,115,116,111,114,101,82,72,83,41,46,10,32,42,10,32,42,32,32,82,72,83,73,110,80,97,99,107,101,116,32,117,115,101,115,32,97,116,32,109,111,115,116,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,
85,77,95,65,67,67,32,97,118,120,32,114,101,103,105,115,116,101,114,115,32,97,110,100,32,65,73,110,80,97,99,107,101,116,32,117,115,101,115,32,97,116,32,109,111,115,116,10,32,42,32,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,32,114,
101,103,105,115,116,101,114,115,46,10,32,42,47,10,116,101,109,112,108,97,116,101,32,60,116,121,112,101,110,97,109,101,32,83,99,97,108,97,114,62,10,99,108,97,115,115,32,116,114,115,109,32,123,10,32,112,117,98,108,105,99,58,10,32,32,117,115,105,110,103,
32,118,101,99,32,61,32,116,121,112,101,110,97,109,101,32,115,116,100,58,58,99,111,110,100,105,116,105,111,110,97,108,60,115,116,100,58,58,105,115,95,115,97,109,101,60,83,99,97,108,97,114,44,32,102,108,111,97,116,62,58,58,118,97,108,117,101,44,32,118,
101,99,70,117,108,108,70,108,111,97,116,44,32,118,101,99,70,117,108,108,68,111,117,98,108,101,62,58,58,116,121,112,101,59,10,32,32,115,116,97,116,105,99,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,80,97,99,107,101,116,83,105,
122,101,32,61,32,112,97,99,107,101,116,95,116,114,97,105,116,115,60,83,99,97,108,97,114,62,58,58,115,105,122,101,59,10,10,32,32,47,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,10,32,32,32,42,
32,65,117,120,105,108,108,97,114,121,32,70,117,110,99,116,105,111,110,115,32,102,111,114,58,10,32,32,32,42,32,32,45,32,108,111,97,100,82,72,83,10,32,32,32,42,32,32,45,32,115,116,111,114,101,82,72,83,10,32,32,32,42,32,32,45,32,100,105,118,82,72,83,66,
121,68,105,97,103,10,32,32,32,42,32,32,45,32,117,112,100,97,116,101,82,72,83,10,32,32,32,42,32,32,45,32,116,114,105,83,111,108,118,101,77,105,99,114,111,75,101,114,110,101,108,10,32,32,32,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,
42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,47,10,32,32,47,42,42,10,32,32,32,42,32,97,117,120,95,108,111,97,100,82,72,83,10,32,32,32,42,10,32,32,32,42,32,50,45,68,32,117,110,114,111,108,108,10,32,32,32,42,32,32,32,32,32,32,102,111,114,40,115,116,97,
114,116,77,32,61,32,48,59,32,115,116,97,114,116,77,32,60,32,101,110,100,77,59,32,115,116,97,114,116,77,43,43,41,10,32,32,32,42,32,32,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,75,32,61,32,48,59,32,115,116,97,114,116,75,32,60,32,101,110,100,75,
59,32,115,116,97,114,116,75,43,43,41,10,32,32,32,42,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,98,111,111,108,32,105,115,70,87,68,83,111,108,118,101,44,32,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,100,
75,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,98,111,111,108,32,107,114,101,109,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,
95,116,60,40,99,111,117,110,116,101,114,32,62,32,48,41,62,32,97,117,120,95,108,111,97,100,82,72,83,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,66,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,80,97,99,107,101,116,66,108,111,99,
107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,65,67,67,62,32,38,82,72,83,73,110,80,97,99,107,101,116,44,32,105,110,116,54,52,95,116,32,114,101,109,32,61,32,48,41,32,123,10,32,32,32,32,99,111,110,115,116,101,120,112,114,
32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,61,32,101,110,100,77,32,42,32,101,110,100,75,32,45,32,99,111,117,110,116,101,114,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,
32,115,116,97,114,116,77,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,47,32,40,101,110,100,75,41,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,115,116,97,114,116,75,32,61,32,99,111,117,110,116,
101,114,82,101,118,101,114,115,101,32,37,32,101,110,100,75,59,10,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,112,97,99,107,101,116,73,110,100,101,120,32,61,32,115,116,97,114,116,77,32,42,32,101,110,100,75,32,43,32,
115,116,97,114,116,75,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,115,116,97,114,116,77,95,32,61,32,105,115,70,87,68,83,111,108,118,101,32,63,32,115,116,97,114,116,77,32,58,32,45,115,116,97,114,116,77,59,10,32,
32,32,32,99,111,110,115,116,32,105,110,116,54,52,95,116,32,114,104,115,73,110,100,101,120,32,61,32,40,115,116,97,114,116,75,32,42,32,80,97,99,107,101,116,83,105,122,101,41,32,43,32,115,116,97,114,116,77,95,32,42,32,76,68,66,59,10,32,32,32,32,69,73,71,
69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,107,114,101,109,41,32,123,10,32,32,32,32,32,32,82,72,83,73,110,80,97,99,107,101,116,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,93,32,61,32,112,108,111,97,100,117,60,118,101,99,
62,40,38,66,95,97,114,114,91,114,104,115,73,110,100,101,120,93,44,32,114,101,109,77,97,115,107,60,80,97,99,107,101,116,83,105,122,101,62,40,114,101,109,41,41,59,10,32,32,32,32,125,10,32,32,32,32,101,108,115,101,32,123,10,32,32,32,32,32,32,82,72,83,73,
110,80,97,99,107,101,116,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,93,32,61,32,112,108,111,97,100,117,60,118,101,99,62,40,38,66,95,97,114,114,91,114,104,115,73,110,100,101,120,93,41,59,10,32,32,32,32,125,10,32,32,32,32,97,117,
120,95,108,111,97,100,82,72,83,60,105,115,70,87,68,83,111,108,118,101,44,32,101,110,100,77,44,32,101,110,100,75,44,32,99,111,117,110,116,101,114,32,45,32,49,44,32,107,114,101,109,62,40,66,95,97,114,114,44,32,76,68,66,44,32,82,72,83,73,110,80,97,99,107,
101,116,44,32,114,101,109,41,59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,98,111,111,108,32,105,115,70,87,68,83,111,108,118,101,44,32,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,100,75,44,32,
105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,98,111,111,108,32,107,114,101,109,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,
60,40,99,111,117,110,116,101,114,32,60,61,32,48,41,62,32,97,117,120,95,108,111,97,100,82,72,83,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,66,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,80,97,99,107,101,116,66,108,111,99,107,
60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,65,67,67,62,32,38,82,72,83,73,110,80,97,99,107,101,116,44,32,105,110,116,54,52,95,116,32,114,101,109,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,
86,65,82,73,65,66,76,69,40,66,95,97,114,114,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,76,68,66,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,82,72,83,73,110,80,97,99,
107,101,116,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,114,101,109,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,97,117,120,95,115,116,111,114,101,82,72,83,10,32,32,32,42,10,32,32,32,42,32,50,45,
68,32,117,110,114,111,108,108,10,32,32,32,42,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,77,32,61,32,48,59,32,115,116,97,114,116,77,32,60,32,101,110,100,77,59,32,115,116,97,114,116,77,43,43,41,10,32,32,32,42,32,32,32,32,32,32,32,32,102,111,114,
40,115,116,97,114,116,75,32,61,32,48,59,32,115,116,97,114,116,75,32,60,32,101,110,100,75,59,32,115,116,97,114,116,75,43,43,41,10,32,32,32,42,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,98,111,111,108,32,105,115,70,87,68,83,111,108,118,101,44,
32,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,100,75,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,98,111,111,108,32,107,114,101,109,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,
65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,62,32,48,41,62,32,97,117,120,95,115,116,111,114,101,82,72,83,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,66,
95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,65,67,67,62,32,38,82,72,83,73,110,80,97,99,107,101,116,44,32,105,110,116,54,
52,95,116,32,114,101,109,32,61,32,48,41,32,123,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,61,32,101,110,100,77,32,42,32,101,110,100,75,32,45,32,99,111,117,
110,116,101,114,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,115,116,97,114,116,77,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,47,32,40,101,110,100,75,41,59,10,32,32,32,32,99,111,110,115,116,
101,120,112,114,32,105,110,116,54,52,95,116,32,115,116,97,114,116,75,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,37,32,101,110,100,75,59,10,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,112,97,
99,107,101,116,73,110,100,101,120,32,61,32,115,116,97,114,116,77,32,42,32,101,110,100,75,32,43,32,115,116,97,114,116,75,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,115,116,97,114,116,77,95,32,61,32,105,115,70,87,
68,83,111,108,118,101,32,63,32,115,116,97,114,116,77,32,58,32,45,115,116,97,114,116,77,59,10,32,32,32,32,99,111,110,115,116,32,105,110,116,54,52,95,116,32,114,104,115,73,110,100,101,120,32,61,32,40,115,116,97,114,116,75,32,42,32,80,97,99,107,101,116,
83,105,122,101,41,32,43,32,115,116,97,114,116,77,95,32,42,32,76,68,66,59,10,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,107,114,101,109,41,32,123,10,32,32,32,32,32,32,112,115,116,111,114,101,117,60,83,99,97,108,97,114,62,40,38,
66,95,97,114,114,91,114,104,115,73,110,100,101,120,93,44,32,82,72,83,73,110,80,97,99,107,101,116,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,93,44,32,114,101,109,77,97,115,107,60,80,97,99,107,101,116,83,105,122,101,62,40,114,
101,109,41,41,59,10,32,32,32,32,125,10,32,32,32,32,101,108,115,101,32,123,10,32,32,32,32,32,32,112,115,116,111,114,101,117,60,83,99,97,108,97,114,62,40,38,66,95,97,114,114,91,114,104,115,73,110,100,101,120,93,44,32,82,72,83,73,110,80,97,99,107,101,116,
46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,93,41,59,10,32,32,32,32,125,10,32,32,32,32,97,117,120,95,115,116,111,114,101,82,72,83,60,105,115,70,87,68,83,111,108,118,101,44,32,101,110,100,77,44,32,101,110,100,75,44,32,99,111,117,
110,116,101,114,32,45,32,49,44,32,107,114,101,109,62,40,66,95,97,114,114,44,32,76,68,66,44,32,82,72,83,73,110,80,97,99,107,101,116,44,32,114,101,109,41,59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,98,111,111,108,32,105,115,70,87,68,
83,111,108,118,101,44,32,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,100,75,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,98,111,111,108,32,107,114,101,109,62,10,32,32,115,116,97,116,105,99,
32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,60,61,32,48,41,62,32,97,117,120,95,115,116,111,114,101,82,72,83,40,10,32,32,32,32,32,32,83,99,
97,108,97,114,32,42,66,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,65,67,67,62,32,38,82,72,83,73,110,80,97,99,107,101,
116,44,32,105,110,116,54,52,95,116,32,114,101,109,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,66,95,97,114,114,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,
69,40,76,68,66,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,82,72,83,73,110,80,97,99,107,101,116,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,114,101,109,41,59,10,32,
32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,97,117,120,95,100,105,118,82,72,83,66,121,68,105,97,103,10,32,32,32,42,10,32,32,32,42,32,99,117,114,114,77,32,109,97,121,32,98,101,32,45,49,44,32,40,99,117,114,114,77,32,62,61,48,41,32,105,110,32,101,110,
97,98,108,101,95,105,102,32,99,104,101,99,107,115,32,102,111,114,32,116,104,105,115,10,32,32,32,42,10,32,32,32,42,32,49,45,68,32,117,110,114,111,108,108,10,32,32,32,42,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,75,32,61,32,48,59,32,115,116,97,
114,116,75,32,60,32,101,110,100,75,59,32,115,116,97,114,116,75,43,43,41,10,32,32,32,42,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,99,117,114,114,77,44,32,105,110,116,54,52,95,116,32,101,110,100,75,44,32,105,110,116,
54,52,95,116,32,99,111,117,110,116,101,114,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,62,32,48,32,38,38,
32,99,117,114,114,77,32,62,61,32,48,41,62,32,97,117,120,95,100,105,118,82,72,83,66,121,68,105,97,103,40,10,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,65,67,67,62,
32,38,82,72,83,73,110,80,97,99,107,101,116,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,32,38,65,73,110,80,97,99,107,101,116,41,32,123,10,32,32,32,32,99,111,110,115,
116,101,120,112,114,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,61,32,101,110,100,75,32,45,32,99,111,117,110,116,101,114,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,115,
116,97,114,116,75,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,59,10,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,112,97,99,107,101,116,73,110,100,101,120,32,61,32,99,117,114,114,77,32,42,32,101,110,
100,75,32,43,32,115,116,97,114,116,75,59,10,32,32,32,32,82,72,83,73,110,80,97,99,107,101,116,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,93,32,61,32,112,109,117,108,40,65,73,110,80,97,99,107,101,116,46,112,97,99,107,101,116,91,
99,117,114,114,77,93,44,32,82,72,83,73,110,80,97,99,107,101,116,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,93,41,59,10,32,32,32,32,97,117,120,95,100,105,118,82,72,83,66,121,68,105,97,103,60,99,117,114,114,77,44,32,101,110,100,
75,44,32,99,111,117,110,116,101,114,32,45,32,49,62,40,82,72,83,73,110,80,97,99,107,101,116,44,32,65,73,110,80,97,99,107,101,116,41,59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,99,117,114,114,77,44,32,105,
110,116,54,52,95,116,32,101,110,100,75,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,
116,60,33,40,99,111,117,110,116,101,114,32,62,32,48,32,38,38,32,99,117,114,114,77,32,62,61,32,48,41,62,32,97,117,120,95,100,105,118,82,72,83,66,121,68,105,97,103,40,10,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,
71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,65,67,67,62,32,38,82,72,83,73,110,80,97,99,107,101,116,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,32,38,65,73,110,
80,97,99,107,101,116,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,82,72,83,73,110,80,97,99,107,101,116,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,65,73,110,80,97,
99,107,101,116,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,97,117,120,95,117,112,100,97,116,101,82,72,83,10,32,32,32,42,10,32,32,32,42,32,50,45,68,32,117,110,114,111,108,108,10,32,32,32,42,32,32,32,32,32,32,102,111,114,40,115,116,97,114,
116,77,32,61,32,105,110,105,116,77,59,32,115,116,97,114,116,77,32,60,32,101,110,100,77,59,32,115,116,97,114,116,77,43,43,41,10,32,32,32,42,32,32,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,75,32,61,32,48,59,32,115,116,97,114,116,75,32,60,32,101,
110,100,75,59,32,115,116,97,114,116,75,43,43,41,10,32,32,32,42,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,98,111,111,108,32,105,115,65,82,111,119,77,97,106,111,114,44,32,98,111,111,108,32,105,115,70,87,68,83,111,108,118,101,44,32,98,111,111,
108,32,105,115,85,110,105,116,68,105,97,103,44,32,105,110,116,54,52,95,116,32,105,110,105,116,77,44,32,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,100,75,44,10,32,32,32,32,32,32,32,32,32,32,32,32,105,110,116,54,
52,95,116,32,99,111,117,110,116,101,114,44,32,105,110,116,54,52,95,116,32,99,117,114,114,101,110,116,77,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,
95,116,60,40,99,111,117,110,116,101,114,32,62,32,48,41,62,32,97,117,120,95,117,112,100,97,116,101,82,72,83,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,65,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,65,44,32,80,97,99,107,101,116,66,108,
111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,65,67,67,62,32,38,82,72,83,73,110,80,97,99,107,101,116,44,10,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,
95,77,65,88,95,78,85,77,95,82,79,87,62,32,38,65,73,110,80,97,99,107,101,116,41,32,123,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,61,32,40,101,110,100,77,32,
45,32,105,110,105,116,77,41,32,42,32,101,110,100,75,32,45,32,99,111,117,110,116,101,114,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,115,116,97,114,116,77,32,61,32,105,110,105,116,77,32,43,32,99,111,117,110,116,
101,114,82,101,118,101,114,115,101,32,47,32,40,101,110,100,75,41,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,115,116,97,114,116,75,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,37,32,101,110,
100,75,59,10,10,32,32,32,32,47,47,32,70,111,114,32,101,97,99,104,32,114,111,119,32,111,102,32,65,44,32,102,105,114,115,116,32,117,112,100,97,116,101,32,97,108,108,32,99,111,114,114,101,115,112,111,110,100,105,110,103,32,82,72,83,10,32,32,32,32,99,111,
110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,112,97,99,107,101,116,73,110,100,101,120,32,61,32,115,116,97,114,116,77,32,42,32,101,110,100,75,32,43,32,115,116,97,114,116,75,59,10,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,
80,82,40,99,117,114,114,101,110,116,77,32,62,32,48,41,32,123,10,32,32,32,32,32,32,82,72,83,73,110,80,97,99,107,101,116,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,93,32,61,10,32,32,32,32,32,32,32,32,32,32,112,110,109,97,100,100,
40,65,73,110,80,97,99,107,101,116,46,112,97,99,107,101,116,91,115,116,97,114,116,77,93,44,32,82,72,83,73,110,80,97,99,107,101,116,46,112,97,99,107,101,116,91,40,99,117,114,114,101,110,116,77,32,45,32,49,41,32,42,32,101,110,100,75,32,43,32,115,116,97,
114,116,75,93,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,82,72,83,73,110,80,97,99,107,101,116,46,112,97,99,107,101,116,91,112,97,99,107,101,116,73,110,100,101,120,93,41,59,10,32,32,32,32,125,10,10,32,32,32,32,69,73,71,69,78,95,73,70,95,
67,79,78,83,84,69,88,80,82,40,115,116,97,114,116,75,32,61,61,32,101,110,100,75,32,45,32,49,41,32,123,10,32,32,32,32,32,32,47,47,32,79,110,99,101,32,97,108,108,32,82,72,83,32,102,111,114,32,112,114,101,118,105,111,117,115,32,114,111,119,32,111,102,32,
65,32,105,115,32,117,112,100,97,116,101,100,44,32,119,101,32,98,114,111,97,100,99,97,115,116,32,116,104,101,32,110,101,120,116,32,101,108,101,109,101,110,116,32,105,110,32,116,104,101,32,99,111,108,117,109,110,32,65,95,123,105,44,32,99,117,114,114,101,
110,116,77,125,46,10,32,32,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,115,116,97,114,116,77,32,61,61,32,99,117,114,114,101,110,116,77,32,38,38,32,33,105,115,85,110,105,116,68,105,97,103,41,32,123,10,32,32,32,32,32,32,32,32,47,
47,32,73,102,32,100,105,97,103,111,110,97,108,32,105,115,32,110,111,116,32,117,110,105,116,44,32,119,101,32,98,114,111,97,100,99,97,115,116,32,114,101,99,105,112,114,111,99,97,108,115,32,111,102,32,100,105,97,103,111,110,97,108,115,32,65,105,110,80,97,
99,107,101,116,46,112,97,99,107,101,116,91,99,117,114,114,101,110,116,77,93,46,10,32,32,32,32,32,32,32,32,47,47,32,84,104,105,115,32,119,105,108,108,32,98,101,32,117,115,101,100,32,105,110,32,100,105,118,82,72,83,66,121,68,105,97,103,10,32,32,32,32,32,
32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,105,115,70,87,68,83,111,108,118,101,41,10,32,32,32,32,32,32,32,32,65,73,110,80,97,99,107,101,116,46,112,97,99,107,101,116,91,99,117,114,114,101,110,116,77,93,32,61,32,112,115,101,116,49,
60,118,101,99,62,40,83,99,97,108,97,114,40,49,41,32,47,32,65,95,97,114,114,91,105,100,65,60,105,115,65,82,111,119,77,97,106,111,114,62,40,99,117,114,114,101,110,116,77,44,32,99,117,114,114,101,110,116,77,44,32,76,68,65,41,93,41,59,10,32,32,32,32,32,32,
32,32,101,108,115,101,32,65,73,110,80,97,99,107,101,116,46,112,97,99,107,101,116,91,99,117,114,114,101,110,116,77,93,32,61,32,112,115,101,116,49,60,118,101,99,62,40,83,99,97,108,97,114,40,49,41,32,47,32,65,95,97,114,114,91,105,100,65,60,105,115,65,82,
111,119,77,97,106,111,114,62,40,45,99,117,114,114,101,110,116,77,44,32,45,99,117,114,114,101,110,116,77,44,32,76,68,65,41,93,41,59,10,32,32,32,32,32,32,125,10,32,32,32,32,32,32,101,108,115,101,32,123,10,32,32,32,32,32,32,32,32,47,47,32,66,114,111,97,
100,99,97,115,116,32,110,101,120,116,32,111,102,102,32,100,105,97,103,111,110,97,108,32,101,108,101,109,101,110,116,32,111,102,32,65,10,32,32,32,32,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,105,115,70,87,68,83,111,108,118,101,
41,10,32,32,32,32,32,32,32,32,65,73,110,80,97,99,107,101,116,46,112,97,99,107,101,116,91,115,116,97,114,116,77,93,32,61,32,112,115,101,116,49,60,118,101,99,62,40,65,95,97,114,114,91,105,100,65,60,105,115,65,82,111,119,77,97,106,111,114,62,40,115,116,
97,114,116,77,44,32,99,117,114,114,101,110,116,77,44,32,76,68,65,41,93,41,59,10,32,32,32,32,32,32,32,32,101,108,115,101,32,65,73,110,80,97,99,107,101,116,46,112,97,99,107,101,116,91,115,116,97,114,116,77,93,32,61,32,112,115,101,116,49,60,118,101,99,62,
40,65,95,97,114,114,91,105,100,65,60,105,115,65,82,111,119,77,97,106,111,114,62,40,45,115,116,97,114,116,77,44,32,45,99,117,114,114,101,110,116,77,44,32,76,68,65,41,93,41,59,10,32,32,32,32,32,32,125,10,32,32,32,32,125,10,10,32,32,32,32,97,117,120,95,
117,112,100,97,116,101,82,72,83,60,105,115,65,82,111,119,77,97,106,111,114,44,32,105,115,70,87,68,83,111,108,118,101,44,32,105,115,85,110,105,116,68,105,97,103,44,32,105,110,105,116,77,44,32,101,110,100,77,44,32,101,110,100,75,44,32,99,111,117,110,116,
101,114,32,45,32,49,44,32,99,117,114,114,101,110,116,77,62,40,10,32,32,32,32,32,32,32,32,65,95,97,114,114,44,32,76,68,65,44,32,82,72,83,73,110,80,97,99,107,101,116,44,32,65,73,110,80,97,99,107,101,116,41,59,10,32,32,125,10,10,32,32,116,101,109,112,108,
97,116,101,32,60,98,111,111,108,32,105,115,65,82,111,119,77,97,106,111,114,44,32,98,111,111,108,32,105,115,70,87,68,83,111,108,118,101,44,32,98,111,111,108,32,105,115,85,110,105,116,68,105,97,103,44,32,105,110,116,54,52,95,116,32,105,110,105,116,77,44,
32,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,100,75,44,10,32,32,32,32,32,32,32,32,32,32,32,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,105,110,116,54,52,95,116,32,99,117,114,114,101,110,116,
77,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,60,61,32,48,41,62,32,97,117,120,95,117,112,100,97,116,101,
82,72,83,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,65,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,65,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,65,67,67,62,
32,38,82,72,83,73,110,80,97,99,107,101,116,44,10,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,32,38,65,73,110,80,97,99,107,101,116,41,32,123,10,32,32,32,
32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,65,95,97,114,114,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,76,68,65,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,
73,65,66,76,69,40,82,72,83,73,110,80,97,99,107,101,116,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,65,73,110,80,97,99,107,101,116,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,97,117,120,95,116,114,
105,83,111,108,118,101,114,77,105,99,114,111,75,101,114,110,101,108,10,32,32,32,42,10,32,32,32,42,32,49,45,68,32,117,110,114,111,108,108,10,32,32,32,42,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,77,32,61,32,48,59,32,115,116,97,114,116,77,32,
60,32,101,110,100,77,59,32,115,116,97,114,116,77,43,43,41,10,32,32,32,42,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,98,111,111,108,32,105,115,65,82,111,119,77,97,106,111,114,44,32,98,111,111,108,32,105,115,70,87,68,83,111,108,118,101,44,32,98,
111,111,108,32,105,115,85,110,105,116,68,105,97,103,44,32,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,105,110,116,54,52,95,116,32,110,117,109,75,62,10,32,32,115,116,97,116,105,99,32,69,
73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,62,32,48,41,62,32,97,117,120,95,116,114,105,83,111,108,118,101,77,105,99,114,111,75,101,114,110,101,
108,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,65,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,65,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,65,67,67,62,32,38,
82,72,83,73,110,80,97,99,107,101,116,44,10,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,32,38,65,73,110,80,97,99,107,101,116,41,32,123,10,32,32,32,32,99,
111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,61,32,101,110,100,77,32,45,32,99,111,117,110,116,101,114,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,
95,116,32,115,116,97,114,116,77,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,59,10,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,99,117,114,114,101,110,116,77,32,61,32,115,116,97,114,116,77,59,10,32,
32,32,32,47,47,32,68,105,118,105,100,101,115,32,116,104,101,32,114,105,103,104,116,45,104,97,110,100,32,115,105,100,101,32,105,110,32,114,111,119,32,115,116,97,114,116,77,44,32,98,121,32,100,105,103,111,110,97,108,32,118,97,108,117,101,32,111,102,32,
65,10,32,32,32,32,47,47,32,98,114,111,97,100,99,97,115,116,101,100,32,116,111,32,65,73,110,80,97,99,107,101,116,46,112,97,99,107,101,116,91,115,116,97,114,116,77,45,49,93,32,105,110,32,116,104,101,32,112,114,101,118,105,111,117,115,32,105,116,101,114,
97,116,105,111,110,46,10,32,32,32,32,47,47,10,32,32,32,32,47,47,32,87,105,116,104,111,117,116,32,34,105,102,32,99,111,110,115,116,101,120,112,114,34,32,116,104,101,32,99,111,109,112,105,108,101,114,32,105,110,115,116,97,110,116,105,97,116,101,115,32,
116,104,101,32,99,97,115,101,32,60,45,49,44,32,110,117,109,75,62,10,32,32,32,32,47,47,32,116,104,105,115,32,105,115,32,104,97,110,100,108,101,100,32,119,105,116,104,32,101,110,97,98,108,101,95,105,102,32,116,111,32,112,114,101,118,101,110,116,32,111,
117,116,45,111,102,45,98,111,117,110,100,32,119,97,114,110,105,110,103,115,10,32,32,32,32,47,47,32,102,114,111,109,32,116,104,101,32,99,111,109,112,105,108,101,114,10,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,33,105,115,85,
110,105,116,68,105,97,103,32,38,38,32,115,116,97,114,116,77,32,62,32,48,41,10,32,32,32,32,116,114,115,109,58,58,116,101,109,112,108,97,116,101,32,100,105,118,82,72,83,66,121,68,105,97,103,60,115,116,97,114,116,77,32,45,32,49,44,32,110,117,109,75,62,40,
82,72,83,73,110,80,97,99,107,101,116,44,32,65,73,110,80,97,99,107,101,116,41,59,10,10,32,32,32,32,47,47,32,65,102,116,101,114,32,100,105,118,105,115,105,111,110,44,32,116,104,101,32,114,104,115,32,99,111,114,114,101,115,112,111,110,100,105,110,103,32,
116,111,32,115,117,98,115,101,113,117,101,110,116,32,114,111,119,115,32,111,102,32,65,32,99,97,110,32,98,101,32,112,97,114,116,105,97,108,108,121,32,117,112,100,97,116,101,100,10,32,32,32,32,47,47,32,87,101,32,97,108,115,111,32,98,114,111,97,100,99,97,
115,116,32,116,104,101,32,114,101,99,105,112,114,111,99,97,108,32,111,102,32,116,104,101,32,110,101,120,116,32,100,105,97,103,111,110,97,108,32,116,111,32,65,73,110,80,97,99,107,101,116,46,112,97,99,107,101,116,91,99,117,114,114,101,110,116,77,93,32,
40,105,102,32,110,101,101,100,101,100,41,10,32,32,32,32,47,47,32,116,111,32,98,101,32,117,115,101,100,32,105,110,32,116,104,101,32,110,101,120,116,32,105,116,101,114,97,116,105,111,110,46,10,32,32,32,32,116,114,115,109,58,58,116,101,109,112,108,97,116,
101,32,117,112,100,97,116,101,82,72,83,60,105,115,65,82,111,119,77,97,106,111,114,44,32,105,115,70,87,68,83,111,108,118,101,44,32,105,115,85,110,105,116,68,105,97,103,44,32,115,116,97,114,116,77,44,32,101,110,100,77,44,32,110,117,109,75,44,32,99,117,
114,114,101,110,116,77,62,40,65,95,97,114,114,44,32,76,68,65,44,32,82,72,83,73,110,80,97,99,107,101,116,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,65,73,110,80,97,99,107,101,116,41,59,10,10,32,32,32,32,47,47,32,72,97,110,100,108,101,32,100,105,118,105,
115,105,111,110,32,102,111,114,32,116,104,101,32,82,72,83,32,99,111,114,114,101,115,112,111,110,100,105,110,103,32,116,111,32,116,104,101,32,102,105,110,97,108,32,114,111,119,32,111,102,32,65,46,10,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,
69,88,80,82,40,33,105,115,85,110,105,116,68,105,97,103,32,38,38,32,115,116,97,114,116,77,32,61,61,32,101,110,100,77,32,45,32,49,41,10,32,32,32,32,116,114,115,109,58,58,116,101,109,112,108,97,116,101,32,100,105,118,82,72,83,66,121,68,105,97,103,60,115,
116,97,114,116,77,44,32,110,117,109,75,62,40,82,72,83,73,110,80,97,99,107,101,116,44,32,65,73,110,80,97,99,107,101,116,41,59,10,10,32,32,32,32,97,117,120,95,116,114,105,83,111,108,118,101,77,105,99,114,111,75,101,114,110,101,108,60,105,115,65,82,111,
119,77,97,106,111,114,44,32,105,115,70,87,68,83,111,108,118,101,44,32,105,115,85,110,105,116,68,105,97,103,44,32,101,110,100,77,44,32,99,111,117,110,116,101,114,32,45,32,49,44,32,110,117,109,75,62,40,65,95,97,114,114,44,32,76,68,65,44,32,82,72,83,73,
110,80,97,99,107,101,116,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,65,73,110,80,97,99,107,101,116,41,59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,98,111,111,108,32,105,115,65,82,111,119,77,97,106,111,114,44,32,98,111,111,108,32,105,115,70,87,68,83,
111,108,118,101,44,32,98,111,111,108,32,105,115,85,110,105,116,68,105,97,103,44,32,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,105,110,116,54,52,95,116,32,110,117,109,75,62,10,32,32,115,
116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,60,61,32,48,41,62,32,97,117,120,95,116,114,105,83,111,108,118,101,77,105,99,
114,111,75,101,114,110,101,108,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,65,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,65,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,
85,77,95,65,67,67,62,32,38,82,72,83,73,110,80,97,99,107,101,116,44,10,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,32,38,65,73,110,80,97,99,107,101,116,
41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,65,95,97,114,114,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,76,68,65,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,
83,69,68,95,86,65,82,73,65,66,76,69,40,82,72,83,73,110,80,97,99,107,101,116,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,65,73,110,80,97,99,107,101,116,41,59,10,32,32,125,10,10,32,32,47,42,42,42,42,42,42,42,42,
42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,10,32,32,32,42,32,87,114,97,112,112,101,114,115,32,102,111,114,32,97,117,120,95,88,88,88,88,32,116,111,32,
104,105,100,101,32,99,111,117,110,116,101,114,32,112,97,114,97,109,101,116,101,114,10,32,32,32,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,
42,42,42,42,47,10,10,32,32,47,42,42,10,32,32,32,42,32,76,111,97,100,32,101,110,100,77,120,101,110,100,75,32,98,108,111,99,107,32,111,102,32,66,32,116,111,32,82,72,83,73,110,80,97,99,107,101,116,10,32,32,32,42,32,77,97,115,107,101,100,32,108,111,97,100,
115,32,97,114,101,32,117,115,101,100,32,102,111,114,32,99,97,115,101,115,32,119,104,101,114,101,32,101,110,100,75,32,105,115,32,110,111,116,32,97,32,109,117,108,116,105,112,108,101,32,111,102,32,80,97,99,107,101,116,83,105,122,101,10,32,32,32,42,47,10,
32,32,116,101,109,112,108,97,116,101,32,60,98,111,111,108,32,105,115,70,87,68,83,111,108,118,101,44,32,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,100,75,44,32,98,111,111,108,32,107,114,101,109,32,61,32,102,97,
108,115,101,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,105,100,32,108,111,97,100,82,72,83,40,83,99,97,108,97,114,32,42,66,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,66,44,10,32,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,65,67,67,62,32,
38,82,72,83,73,110,80,97,99,107,101,116,44,32,105,110,116,54,52,95,116,32,114,101,109,32,61,32,48,41,32,123,10,32,32,32,32,97,117,120,95,108,111,97,100,82,72,83,60,105,115,70,87,68,83,111,108,118,101,44,32,101,110,100,77,44,32,101,110,100,75,44,32,101,
110,100,77,32,42,32,101,110,100,75,44,32,107,114,101,109,62,40,66,95,97,114,114,44,32,76,68,66,44,32,82,72,83,73,110,80,97,99,107,101,116,44,32,114,101,109,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,76,111,97,100,32,101,110,100,77,120,
101,110,100,75,32,98,108,111,99,107,32,111,102,32,66,32,116,111,32,82,72,83,73,110,80,97,99,107,101,116,10,32,32,32,42,32,77,97,115,107,101,100,32,108,111,97,100,115,32,97,114,101,32,117,115,101,100,32,102,111,114,32,99,97,115,101,115,32,119,104,101,
114,101,32,101,110,100,75,32,105,115,32,110,111,116,32,97,32,109,117,108,116,105,112,108,101,32,111,102,32,80,97,99,107,101,116,83,105,122,101,10,32,32,32,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,98,111,111,108,32,105,115,70,87,68,83,111,108,
118,101,44,32,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,100,75,44,32,98,111,111,108,32,107,114,101,109,32,61,32,102,97,108,115,101,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,
73,78,76,73,78,69,32,118,111,105,100,32,115,116,111,114,101,82,72,83,40,83,99,97,108,97,114,32,42,66,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,66,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,65,67,67,62,32,38,82,72,83,73,110,80,97,99,107,101,116,44,32,105,110,116,54,52,95,116,32,114,101,
109,32,61,32,48,41,32,123,10,32,32,32,32,97,117,120,95,115,116,111,114,101,82,72,83,60,105,115,70,87,68,83,111,108,118,101,44,32,101,110,100,77,44,32,101,110,100,75,44,32,101,110,100,77,32,42,32,101,110,100,75,44,32,107,114,101,109,62,40,66,95,97,114,
114,44,32,76,68,66,44,32,82,72,83,73,110,80,97,99,107,101,116,44,32,114,101,109,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,79,110,108,121,32,117,115,101,100,32,105,102,32,84,114,105,97,110,103,117,108,97,114,32,109,97,116,114,105,120,32,
104,97,115,32,110,111,110,45,117,110,105,116,32,100,105,97,103,111,110,97,108,32,118,97,108,117,101,115,10,32,32,32,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,99,117,114,114,77,44,32,105,110,116,54,52,95,116,32,101,
110,100,75,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,105,100,32,100,105,118,82,72,83,66,121,68,105,97,103,40,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,
86,88,95,77,65,88,95,78,85,77,95,65,67,67,62,32,38,82,72,83,73,110,80,97,99,107,101,116,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,80,97,99,107,101,
116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,32,38,65,73,110,80,97,99,107,101,116,41,32,123,10,32,32,32,32,97,117,120,95,100,105,118,82,72,83,66,121,68,105,97,103,60,99,117,114,114,77,44,
32,101,110,100,75,44,32,101,110,100,75,62,40,82,72,83,73,110,80,97,99,107,101,116,44,32,65,73,110,80,97,99,107,101,116,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,85,112,100,97,116,101,32,114,105,103,104,116,45,104,97,110,100,32,115,105,
100,101,115,32,40,115,116,111,114,101,100,32,105,110,32,97,118,120,32,114,101,103,105,115,116,101,114,115,41,10,32,32,32,42,32,84,114,97,118,101,114,115,105,110,103,32,97,108,111,110,103,32,116,104,101,32,99,111,108,117,109,110,32,65,95,123,105,44,99,
117,114,114,101,110,116,77,125,44,32,119,104,101,114,101,32,99,117,114,114,101,110,116,77,32,60,61,32,105,32,60,61,32,101,110,100,77,44,32,97,110,100,32,98,114,111,97,100,99,97,115,116,105,110,103,32,101,97,99,104,32,118,97,108,117,101,32,116,111,32,
65,73,110,80,97,99,107,101,116,46,10,32,32,32,42,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,98,111,111,108,32,105,115,65,82,111,119,77,97,106,111,114,44,32,98,111,111,108,32,105,115,70,87,68,83,111,108,118,101,44,32,98,111,111,108,32,105,115,
85,110,105,116,68,105,97,103,44,32,105,110,116,54,52,95,116,32,115,116,97,114,116,77,44,32,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,100,75,44,10,32,32,32,32,32,32,32,32,32,32,32,32,105,110,116,54,52,95,116,
32,99,117,114,114,101,110,116,77,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,105,100,32,117,112,100,97,116,101,82,72,83,40,83,99,97,108,97,114,32,42,65,95,97,114,114,44,32,105,110,116,54,52,
95,116,32,76,68,65,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,
88,95,78,85,77,95,65,67,67,62,32,38,82,72,83,73,110,80,97,99,107,101,116,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,
60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,62,32,38,65,73,110,80,97,99,107,101,116,41,32,123,10,32,32,32,32,97,117,120,95,117,112,100,97,116,101,82,72,83,60,105,115,65,82,111,119,77,97,106,111,114,44,32,105,115,
70,87,68,83,111,108,118,101,44,32,105,115,85,110,105,116,68,105,97,103,44,32,115,116,97,114,116,77,44,32,101,110,100,77,44,32,101,110,100,75,44,32,40,101,110,100,77,32,45,32,115,116,97,114,116,77,41,32,42,32,101,110,100,75,44,32,99,117,114,114,101,110,
116,77,62,40,10,32,32,32,32,32,32,32,32,65,95,97,114,114,44,32,76,68,65,44,32,82,72,83,73,110,80,97,99,107,101,116,44,32,65,73,110,80,97,99,107,101,116,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,101,110,100,77,58,32,100,105,109,101,110,
115,105,111,110,32,111,102,32,65,46,32,49,32,60,61,32,101,110,100,77,32,60,61,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,82,79,87,10,32,32,32,42,32,110,117,109,75,58,32,110,117,109,98,101,114,32,111,102,32,97,118,120,32,114,101,103,105,
115,116,101,114,115,32,116,111,32,117,115,101,32,102,111,114,32,101,97,99,104,32,114,111,119,32,111,102,32,66,32,40,101,120,32,102,112,51,50,58,32,52,56,32,114,104,115,32,61,62,32,51,32,97,118,120,32,114,101,103,32,117,115,101,100,41,46,32,49,32,60,61,
32,101,110,100,75,32,60,61,32,51,46,10,32,32,32,42,32,105,115,70,87,68,83,111,108,118,101,58,32,116,114,117,101,32,61,62,32,102,111,114,119,97,114,100,32,115,117,98,115,116,105,116,117,116,105,111,110,44,32,102,97,108,115,101,32,61,62,32,98,97,99,107,
119,97,114,100,115,32,115,117,98,115,116,105,116,117,116,105,111,110,10,32,32,32,42,32,105,115,85,110,105,116,68,105,97,103,58,32,116,114,117,101,32,61,62,32,116,114,105,97,110,103,117,108,97,114,32,109,97,116,114,105,120,32,104,97,115,32,117,110,105,
116,32,100,105,97,103,111,110,97,108,46,10,32,32,32,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,98,111,111,108,32,105,115,65,82,111,119,77,97,106,111,114,44,32,98,111,111,108,32,105,115,70,87,68,83,111,108,118,101,44,32,98,111,111,108,32,105,
115,85,110,105,116,68,105,97,103,44,32,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,110,117,109,75,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,105,100,32,116,
114,105,83,111,108,118,101,77,105,99,114,111,75,101,114,110,101,108,40,83,99,97,108,97,114,32,42,65,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,65,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,65,88,95,78,85,77,95,65,67,67,62,32,38,82,72,83,73,110,80,97,99,107,101,116,44,10,32,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,86,88,95,77,
65,88,95,78,85,77,95,82,79,87,62,32,38,65,73,110,80,97,99,107,101,116,41,32,123,10,32,32,32,32,115,116,97,116,105,99,95,97,115,115,101,114,116,40,110,117,109,75,32,62,61,32,49,32,38,38,32,110,117,109,75,32,60,61,32,51,44,32,34,110,117,109,75,32,111,117,
116,32,111,102,32,114,97,110,103,101,34,41,59,10,32,32,32,32,97,117,120,95,116,114,105,83,111,108,118,101,77,105,99,114,111,75,101,114,110,101,108,60,105,115,65,82,111,119,77,97,106,111,114,44,32,105,115,70,87,68,83,111,108,118,101,44,32,105,115,85,110,
105,116,68,105,97,103,44,32,101,110,100,77,44,32,101,110,100,77,44,32,110,117,109,75,62,40,65,95,97,114,114,44,32,76,68,65,44,32,82,72,83,73,110,80,97,99,107,101,116,44,32,65,73,110,80,97,99,107,101,116,41,59,10,32,32,125,10,125,59,10,10,47,42,42,10,
32,42,32,85,110,114,111,108,108,115,32,102,111,114,32,103,101,109,109,32,107,101,114,110,101,108,10,32,42,10,32,42,32,105,115,65,100,100,58,32,116,114,117,101,32,61,62,32,67,32,43,61,32,65,42,66,44,32,102,97,108,115,101,32,61,62,32,67,32,45,61,32,65,
42,66,10,32,42,47,10,116,101,109,112,108,97,116,101,32,60,116,121,112,101,110,97,109,101,32,83,99,97,108,97,114,44,32,98,111,111,108,32,105,115,65,100,100,62,10,99,108,97,115,115,32,103,101,109,109,32,123,10,32,112,117,98,108,105,99,58,10,32,32,117,115,
105,110,103,32,118,101,99,32,61,32,116,121,112,101,110,97,109,101,32,115,116,100,58,58,99,111,110,100,105,116,105,111,110,97,108,60,115,116,100,58,58,105,115,95,115,97,109,101,60,83,99,97,108,97,114,44,32,102,108,111,97,116,62,58,58,118,97,108,117,101,
44,32,118,101,99,70,117,108,108,70,108,111,97,116,44,32,118,101,99,70,117,108,108,68,111,117,98,108,101,62,58,58,116,121,112,101,59,10,32,32,115,116,97,116,105,99,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,80,97,99,107,101,116,
83,105,122,101,32,61,32,112,97,99,107,101,116,95,116,114,97,105,116,115,60,83,99,97,108,97,114,62,58,58,115,105,122,101,59,10,10,32,32,47,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,10,32,32,
32,42,32,65,117,120,105,108,108,97,114,121,32,70,117,110,99,116,105,111,110,115,32,102,111,114,58,10,32,32,32,42,32,32,45,32,115,101,116,122,101,114,111,10,32,32,32,42,32,32,45,32,117,112,100,97,116,101,67,10,32,32,32,42,32,32,45,32,115,116,111,114,101,
67,10,32,32,32,42,32,32,45,32,115,116,97,114,116,76,111,97,100,66,10,32,32,32,42,32,32,45,32,116,114,105,83,111,108,118,101,77,105,99,114,111,75,101,114,110,101,108,10,32,32,32,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,
42,42,42,42,42,42,42,42,42,42,42,47,10,10,32,32,47,42,42,10,32,32,32,42,32,97,117,120,95,115,101,116,122,101,114,111,10,32,32,32,42,10,32,32,32,42,32,50,45,68,32,117,110,114,111,108,108,10,32,32,32,42,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,
77,32,61,32,48,59,32,115,116,97,114,116,77,32,60,32,101,110,100,77,59,32,115,116,97,114,116,77,43,43,41,10,32,32,32,42,32,32,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,78,32,61,32,48,59,32,115,116,97,114,116,78,32,60,32,101,110,100,78,59,32,
115,116,97,114,116,78,43,43,41,10,32,32,32,42,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,
62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,62,32,48,41,62,32,97,117,120,95,115,101,116,122,101,114,111,
40,10,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,41,32,123,10,32,32,32,32,99,111,
110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,61,32,101,110,100,77,32,42,32,101,110,100,78,32,45,32,99,111,117,110,116,101,114,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,
32,105,110,116,54,52,95,116,32,115,116,97,114,116,77,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,47,32,40,101,110,100,78,41,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,115,116,97,114,116,78,
32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,37,32,101,110,100,78,59,10,10,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,115,116,97,114,116,78,32,42,32,101,110,100,77,32,43,32,115,116,97,114,116,77,93,32,61,32,112,122,101,114,
111,40,122,109,109,46,112,97,99,107,101,116,91,115,116,97,114,116,78,32,42,32,101,110,100,77,32,43,32,115,116,97,114,116,77,93,41,59,10,32,32,32,32,97,117,120,95,115,101,116,122,101,114,111,60,101,110,100,77,44,32,101,110,100,78,44,32,99,111,117,110,
116,101,114,32,45,32,49,62,40,122,109,109,41,59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,52,95,116,32,99,111,117,110,
116,101,114,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,60,61,32,48,41,62,32,97,117,120,95,115,101,116,
122,101,114,111,40,10,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,41,32,123,10,32,
32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,122,109,109,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,97,117,120,95,117,112,100,97,116,101,67,10,32,32,32,42,10,32,32,32,42,32,50,45,68,32,117,110,114,111,108,
108,10,32,32,32,42,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,77,32,61,32,48,59,32,115,116,97,114,116,77,32,60,32,101,110,100,77,59,32,115,116,97,114,116,77,43,43,41,10,32,32,32,42,32,32,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,78,
32,61,32,48,59,32,115,116,97,114,116,78,32,60,32,101,110,100,78,59,32,115,116,97,114,116,78,43,43,41,10,32,32,32,42,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,
100,78,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,98,111,111,108,32,114,101,109,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,
95,116,60,40,99,111,117,110,116,101,114,32,62,32,48,41,62,32,97,117,120,95,117,112,100,97,116,101,67,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,67,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,67,44,32,80,97,99,107,101,116,66,108,111,99,
107,60,118,101,99,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,44,32,105,110,116,54,52,95,116,32,114,101,109,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,
71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,114,101,109,95,41,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,61,32,101,110,100,77,32,42,32,101,
110,100,78,32,45,32,99,111,117,110,116,101,114,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,115,116,97,114,116,77,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,47,32,40,101,110,100,78,41,59,10,
32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,115,116,97,114,116,78,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,37,32,101,110,100,78,59,10,10,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,
69,88,80,82,40,114,101,109,41,10,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,115,116,97,114,116,78,32,42,32,101,110,100,77,32,43,32,115,116,97,114,116,77,93,32,61,10,32,32,32,32,32,32,32,32,112,97,100,100,40,112,108,111,97,100,117,60,118,101,
99,62,40,38,67,95,97,114,114,91,40,115,116,97,114,116,78,41,42,76,68,67,32,43,32,115,116,97,114,116,77,32,42,32,80,97,99,107,101,116,83,105,122,101,93,44,32,114,101,109,77,97,115,107,60,80,97,99,107,101,116,83,105,122,101,62,40,114,101,109,95,41,41,44,
10,32,32,32,32,32,32,32,32,32,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,115,116,97,114,116,78,32,42,32,101,110,100,77,32,43,32,115,116,97,114,116,77,93,44,32,114,101,109,77,97,115,107,60,80,97,99,107,101,116,83,105,122,101,62,40,114,101,109,
95,41,41,59,10,32,32,32,32,101,108,115,101,32,122,109,109,46,112,97,99,107,101,116,91,115,116,97,114,116,78,32,42,32,101,110,100,77,32,43,32,115,116,97,114,116,77,93,32,61,10,32,32,32,32,32,32,32,32,112,97,100,100,40,112,108,111,97,100,117,60,118,101,
99,62,40,38,67,95,97,114,114,91,40,115,116,97,114,116,78,41,42,76,68,67,32,43,32,115,116,97,114,116,77,32,42,32,80,97,99,107,101,116,83,105,122,101,93,41,44,32,122,109,109,46,112,97,99,107,101,116,91,115,116,97,114,116,78,32,42,32,101,110,100,77,32,43,
32,115,116,97,114,116,77,93,41,59,10,32,32,32,32,97,117,120,95,117,112,100,97,116,101,67,60,101,110,100,77,44,32,101,110,100,78,44,32,99,111,117,110,116,101,114,32,45,32,49,44,32,114,101,109,62,40,67,95,97,114,114,44,32,76,68,67,44,32,122,109,109,44,
32,114,101,109,95,41,59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,98,111,
111,108,32,114,101,109,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,60,61,32,48,41,62,32,97,117,120,95,117,
112,100,97,116,101,67,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,67,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,67,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,
95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,44,32,105,110,116,54,52,95,116,32,114,101,109,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,67,95,97,114,114,
41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,76,68,67,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,122,109,109,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,
95,86,65,82,73,65,66,76,69,40,114,101,109,95,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,97,117,120,95,115,116,111,114,101,67,10,32,32,32,42,10,32,32,32,42,32,50,45,68,32,117,110,114,111,108,108,10,32,32,32,42,32,32,32,32,32,32,102,111,
114,40,115,116,97,114,116,77,32,61,32,48,59,32,115,116,97,114,116,77,32,60,32,101,110,100,77,59,32,115,116,97,114,116,77,43,43,41,10,32,32,32,42,32,32,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,78,32,61,32,48,59,32,115,116,97,114,116,78,32,60,
32,101,110,100,78,59,32,115,116,97,114,116,78,43,43,41,10,32,32,32,42,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,52,95,116,32,99,
111,117,110,116,101,114,44,32,98,111,111,108,32,114,101,109,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,
62,32,48,41,62,32,97,117,120,95,115,116,111,114,101,67,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,67,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,67,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,
82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,44,32,105,110,116,54,52,95,116,32,114,101,109,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,
65,66,76,69,40,114,101,109,95,41,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,61,32,101,110,100,77,32,42,32,101,110,100,78,32,45,32,99,111,117,110,116,101,
114,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,115,116,97,114,116,77,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,47,32,40,101,110,100,78,41,59,10,32,32,32,32,99,111,110,115,116,101,120,112,
114,32,105,110,116,54,52,95,116,32,115,116,97,114,116,78,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,37,32,101,110,100,78,59,10,10,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,114,101,109,41,10,32,32,32,32,
112,115,116,111,114,101,117,60,83,99,97,108,97,114,62,40,38,67,95,97,114,114,91,40,115,116,97,114,116,78,41,42,76,68,67,32,43,32,115,116,97,114,116,77,32,42,32,80,97,99,107,101,116,83,105,122,101,93,44,32,122,109,109,46,112,97,99,107,101,116,91,115,116,
97,114,116,78,32,42,32,101,110,100,77,32,43,32,115,116,97,114,116,77,93,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,114,101,109,77,97,115,107,60,80,97,99,107,101,116,83,105,122,101,62,40,114,101,109,95,41,41,59,10,32,32,32,32,101,
108,115,101,32,112,115,116,111,114,101,117,60,83,99,97,108,97,114,62,40,38,67,95,97,114,114,91,40,115,116,97,114,116,78,41,42,76,68,67,32,43,32,115,116,97,114,116,77,32,42,32,80,97,99,107,101,116,83,105,122,101,93,44,32,122,109,109,46,112,97,99,107,101,
116,91,115,116,97,114,116,78,32,42,32,101,110,100,77,32,43,32,115,116,97,114,116,77,93,41,59,10,32,32,32,32,97,117,120,95,115,116,111,114,101,67,60,101,110,100,77,44,32,101,110,100,78,44,32,99,111,117,110,116,101,114,32,45,32,49,44,32,114,101,109,62,
40,67,95,97,114,114,44,32,76,68,67,44,32,122,109,109,44,32,114,101,109,95,41,59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,
54,52,95,116,32,99,111,117,110,116,101,114,44,32,98,111,111,108,32,114,101,109,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,
110,116,101,114,32,60,61,32,48,41,62,32,97,117,120,95,115,116,111,114,101,67,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,67,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,67,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,
69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,44,32,105,110,116,54,52,95,116,32,114,101,109,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,
69,68,95,86,65,82,73,65,66,76,69,40,67,95,97,114,114,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,76,68,67,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,122,109,109,41,
59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,114,101,109,95,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,97,117,120,95,115,116,97,114,116,76,111,97,100,66,10,32,32,32,42,10,32,32,32,42,32,49,45,68,32,
117,110,114,111,108,108,10,32,32,32,42,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,76,32,61,32,48,59,32,115,116,97,114,116,76,32,60,32,101,110,100,76,59,32,115,116,97,114,116,76,43,43,41,10,32,32,32,42,42,47,10,32,32,116,101,109,112,108,97,116,
101,32,60,105,110,116,54,52,95,116,32,117,110,114,111,108,108,77,44,32,105,110,116,54,52,95,116,32,117,110,114,111,108,108,78,44,32,105,110,116,54,52,95,116,32,101,110,100,76,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,98,111,111,
108,32,114,101,109,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,62,32,48,41,62,32,97,117,120,95,115,116,
97,114,116,76,111,97,100,66,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,66,95,116,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,
95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,44,32,105,110,116,54,52,95,116,32,114,101,109,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,114,101,109,95,41,
59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,61,32,101,110,100,76,32,45,32,99,111,117,110,116,101,114,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,
32,105,110,116,54,52,95,116,32,115,116,97,114,116,76,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,59,10,10,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,114,101,109,41,10,32,32,32,32,122,109,109,46,112,97,99,107,
101,116,91,117,110,114,111,108,108,77,32,42,32,117,110,114,111,108,108,78,32,43,32,115,116,97,114,116,76,93,32,61,10,32,32,32,32,32,32,32,32,112,108,111,97,100,117,60,118,101,99,62,40,38,66,95,116,91,40,115,116,97,114,116,76,32,47,32,117,110,114,111,
108,108,77,41,32,42,32,76,68,66,32,43,32,40,115,116,97,114,116,76,32,37,32,117,110,114,111,108,108,77,41,32,42,32,80,97,99,107,101,116,83,105,122,101,93,44,32,114,101,109,77,97,115,107,60,80,97,99,107,101,116,83,105,122,101,62,40,114,101,109,95,41,41,
59,10,32,32,32,32,101,108,115,101,32,122,109,109,46,112,97,99,107,101,116,91,117,110,114,111,108,108,77,32,42,32,117,110,114,111,108,108,78,32,43,32,115,116,97,114,116,76,93,32,61,10,32,32,32,32,32,32,32,32,112,108,111,97,100,117,60,118,101,99,62,40,
38,66,95,116,91,40,115,116,97,114,116,76,32,47,32,117,110,114,111,108,108,77,41,32,42,32,76,68,66,32,43,32,40,115,116,97,114,116,76,32,37,32,117,110,114,111,108,108,77,41,32,42,32,80,97,99,107,101,116,83,105,122,101,93,41,59,10,10,32,32,32,32,97,117,
120,95,115,116,97,114,116,76,111,97,100,66,60,117,110,114,111,108,108,77,44,32,117,110,114,111,108,108,78,44,32,101,110,100,76,44,32,99,111,117,110,116,101,114,32,45,32,49,44,32,114,101,109,62,40,66,95,116,44,32,76,68,66,44,32,122,109,109,44,32,114,101,
109,95,41,59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,117,110,114,111,108,108,77,44,32,105,110,116,54,52,95,116,32,117,110,114,111,108,108,78,44,32,105,110,116,54,52,95,116,32,101,110,100,76,44,32,105,
110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,98,111,111,108,32,114,101,109,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,
111,117,110,116,101,114,32,60,61,32,48,41,62,32,97,117,120,95,115,116,97,114,116,76,111,97,100,66,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,66,95,116,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,
101,99,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,44,32,105,110,116,54,52,95,116,32,114,101,109,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,
85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,66,95,116,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,76,68,66,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,122,109,109,
41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,114,101,109,95,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,97,117,120,95,115,116,97,114,116,66,67,97,115,116,65,10,32,32,32,42,10,32,32,32,42,32,49,45,
68,32,117,110,114,111,108,108,10,32,32,32,42,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,66,32,61,32,48,59,32,115,116,97,114,116,66,32,60,32,101,110,100,66,59,32,115,116,97,114,116,66,43,43,41,10,32,32,32,42,42,47,10,32,32,116,101,109,112,108,
97,116,101,32,60,98,111,111,108,32,105,115,65,82,111,119,77,97,106,111,114,44,32,105,110,116,54,52,95,116,32,117,110,114,111,108,108,77,44,32,105,110,116,54,52,95,116,32,117,110,114,111,108,108,78,44,32,105,110,116,54,52,95,116,32,101,110,100,66,44,32,
105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,105,110,116,54,52,95,116,32,110,117,109,76,111,97,100,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,
95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,62,32,48,41,62,32,97,117,120,95,115,116,97,114,116,66,67,97,115,116,65,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,65,95,116,44,32,105,110,116,54,52,95,116,32,76,68,65,44,32,80,97,99,107,101,
116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,41,32,123,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,
54,52,95,116,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,61,32,101,110,100,66,32,45,32,99,111,117,110,116,101,114,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,115,116,97,114,116,66,32,61,32,99,111,
117,110,116,101,114,82,101,118,101,114,115,101,59,10,10,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,117,110,114,111,108,108,77,32,42,32,117,110,114,111,108,108,78,32,43,32,110,117,109,76,111,97,100,32,43,32,115,116,97,114,116,66,93,32,61,32,112,
108,111,97,100,49,60,118,101,99,62,40,38,65,95,116,91,105,100,65,60,105,115,65,82,111,119,77,97,106,111,114,62,40,115,116,97,114,116,66,44,32,48,44,32,76,68,65,41,93,41,59,10,10,32,32,32,32,97,117,120,95,115,116,97,114,116,66,67,97,115,116,65,60,105,
115,65,82,111,119,77,97,106,111,114,44,32,117,110,114,111,108,108,77,44,32,117,110,114,111,108,108,78,44,32,101,110,100,66,44,32,99,111,117,110,116,101,114,32,45,32,49,44,32,110,117,109,76,111,97,100,62,40,65,95,116,44,32,76,68,65,44,32,122,109,109,41,
59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,98,111,111,108,32,105,115,65,82,111,119,77,97,106,111,114,44,32,105,110,116,54,52,95,116,32,117,110,114,111,108,108,77,44,32,105,110,116,54,52,95,116,32,117,110,114,111,108,108,78,44,32,
105,110,116,54,52,95,116,32,101,110,100,66,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,105,110,116,54,52,95,116,32,110,117,109,76,111,97,100,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,
78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,60,61,32,48,41,62,32,97,117,120,95,115,116,97,114,116,66,67,97,115,116,65,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,65,95,116,44,32,105,110,
116,54,52,95,116,32,76,68,65,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,41,32,123,10,32,32,32,
32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,65,95,116,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,76,68,65,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,
66,76,69,40,122,109,109,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,97,117,120,95,108,111,97,100,66,10,32,32,32,42,32,99,117,114,114,75,58,32,99,117,114,114,101,110,116,32,75,10,32,32,32,42,10,32,32,32,42,32,49,45,68,32,117,110,114,111,
108,108,10,32,32,32,42,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,77,32,61,32,48,59,32,115,116,97,114,116,77,32,60,32,101,110,100,77,59,32,115,116,97,114,116,77,43,43,41,10,32,32,32,42,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,105,
110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,105,110,116,54,52,95,116,32,117,110,114,111,108,108,78,44,32,105,110,116,54,52,95,116,32,99,117,114,114,75,44,32,105,110,116,54,52,95,116,32,117,
110,114,111,108,108,75,44,32,105,110,116,54,52,95,116,32,110,117,109,76,111,97,100,44,10,32,32,32,32,32,32,32,32,32,32,32,32,105,110,116,54,52,95,116,32,110,117,109,66,67,97,115,116,44,32,98,111,111,108,32,114,101,109,62,10,32,32,115,116,97,116,105,99,
32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,62,32,48,41,62,32,97,117,120,95,108,111,97,100,66,40,10,32,32,32,32,32,32,83,99,97,108,97,114,
32,42,66,95,116,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,
109,109,44,32,105,110,116,54,52,95,116,32,114,101,109,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,114,101,109,95,41,59,10,32,32,32,32,105,102,32,40,40,110,117,109,76,111,97,100,32,47,32,101,
110,100,77,32,43,32,99,117,114,114,75,32,60,32,117,110,114,111,108,108,75,41,41,32,123,10,32,32,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,61,32,101,110,100,77,
32,45,32,99,111,117,110,116,101,114,59,10,32,32,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,115,116,97,114,116,77,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,59,10,10,32,32,32,32,32,32,69,73,71,69,
78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,114,101,109,41,32,123,10,32,32,32,32,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,101,110,100,77,32,42,32,117,110,114,111,108,108,78,32,43,32,40,115,116,97,114,116,77,32,43,32,99,117,114,114,75,32,42,
32,101,110,100,77,41,32,37,32,110,117,109,76,111,97,100,93,32,61,10,32,32,32,32,32,32,32,32,32,32,32,32,112,108,111,97,100,117,60,118,101,99,62,40,38,66,95,116,91,40,110,117,109,76,111,97,100,32,47,32,101,110,100,77,32,43,32,99,117,114,114,75,41,32,42,
32,76,68,66,32,43,32,115,116,97,114,116,77,32,42,32,80,97,99,107,101,116,83,105,122,101,93,44,32,114,101,109,77,97,115,107,60,80,97,99,107,101,116,83,105,122,101,62,40,114,101,109,95,41,41,59,10,32,32,32,32,32,32,125,10,32,32,32,32,32,32,101,108,115,
101,32,123,10,32,32,32,32,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,101,110,100,77,32,42,32,117,110,114,111,108,108,78,32,43,32,40,115,116,97,114,116,77,32,43,32,99,117,114,114,75,32,42,32,101,110,100,77,41,32,37,32,110,117,109,76,111,97,100,
93,32,61,10,32,32,32,32,32,32,32,32,32,32,32,32,112,108,111,97,100,117,60,118,101,99,62,40,38,66,95,116,91,40,110,117,109,76,111,97,100,32,47,32,101,110,100,77,32,43,32,99,117,114,114,75,41,32,42,32,76,68,66,32,43,32,115,116,97,114,116,77,32,42,32,80,
97,99,107,101,116,83,105,122,101,93,41,59,10,32,32,32,32,32,32,125,10,10,32,32,32,32,32,32,97,117,120,95,108,111,97,100,66,60,101,110,100,77,44,32,99,111,117,110,116,101,114,32,45,32,49,44,32,117,110,114,111,108,108,78,44,32,99,117,114,114,75,44,32,117,
110,114,111,108,108,75,44,32,110,117,109,76,111,97,100,44,32,110,117,109,66,67,97,115,116,44,32,114,101,109,62,40,66,95,116,44,32,76,68,66,44,32,122,109,109,44,32,114,101,109,95,41,59,10,32,32,32,32,125,10,32,32,125,10,10,32,32,116,101,109,112,108,97,
116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,44,32,105,110,116,54,52,95,116,32,117,110,114,111,108,108,78,44,32,105,110,116,54,52,95,116,32,99,117,114,114,75,44,32,105,110,116,54,
52,95,116,32,117,110,114,111,108,108,75,44,32,105,110,116,54,52,95,116,32,110,117,109,76,111,97,100,44,10,32,32,32,32,32,32,32,32,32,32,32,32,105,110,116,54,52,95,116,32,110,117,109,66,67,97,115,116,44,32,98,111,111,108,32,114,101,109,62,10,32,32,115,
116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,60,61,32,48,41,62,32,97,117,120,95,108,111,97,100,66,40,10,32,32,32,32,32,32,
83,99,97,108,97,114,32,42,66,95,116,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,
82,83,62,32,38,122,109,109,44,32,105,110,116,54,52,95,116,32,114,101,109,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,66,95,116,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,
65,82,73,65,66,76,69,40,76,68,66,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,122,109,109,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,114,101,109,95,41,59,10,32,32,125,
10,10,32,32,47,42,42,10,32,32,32,42,32,97,117,120,95,109,105,99,114,111,75,101,114,110,101,108,10,32,32,32,42,10,32,32,32,42,32,51,45,68,32,117,110,114,111,108,108,10,32,32,32,42,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,77,32,61,32,48,59,32,
115,116,97,114,116,77,32,60,32,101,110,100,77,59,32,115,116,97,114,116,77,43,43,41,10,32,32,32,42,32,32,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,78,32,61,32,48,59,32,115,116,97,114,116,78,32,60,32,101,110,100,78,59,32,115,116,97,114,116,78,
43,43,41,10,32,32,32,42,32,32,32,32,32,32,32,32,32,32,102,111,114,40,115,116,97,114,116,75,32,61,32,48,59,32,115,116,97,114,116,75,32,60,32,101,110,100,75,59,32,115,116,97,114,116,75,43,43,41,10,32,32,32,42,42,47,10,32,32,116,101,109,112,108,97,116,101,
32,60,98,111,111,108,32,105,115,65,82,111,119,77,97,106,111,114,44,32,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,52,95,116,32,101,110,100,75,44,32,105,110,116,54,52,95,116,32,99,111,
117,110,116,101,114,44,32,105,110,116,54,52,95,116,32,110,117,109,76,111,97,100,44,10,32,32,32,32,32,32,32,32,32,32,32,32,105,110,116,54,52,95,116,32,110,117,109,66,67,97,115,116,44,32,98,111,111,108,32,114,101,109,62,10,32,32,115,116,97,116,105,99,32,
69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,62,32,48,41,62,32,97,117,120,95,109,105,99,114,111,75,101,114,110,101,108,40,10,32,32,32,32,32,32,
83,99,97,108,97,114,32,42,66,95,116,44,32,83,99,97,108,97,114,32,42,65,95,116,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,105,110,116,54,52,95,116,32,76,68,65,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,
82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,44,10,32,32,32,32,32,32,105,110,116,54,52,95,116,32,114,101,109,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,
68,95,86,65,82,73,65,66,76,69,40,114,101,109,95,41,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,54,52,95,116,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,61,32,101,110,100,77,32,42,32,101,110,100,78,32,42,32,101,
110,100,75,32,45,32,99,111,117,110,116,101,114,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,32,115,116,97,114,116,75,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,47,32,40,101,110,100,77,32,42,32,101,110,100,
78,41,59,10,32,32,32,32,99,111,110,115,116,101,120,112,114,32,105,110,116,32,115,116,97,114,116,78,32,61,32,40,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,47,32,40,101,110,100,77,41,41,32,37,32,101,110,100,78,59,10,32,32,32,32,99,111,110,
115,116,101,120,112,114,32,105,110,116,32,115,116,97,114,116,77,32,61,32,99,111,117,110,116,101,114,82,101,118,101,114,115,101,32,37,32,101,110,100,77,59,10,10,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,115,116,97,114,116,75,
32,61,61,32,48,32,38,38,32,115,116,97,114,116,77,32,61,61,32,48,32,38,38,32,115,116,97,114,116,78,32,61,61,32,48,41,32,123,10,32,32,32,32,32,32,103,101,109,109,58,58,116,101,109,112,108,97,116,101,32,115,116,97,114,116,76,111,97,100,66,60,101,110,100,
77,44,32,101,110,100,78,44,32,110,117,109,76,111,97,100,44,32,114,101,109,62,40,66,95,116,44,32,76,68,66,44,32,122,109,109,44,32,114,101,109,95,41,59,10,32,32,32,32,32,32,103,101,109,109,58,58,116,101,109,112,108,97,116,101,32,115,116,97,114,116,66,67,
97,115,116,65,60,105,115,65,82,111,119,77,97,106,111,114,44,32,101,110,100,77,44,32,101,110,100,78,44,32,110,117,109,66,67,97,115,116,44,32,110,117,109,76,111,97,100,62,40,65,95,116,44,32,76,68,65,44,32,122,109,109,41,59,10,32,32,32,32,125,10,10,32,32,
32,32,123,10,32,32,32,32,32,32,47,47,32,73,110,116,101,114,108,101,97,118,101,32,70,77,65,32,97,110,100,32,66,99,97,115,116,10,32,32,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,105,115,65,100,100,41,32,123,10,32,32,32,32,32,32,
32,32,122,109,109,46,112,97,99,107,101,116,91,115,116,97,114,116,78,32,42,32,101,110,100,77,32,43,32,115,116,97,114,116,77,93,32,61,10,32,32,32,32,32,32,32,32,32,32,32,32,112,109,97,100,100,40,122,109,109,46,112,97,99,107,101,116,91,101,110,100,77,32,
42,32,101,110,100,78,32,43,32,110,117,109,76,111,97,100,32,43,32,40,115,116,97,114,116,78,32,43,32,115,116,97,114,116,75,32,42,32,101,110,100,78,41,32,37,32,110,117,109,66,67,97,115,116,93,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,122,
109,109,46,112,97,99,107,101,116,91,101,110,100,77,32,42,32,101,110,100,78,32,43,32,40,115,116,97,114,116,77,32,43,32,115,116,97,114,116,75,32,42,32,101,110,100,77,41,32,37,32,110,117,109,76,111,97,100,93,44,32,122,109,109,46,112,97,99,107,101,116,91,
115,116,97,114,116,78,32,42,32,101,110,100,77,32,43,32,115,116,97,114,116,77,93,41,59,10,32,32,32,32,32,32,125,10,32,32,32,32,32,32,101,108,115,101,32,123,10,32,32,32,32,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,115,116,97,114,116,78,32,42,
32,101,110,100,77,32,43,32,115,116,97,114,116,77,93,32,61,10,32,32,32,32,32,32,32,32,32,32,32,32,112,110,109,97,100,100,40,122,109,109,46,112,97,99,107,101,116,91,101,110,100,77,32,42,32,101,110,100,78,32,43,32,110,117,109,76,111,97,100,32,43,32,40,115,
116,97,114,116,78,32,43,32,115,116,97,114,116,75,32,42,32,101,110,100,78,41,32,37,32,110,117,109,66,67,97,115,116,93,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,101,110,100,77,32,42,32,101,110,
100,78,32,43,32,40,115,116,97,114,116,77,32,43,32,115,116,97,114,116,75,32,42,32,101,110,100,77,41,32,37,32,110,117,109,76,111,97,100,93,44,32,122,109,109,46,112,97,99,107,101,116,91,115,116,97,114,116,78,32,42,32,101,110,100,77,32,43,32,115,116,97,114,
116,77,93,41,59,10,32,32,32,32,32,32,125,10,32,32,32,32,32,32,47,47,32,66,99,97,115,116,10,32,32,32,32,32,32,69,73,71,69,78,95,73,70,95,67,79,78,83,84,69,88,80,82,40,115,116,97,114,116,77,32,61,61,32,101,110,100,77,32,45,32,49,32,38,38,32,40,110,117,
109,66,67,97,115,116,32,43,32,115,116,97,114,116,78,32,43,32,115,116,97,114,116,75,32,42,32,101,110,100,78,32,60,32,101,110,100,75,32,42,32,101,110,100,78,41,41,32,123,10,32,32,32,32,32,32,32,32,122,109,109,46,112,97,99,107,101,116,91,101,110,100,77,
32,42,32,101,110,100,78,32,43,32,110,117,109,76,111,97,100,32,43,32,40,115,116,97,114,116,78,32,43,32,115,116,97,114,116,75,32,42,32,101,110,100,78,41,32,37,32,110,117,109,66,67,97,115,116,93,32,61,32,112,108,111,97,100,49,60,118,101,99,62,40,38,65,95,
116,91,105,100,65,60,105,115,65,82,111,119,77,97,106,111,114,62,40,10,32,32,32,32,32,32,32,32,32,32,32,32,40,110,117,109,66,67,97,115,116,32,43,32,115,116,97,114,116,78,32,43,32,115,116,97,114,116,75,32,42,32,101,110,100,78,41,32,37,32,101,110,100,78,
44,32,40,110,117,109,66,67,97,115,116,32,43,32,115,116,97,114,116,78,32,43,32,115,116,97,114,116,75,32,42,32,101,110,100,78,41,32,47,32,101,110,100,78,44,32,76,68,65,41,93,41,59,10,32,32,32,32,32,32,125,10,32,32,32,32,125,10,10,32,32,32,32,47,47,32,87,
101,32,104,97,118,101,32,117,112,100,97,116,101,100,32,97,108,108,32,97,99,99,117,109,108,97,116,111,114,115,44,32,116,105,109,101,32,116,111,32,108,111,97,100,32,110,101,120,116,32,115,101,116,32,111,102,32,66,39,115,10,32,32,32,32,69,73,71,69,78,95,
73,70,95,67,79,78,83,84,69,88,80,82,40,40,115,116,97,114,116,78,32,61,61,32,101,110,100,78,32,45,32,49,41,32,38,38,32,40,115,116,97,114,116,77,32,61,61,32,101,110,100,77,32,45,32,49,41,41,32,123,10,32,32,32,32,32,32,103,101,109,109,58,58,116,101,109,
112,108,97,116,101,32,108,111,97,100,66,60,101,110,100,77,44,32,101,110,100,78,44,32,115,116,97,114,116,75,44,32,101,110,100,75,44,32,110,117,109,76,111,97,100,44,32,110,117,109,66,67,97,115,116,44,32,114,101,109,62,40,66,95,116,44,32,76,68,66,44,32,
122,109,109,44,32,114,101,109,95,41,59,10,32,32,32,32,125,10,32,32,32,32,97,117,120,95,109,105,99,114,111,75,101,114,110,101,108,60,105,115,65,82,111,119,77,97,106,111,114,44,32,101,110,100,77,44,32,101,110,100,78,44,32,101,110,100,75,44,32,99,111,117,
110,116,101,114,32,45,32,49,44,32,110,117,109,76,111,97,100,44,32,110,117,109,66,67,97,115,116,44,32,114,101,109,62,40,66,95,116,44,32,65,95,116,44,32,76,68,66,44,32,76,68,65,44,32,122,109,109,44,32,114,101,109,95,41,59,10,32,32,125,10,10,32,32,116,101,
109,112,108,97,116,101,32,60,98,111,111,108,32,105,115,65,82,111,119,77,97,106,111,114,44,32,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,52,95,116,32,101,110,100,75,44,32,105,110,116,
54,52,95,116,32,99,111,117,110,116,101,114,44,32,105,110,116,54,52,95,116,32,110,117,109,76,111,97,100,44,10,32,32,32,32,32,32,32,32,32,32,32,32,105,110,116,54,52,95,116,32,110,117,109,66,67,97,115,116,44,32,98,111,111,108,32,114,101,109,62,10,32,32,
115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,115,116,100,58,58,101,110,97,98,108,101,95,105,102,95,116,60,40,99,111,117,110,116,101,114,32,60,61,32,48,41,62,32,97,117,120,95,109,105,99,114,111,75,101,114,110,101,
108,40,10,32,32,32,32,32,32,83,99,97,108,97,114,32,42,66,95,116,44,32,83,99,97,108,97,114,32,42,65,95,116,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,105,110,116,54,52,95,116,32,76,68,65,44,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,
44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,44,10,32,32,32,32,32,32,105,110,116,54,52,95,116,32,114,101,109,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,
71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,66,95,116,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,65,95,116,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,
40,76,68,66,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,76,68,65,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,122,109,109,41,59,10,32,32,32,32,69,73,71,69,78,95,85,78,
85,83,69,68,95,86,65,82,73,65,66,76,69,40,114,101,109,95,41,59,10,32,32,125,10,10,32,32,47,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,
42,42,42,10,32,32,32,42,32,87,114,97,112,112,101,114,115,32,102,111,114,32,97,117,120,95,88,88,88,88,32,116,111,32,104,105,100,101,32,99,111,117,110,116,101,114,32,112,97,114,97,109,101,116,101,114,10,32,32,32,42,42,42,42,42,42,42,42,42,42,42,42,42,42,
42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,47,10,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,
32,101,110,100,78,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,105,100,32,115,101,116,122,101,114,111,40,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,82,67,
72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,41,32,123,10,32,32,32,32,97,117,120,95,115,101,116,122,101,114,111,60,101,110,100,77,44,32,101,110,100,78,44,32,101,110,100,77,32,42,32,101,110,
100,78,62,40,122,109,109,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,73,100,101,97,108,108,121,32,116,104,101,32,99,111,109,112,105,108,101,114,32,102,111,108,100,115,32,116,104,101,115,101,32,105,110,116,111,32,118,97,100,100,112,123,115,
44,100,125,32,119,105,116,104,32,97,110,32,101,109,98,101,100,100,101,100,32,109,101,109,111,114,121,32,108,111,97,100,46,10,32,32,32,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,
52,95,116,32,101,110,100,78,44,32,98,111,111,108,32,114,101,109,32,61,32,102,97,108,115,101,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,105,100,32,117,112,100,97,116,101,67,40,83,99,97,108,
97,114,32,42,67,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,67,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,
101,99,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,
32,32,32,32,32,32,32,32,32,32,105,110,116,54,52,95,116,32,114,101,109,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,114,101,109,95,41,59,10,32,32,32,32,97,117,120,95,117,112,100,97,116,101,67,
60,101,110,100,77,44,32,101,110,100,78,44,32,101,110,100,77,32,42,32,101,110,100,78,44,32,114,101,109,62,40,67,95,97,114,114,44,32,76,68,67,44,32,122,109,109,44,32,114,101,109,95,41,59,10,32,32,125,10,10,32,32,116,101,109,112,108,97,116,101,32,60,105,
110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,100,78,44,32,98,111,111,108,32,114,101,109,32,61,32,102,97,108,115,101,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,
111,105,100,32,115,116,111,114,101,67,40,83,99,97,108,97,114,32,42,67,95,97,114,114,44,32,105,110,116,54,52,95,116,32,76,68,67,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,
32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,105,110,116,54,52,95,116,32,114,101,109,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,114,101,109,95,41,59,10,32,
32,32,32,97,117,120,95,115,116,111,114,101,67,60,101,110,100,77,44,32,101,110,100,78,44,32,101,110,100,77,32,42,32,101,110,100,78,44,32,114,101,109,62,40,67,95,97,114,114,44,32,76,68,67,44,32,122,109,109,44,32,114,101,109,95,41,59,10,32,32,125,10,10,
32,32,47,42,42,10,32,32,32,42,32,85,115,101,32,110,117,109,76,111,97,100,32,114,101,103,105,115,116,101,114,115,32,102,111,114,32,108,111,97,100,105,110,103,32,66,32,97,116,32,115,116,97,114,116,32,111,102,32,109,105,99,114,111,75,101,114,110,101,108,
10,32,32,32,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,117,110,114,111,108,108,77,44,32,105,110,116,54,52,95,116,32,117,110,114,111,108,108,78,44,32,105,110,116,54,52,95,116,32,101,110,100,76,44,32,98,111,111,108,
32,114,101,109,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,105,100,32,115,116,97,114,116,76,111,97,100,66,40,83,99,97,108,97,114,32,42,66,95,116,44,32,105,110,116,54,52,95,116,32,76,68,66,44,
10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,
95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,105,110,116,54,52,95,116,32,
114,101,109,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,114,101,109,95,41,59,10,32,32,32,32,97,117,120,95,115,116,97,114,116,76,111,97,100,66,60,117,110,114,111,108,108,77,44,32,117,110,114,
111,108,108,78,44,32,101,110,100,76,44,32,101,110,100,76,44,32,114,101,109,62,40,66,95,116,44,32,76,68,66,44,32,122,109,109,44,32,114,101,109,95,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,85,115,101,32,110,117,109,66,67,97,115,116,32,114,
101,103,105,115,116,101,114,115,32,102,111,114,32,98,114,111,97,100,99,97,115,116,105,110,103,32,65,32,97,116,32,115,116,97,114,116,32,111,102,32,109,105,99,114,111,75,101,114,110,101,108,10,32,32,32,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,
98,111,111,108,32,105,115,65,82,111,119,77,97,106,111,114,44,32,105,110,116,54,52,95,116,32,117,110,114,111,108,108,77,44,32,105,110,116,54,52,95,116,32,117,110,114,111,108,108,78,44,32,105,110,116,54,52,95,116,32,101,110,100,66,44,32,105,110,116,54,
52,95,116,32,110,117,109,76,111,97,100,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,105,100,32,115,116,97,114,116,66,67,97,115,116,65,40,83,99,97,108,97,114,32,42,65,95,116,44,32,105,110,116,
54,52,95,116,32,76,68,65,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,82,
67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,41,32,123,10,32,32,32,32,97,117,120,95,115,116,97,114,116,66,67,97,115,116,65,60,105,115,65,82,111,119,77,97,106,111,114,44,32,117,110,114,
111,108,108,77,44,32,117,110,114,111,108,108,78,44,32,101,110,100,66,44,32,101,110,100,66,44,32,110,117,109,76,111,97,100,62,40,65,95,116,44,32,76,68,65,44,32,122,109,109,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,76,111,97,100,115,32,
110,101,120,116,32,115,101,116,32,111,102,32,66,32,105,110,116,111,32,118,101,99,116,111,114,32,114,101,103,105,115,116,101,114,115,32,98,101,116,119,101,101,110,32,101,97,99,104,32,75,32,117,110,114,111,108,108,46,10,32,32,32,42,47,10,32,32,116,101,
109,112,108,97,116,101,32,60,105,110,116,54,52,95,116,32,101,110,100,77,44,32,105,110,116,54,52,95,116,32,117,110,114,111,108,108,78,44,32,105,110,116,54,52,95,116,32,99,117,114,114,75,44,32,105,110,116,54,52,95,116,32,117,110,114,111,108,108,75,44,32,
105,110,116,54,52,95,116,32,110,117,109,76,111,97,100,44,32,105,110,116,54,52,95,116,32,110,117,109,66,67,97,115,116,44,32,98,111,111,108,32,114,101,109,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,
118,111,105,100,32,108,111,97,100,66,40,83,99,97,108,97,114,32,42,66,95,116,44,32,105,110,116,54,52,95,116,32,76,68,66,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,80,97,
99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,105,110,116,54,52,95,116,32,114,101,109,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,95,86,65,82,73,65,66,76,69,40,114,101,109,95,41,59,10,32,32,32,32,97,117,
120,95,108,111,97,100,66,60,101,110,100,77,44,32,101,110,100,77,44,32,117,110,114,111,108,108,78,44,32,99,117,114,114,75,44,32,117,110,114,111,108,108,75,44,32,110,117,109,76,111,97,100,44,32,110,117,109,66,67,97,115,116,44,32,114,101,109,62,40,66,95,
116,44,32,76,68,66,44,32,122,109,109,44,32,114,101,109,95,41,59,10,32,32,125,10,10,32,32,47,42,42,10,32,32,32,42,32,71,101,110,101,114,97,116,101,115,32,97,32,109,105,99,114,111,107,101,114,110,101,108,32,102,111,114,32,103,101,109,109,32,40,114,111,
119,45,109,97,106,111,114,41,32,119,105,116,104,32,117,110,114,111,108,108,115,32,123,49,44,50,44,52,44,56,125,120,123,85,49,44,85,50,44,85,51,125,32,116,111,32,99,111,109,112,117,116,101,32,67,32,45,61,32,65,42,66,46,10,32,32,32,42,32,65,32,109,97,116,
114,105,120,32,99,97,110,32,98,101,32,114,111,119,47,99,111,108,45,109,97,106,111,114,46,32,66,32,109,97,116,114,105,120,32,105,115,32,97,115,115,117,109,101,100,32,114,111,119,45,109,97,106,111,114,46,10,32,32,32,42,10,32,32,32,42,32,105,115,65,82,111,
119,77,97,106,111,114,58,32,105,115,32,65,32,114,111,119,32,109,97,106,111,114,10,32,32,32,42,32,101,110,100,77,58,32,78,117,109,98,101,114,32,114,101,103,105,115,116,101,114,115,32,112,101,114,32,114,111,119,10,32,32,32,42,32,101,110,100,78,58,32,78,
117,109,98,101,114,32,111,102,32,114,111,119,115,10,32,32,32,42,32,101,110,100,75,58,32,76,111,111,112,32,117,110,114,111,108,108,32,102,111,114,32,75,46,10,32,32,32,42,32,110,117,109,76,111,97,100,58,32,78,117,109,98,101,114,32,111,102,32,114,101,103,
105,115,116,101,114,115,32,102,111,114,32,108,111,97,100,105,110,103,32,66,46,10,32,32,32,42,32,110,117,109,66,67,97,115,116,58,32,78,117,109,98,101,114,32,111,102,32,114,101,103,105,115,116,101,114,115,32,102,111,114,32,98,114,111,97,100,99,97,115,116,
105,110,103,32,65,46,10,32,32,32,42,10,32,32,32,42,32,69,120,58,32,109,105,99,114,111,107,101,114,110,101,108,60,105,115,65,82,111,119,77,97,106,111,114,44,48,44,51,44,48,44,52,44,48,44,52,44,54,44,50,62,58,32,56,120,52,56,32,117,110,114,111,108,108,
32,40,50,52,32,97,99,99,117,109,117,108,97,116,111,114,115,41,44,32,107,32,117,110,114,111,108,108,101,100,32,52,32,116,105,109,101,115,44,10,32,32,32,42,32,54,32,114,101,103,105,115,116,101,114,32,102,111,114,32,108,111,97,100,105,110,103,32,66,44,32,
50,32,102,111,114,32,98,114,111,97,100,99,97,115,116,105,110,103,32,65,46,10,32,32,32,42,10,32,32,32,42,32,78,111,116,101,58,32,73,100,101,97,108,108,121,32,116,104,101,32,109,105,99,114,111,107,101,114,110,101,108,32,115,104,111,117,108,100,32,110,111,
116,32,104,97,118,101,32,97,110,121,32,114,101,103,105,115,116,101,114,32,115,112,105,108,108,105,110,103,46,10,32,32,32,42,32,84,104,101,32,97,118,120,32,105,110,115,116,114,117,99,116,105,111,110,32,99,111,117,110,116,115,32,115,104,111,117,108,100,
32,98,101,58,10,32,32,32,42,32,32,32,45,32,101,110,100,75,42,101,110,100,78,32,118,98,114,111,97,100,99,97,115,116,115,123,115,44,100,125,10,32,32,32,42,32,32,32,45,32,101,110,100,75,42,101,110,100,77,32,118,109,111,118,117,112,123,115,44,100,125,10,
32,32,32,42,32,32,32,45,32,101,110,100,75,42,101,110,100,78,42,101,110,100,77,32,70,77,65,115,10,32,32,32,42,10,32,32,32,42,32,70,114,111,109,32,116,101,115,116,105,110,103,44,32,116,104,101,114,101,32,97,114,101,32,110,111,32,114,101,103,105,115,116,
101,114,32,115,112,105,108,108,115,32,119,105,116,104,32,99,108,97,110,103,46,32,84,104,101,114,101,32,97,114,101,32,114,101,103,105,115,116,101,114,32,115,112,105,108,108,115,32,119,105,116,104,32,71,78,85,44,32,119,104,105,99,104,10,32,32,32,42,32,
99,97,117,115,101,115,32,97,32,112,101,114,102,111,114,109,97,110,99,101,32,104,105,116,46,10,32,32,32,42,47,10,32,32,116,101,109,112,108,97,116,101,32,60,98,111,111,108,32,105,115,65,82,111,119,77,97,106,111,114,44,32,105,110,116,54,52,95,116,32,101,
110,100,77,44,32,105,110,116,54,52,95,116,32,101,110,100,78,44,32,105,110,116,54,52,95,116,32,101,110,100,75,44,32,105,110,116,54,52,95,116,32,110,117,109,76,111,97,100,44,32,105,110,116,54,52,95,116,32,110,117,109,66,67,97,115,116,44,10,32,32,32,32,
32,32,32,32,32,32,32,32,98,111,111,108,32,114,101,109,32,61,32,102,97,108,115,101,62,10,32,32,115,116,97,116,105,99,32,69,73,71,69,78,95,65,76,87,65,89,83,95,73,78,76,73,78,69,32,118,111,105,100,32,109,105,99,114,111,75,101,114,110,101,108,40,83,99,97,
108,97,114,32,42,66,95,116,44,32,83,99,97,108,97,114,32,42,65,95,116,44,32,105,110,116,54,52,95,116,32,76,68,66,44,32,105,110,116,54,52,95,116,32,76,68,65,44,10,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,80,97,99,107,101,116,66,108,111,99,107,60,118,101,99,44,32,69,73,71,69,78,95,65,82,67,72,95,68,69,70,65,85,76,84,95,78,85,77,66,69,82,95,79,70,95,82,69,71,73,83,84,69,82,83,62,32,38,122,109,109,44,10,32,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,105,110,116,54,52,95,116,32,114,101,109,95,32,61,32,48,41,32,123,10,32,32,32,32,69,73,71,69,78,95,85,78,85,83,69,68,
95,86,65,82,73,65,66,76,69,40,114,101,109,95,41,59,10,32,32,32,32,97,117,120,95,109,105,99,114,111,75,101,114,110,101,108,60,105,115,65,82,111,119,77,97,106,111,114,44,32,101,110,100,77,44,32,101,110,100,78,44,32,101,110,100,75,44,32,101,110,100,77,32,
42,32,101,110,100,78,32,42,32,101,110,100,75,44,32,110,117,109,76,111,97,100,44,32,110,117,109,66,67,97,115,116,44,32,114,101,109,62,40,66,95,116,44,32,65,95,116,44,32,76,68,66,44,32,76,68,65,44,32,122,109,109,44,10,32,32,32,32,32,32,32,32,32,32,32,32,
32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,114,
101,109,95,41,59,10,32,32,125,10,125,59,10,125,32,32,47,47,32,110,97,109,101,115,112,97,99,101,32,117,110,114,111,108,108,115,10,10,35,101,110,100,105,102,32,32,47,47,32,69,73,71,69,78,95,67,79,82,69,95,65,82,67,72,95,65,86,88,53,49,50,95,84,82,83,77,
95,85,78,82,79,76,76,83,95,72,10,0,0 };

const char* TrsmUnrolls_inc = (const char*) temp_binary_data_0;

//================== AccelerateSupport ==================
static const unsigned char temp_binary_data_1[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_ACCELERATESUPPORT_MODULE_H\n"
"#define EIGEN_ACCELERATESUPPORT_MODULE_H\n"
"\n"
"#include \"SparseCore\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"/** \\ingroup Support_modules\n"
"  * \\defgroup AccelerateSupport_Module AccelerateSupport module\n"
"  *\n"
"  * This module provides an interface to the Apple Accelerate library.\n"
"  * It provides the seven following main factorization classes:\n"
"  * - class AccelerateLLT: a Cholesky (LL^T) factorization.\n"
"  * - class AccelerateLDLT: the default LDL^T factorization.\n"
"  * - class AccelerateLDLTUnpivoted: a Cholesky-like LDL^T factorization with only 1x1 pivots and no pivoting\n"
"  * - class AccelerateLDLTSBK: an LDL^T factorization with Supernode Bunch-Kaufman and static pivoting\n"
"  * - class AccelerateLDLTTPP: an LDL^T factorization with full threshold partial pivoting\n"
"  * - class AccelerateQR: a QR factorization\n"
"  * - class AccelerateCholeskyAtA: a QR factorization without storing Q (equivalent to A^TA = R^T R)\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/AccelerateSupport>\n"
"  * \\endcode\n"
"  *\n"
"  * In order to use this module, the Accelerate headers must be accessible from\n"
"  * the include paths, and your binary must be linked to the Accelerate framework.\n"
"  * The Accelerate library is only available on Apple hardware.\n"
"  * \n"
"  * Note that many of the algorithms can be influenced by the UpLo template\n"
"  * argument. All matrices are assumed to be symmetric. For example, the following\n"
"  * creates an LDLT factorization where your matrix is symmetric (implicit) and\n"
"  * uses the lower triangle:\n"
"  * \n"
"  * \\code\n"
"  * AccelerateLDLT<SparseMatrix<float>, Lower> ldlt;\n"
"  * \\endcode\n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/AccelerateSupport/AccelerateSupport.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_ACCELERATESUPPORT_MODULE_H\n";

const char* AccelerateSupport = (const char*) temp_binary_data_1;

//================== Cholesky ==================
static const unsigned char temp_binary_data_2[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_CHOLESKY_MODULE_H\n"
"#define EIGEN_CHOLESKY_MODULE_H\n"
"\n"
"#include \"Core\"\n"
"#include \"Jacobi\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"/** \\defgroup Cholesky_Module Cholesky module\n"
"  *\n"
"  *\n"
"  *\n"
"  * This module provides two variants of the Cholesky decomposition for selfadjoint (hermitian) matrices.\n"
"  * Those decompositions are also accessible via the following methods:\n"
"  *  - MatrixBase::llt()\n"
"  *  - MatrixBase::ldlt()\n"
"  *  - SelfAdjointView::llt()\n"
"  *  - SelfAdjointView::ldlt()\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/Cholesky>\n"
"  * \\endcode\n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/Cholesky/LLT.h\"\n"
"#include \"src/Cholesky/LDLT.h\"\n"
"#ifdef EIGEN_USE_LAPACKE\n"
"#include \"src/misc/lapacke_helpers.h\"\n"
"#include \"src/Cholesky/LLT_LAPACKE.h\"\n"
"#endif\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_CHOLESKY_MODULE_H\n";

const char* Cholesky = (const char*) temp_binary_data_2;

//================== CholmodSupport ==================
static const unsigned char temp_binary_data_3[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_CHOLMODSUPPORT_MODULE_H\n"
"#define EIGEN_CHOLMODSUPPORT_MODULE_H\n"
"\n"
"#include \"SparseCore\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"extern \"C\" {\n"
"  #include <cholmod.h>\n"
"}\n"
"\n"
"/** \\ingroup Support_modules\n"
"  * \\defgroup CholmodSupport_Module CholmodSupport module\n"
"  *\n"
"  * This module provides an interface to the Cholmod library which is part of the <a href=\"http://www.suitesparse.com\">suitesparse</a> package.\n"
"  * It provides the two following main factorization classes:\n"
"  * - class CholmodSupernodalLLT: a supernodal LLT Cholesky factorization.\n"
"  * - class CholmodDecomposition: a general L(D)LT Cholesky factorization with automatic or explicit runtime selection of the underlying factorization method (supernodal or simplicial).\n"
"  *\n"
"  * For the sake of completeness, this module also propose the two following classes:\n"
"  * - class CholmodSimplicialLLT\n"
"  * - class CholmodSimplicialLDLT\n"
"  * Note that these classes does not bring any particular advantage compared to the built-in\n"
"  * SimplicialLLT and SimplicialLDLT factorization classes.\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/CholmodSupport>\n"
"  * \\endcode\n"
"  *\n"
"  * In order to use this module, the cholmod headers must be accessible from the include paths, and your binary must be linked to the cholmod library and its dependencies.\n"
"  * The dependencies depend on how cholmod has been compiled.\n"
"  * For a cmake based project, you can use our FindCholmod.cmake module to help you in this task.\n"
"  *\n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/CholmodSupport/CholmodSupport.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_CHOLMODSUPPORT_MODULE_H\n"
"\n";

const char* CholmodSupport = (const char*) temp_binary_data_3;

//================== Core ==================
static const unsigned char temp_binary_data_4[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// Copyright (C) 2008 Gael Guennebaud <gael.guennebaud@inria.fr>\n"
"// Copyright (C) 2007-2011 Benoit Jacob <jacob.benoit.1@gmail.com>\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_CORE_MODULE_H\n"
"#define EIGEN_CORE_MODULE_H\n"
"\n"
"// first thing Eigen does: stop the compiler from reporting useless warnings.\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"// then include this file where all our macros are defined. It's really important to do it first because\n"
"// it's where we do all the compiler/OS/arch detections and define most defaults.\n"
"#include \"src/Core/util/Macros.h\"\n"
"\n"
"// This detects SSE/AVX/NEON/etc. and configure alignment settings\n"
"#include \"src/Core/util/ConfigureVectorization.h\"\n"
"\n"
"// We need cuda_runtime.h/hip_runtime.h to ensure that\n"
"// the EIGEN_USING_STD macro works properly on the device side\n"
"#if defined(EIGEN_CUDACC)\n"
"  #include <cuda_runtime.h>\n"
"#elif defined(EIGEN_HIPCC)\n"
"  #include <hip/hip_runtime.h>\n"
"#endif\n"
"\n"
"\n"
"#ifdef EIGEN_EXCEPTIONS\n"
"  #include <new>\n"
"#endif\n"
"\n"
"// Disable the ipa-cp-clone optimization flag with MinGW 6.x or older (enabled by default with -O3)\n"
"// See http://eigen.tuxfamily.org/bz/show_bug.cgi?id=556 for details.\n"
"#if EIGEN_COMP_MINGW && EIGEN_GNUC_STRICT_LESS_THAN(6,0,0)\n"
"  #pragma GCC optimize (\"-fno-ipa-cp-clone\")\n"
"#endif\n"
"\n"
"// Prevent ICC from specializing std::complex operators that silently fail\n"
"// on device. This allows us to use our own device-compatible specializations\n"
"// instead.\n"
"#if EIGEN_COMP_ICC && defined(EIGEN_GPU_COMPILE_PHASE) \\\n"
"    && !defined(_OVERRIDE_COMPLEX_SPECIALIZATION_)\n"
"#define _OVERRIDE_COMPLEX_SPECIALIZATION_ 1\n"
"#endif\n"
"#include <complex>\n"
"\n"
"// this include file manages BLAS and MKL related macros\n"
"// and inclusion of their respective header files\n"
"#include \"src/Core/util/MKL_support.h\"\n"
"\n"
"\n"
"#if defined(EIGEN_HAS_CUDA_FP16) || defined(EIGEN_HAS_HIP_FP16)\n"
"  #define EIGEN_HAS_GPU_FP16\n"
"#endif\n"
"\n"
"#if defined(EIGEN_HAS_CUDA_BF16) || defined(EIGEN_HAS_HIP_BF16)\n"
"  #define EIGEN_HAS_GPU_BF16\n"
"#endif\n"
"\n"
"#if (defined _OPENMP) && (!defined EIGEN_DONT_PARALLELIZE)\n"
"  #define EIGEN_HAS_OPENMP\n"
"#endif\n"
"\n"
"#ifdef EIGEN_HAS_OPENMP\n"
"#include <atomic>\n"
"#include <omp.h>\n"
"#endif\n"
"\n"
"// MSVC for windows mobile does not have the errno.h file\n"
"#if !(EIGEN_COMP_MSVC && EIGEN_OS_WINCE) && !EIGEN_COMP_ARM\n"
"#define EIGEN_HAS_ERRNO\n"
"#endif\n"
"\n"
"#ifdef EIGEN_HAS_ERRNO\n"
"#include <cerrno>\n"
"#endif\n"
"#include <cstddef>\n"
"#include <cstdlib>\n"
"#include <cmath>\n"
"#include <functional>\n"
"#ifndef EIGEN_NO_IO\n"
"  #include <sstream>\n"
"  #include <iosfwd>\n"
"#endif\n"
"#include <cstring>\n"
"#include <string>\n"
"#include <limits>\n"
"#include <climits> // for CHAR_BIT\n"
"// for min/max:\n"
"#include <algorithm>\n"
"\n"
"#include <array>\n"
"#include <vector>\n"
"\n"
"// for std::is_nothrow_move_assignable\n"
"#include <type_traits>\n"
"\n"
"// for outputting debug info\n"
"#ifdef EIGEN_DEBUG_ASSIGN\n"
"#include <iostream>\n"
"#endif\n"
"\n"
"// required for __cpuid, needs to be included after cmath\n"
"// also required for _BitScanReverse on Windows on ARM\n"
"#if EIGEN_COMP_MSVC && (EIGEN_ARCH_i386_OR_x86_64 || EIGEN_ARCH_ARM64) && !EIGEN_OS_WINCE\n"
"  #include <intrin.h>\n"
"#endif\n"
"\n"
"#if defined(EIGEN_USE_SYCL)\n"
"  #undef min\n"
"  #undef max\n"
"  #undef isnan\n"
"  #undef isinf\n"
"  #undef isfinite\n"
"  #include <CL/sycl.hpp>\n"
"  #include <map>\n"
"  #include <memory>\n"
"  #include <utility>\n"
"  #include <thread>\n"
"  #ifndef EIGEN_SYCL_LOCAL_THREAD_DIM0\n"
"  #define EIGEN_SYCL_LOCAL_THREAD_DIM0 16\n"
"  #endif\n"
"  #ifndef EIGEN_SYCL_LOCAL_THREAD_DIM1\n"
"  #define EIGEN_SYCL_LOCAL_THREAD_DIM1 16\n"
"  #endif\n"
"#endif\n"
"\n"
"\n"
"#if defined EIGEN2_SUPPORT_STAGE40_FULL_EIGEN3_STRICTNESS || defined EIGEN2_SUPPORT_STAGE30_FULL_EIGEN3_API || defined EIGEN2_SUPPORT_STAGE20_RESOLVE_API_CONFLICTS || defined EIGEN2_SUPPORT_STAGE10_FULL_EIGEN2_API || defined EIGEN2_SUPPORT\n"
"// This will generate an error message:\n"
"#error Eigen2-support is only available up to version 3.2. Please go to \"http://eigen.tuxfamily.org/index.php?title=Eigen2\" for further information\n"
"#endif\n"
"\n"
"namespace Eigen {\n"
"\n"
"// we use size_t frequently and we'll never remember to prepend it with std:: every time just to\n"
"// ensure QNX/QCC support\n"
"using std::size_t;\n"
"// gcc 4.6.0 wants std:: for ptrdiff_t\n"
"using std::ptrdiff_t;\n"
"\n"
"}\n"
"\n"
"/** \\defgroup Core_Module Core module\n"
"  * This is the main module of Eigen providing dense matrix and vector support\n"
"  * (both fixed and dynamic size) with all the features corresponding to a BLAS library\n"
"  * and much more...\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/Core>\n"
"  * \\endcode\n"
"  */\n"
"\n"
"#ifdef EIGEN_USE_LAPACKE\n"
"  #ifdef EIGEN_USE_MKL\n"
"    #include \"mkl_lapacke.h\"\n"
"  #else\n"
"    #include \"src/misc/lapacke.h\"\n"
"  #endif\n"
"#endif\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/Core/util/Constants.h\"\n"
"#include \"src/Core/util/Meta.h\"\n"
"#include \"src/Core/util/Assert.h\"\n"
"#include \"src/Core/util/ForwardDeclarations.h\"\n"
"#include \"src/Core/util/StaticAssert.h\"\n"
"#include \"src/Core/util/XprHelper.h\"\n"
"#include \"src/Core/util/Memory.h\"\n"
"#include \"src/Core/util/IntegralConstant.h\"\n"
"#include \"src/Core/util/Serializer.h\"\n"
"#include \"src/Core/util/SymbolicIndex.h\"\n"
"#include \"src/Core/util/EmulateArray.h\"\n"
"#include \"src/Core/util/MoreMeta.h\"\n"
"\n"
"#include \"src/Core/NumTraits.h\"\n"
"#include \"src/Core/MathFunctions.h\"\n"
"#include \"src/Core/GenericPacketMath.h\"\n"
"#include \"src/Core/MathFunctionsImpl.h\"\n"
"#include \"src/Core/arch/Default/ConjHelper.h\"\n"
"// Generic half float support\n"
"#include \"src/Core/arch/Default/Half.h\"\n"
"#include \"src/Core/arch/Default/BFloat16.h\"\n"
"#include \"src/Core/arch/Default/GenericPacketMathFunctionsFwd.h\"\n"
"\n"
"#if defined EIGEN_VECTORIZE_AVX512\n"
"  #if defined EIGEN_VECTORIZE_AVX512FP16\n"
"    #include \"src/Core/arch/AVX512/PacketMathFP16.h\"\n"
"  #endif\n"
"  #include \"src/Core/arch/SSE/PacketMath.h\"\n"
"  #include \"src/Core/arch/SSE/TypeCasting.h\"\n"
"  #include \"src/Core/arch/SSE/Complex.h\"\n"
"  #include \"src/Core/arch/AVX/PacketMath.h\"\n"
"  #include \"src/Core/arch/AVX/TypeCasting.h\"\n"
"  #include \"src/Core/arch/AVX/Complex.h\"\n"
"  #include \"src/Core/arch/AVX512/PacketMath.h\"\n"
"  #include \"src/Core/arch/AVX512/TypeCasting.h\"\n"
"  #include \"src/Core/arch/AVX512/Complex.h\"\n"
"  #include \"src/Core/arch/SSE/MathFunctions.h\"\n"
"  #include \"src/Core/arch/AVX/MathFunctions.h\"\n"
"  #include \"src/Core/arch/AVX512/MathFunctions.h\"\n"
"  #include \"src/Core/arch/AVX512/TrsmKernel.h\"\n"
"#elif defined EIGEN_VECTORIZE_AVX\n"
"  // Use AVX for floats and doubles, SSE for integers\n"
"  #include \"src/Core/arch/SSE/PacketMath.h\"\n"
"  #include \"src/Core/arch/SSE/TypeCasting.h\"\n"
"  #include \"src/Core/arch/SSE/Complex.h\"\n"
"  #include \"src/Core/arch/AVX/PacketMath.h\"\n"
"  #include \"src/Core/arch/AVX/TypeCasting.h\"\n"
"  #include \"src/Core/arch/AVX/Complex.h\"\n"
"  #include \"src/Core/arch/SSE/MathFunctions.h\"\n"
"  #include \"src/Core/arch/AVX/MathFunctions.h\"\n"
"#elif defined EIGEN_VECTORIZE_SSE\n"
"  #include \"src/Core/arch/SSE/PacketMath.h\"\n"
"  #include \"src/Core/arch/SSE/TypeCasting.h\"\n"
"  #include \"src/Core/arch/SSE/MathFunctions.h\"\n"
"  #include \"src/Core/arch/SSE/Complex.h\"\n"
"#elif defined(EIGEN_VECTORIZE_ALTIVEC) || defined(EIGEN_VECTORIZE_VSX)\n"
"  #include \"src/Core/arch/AltiVec/PacketMath.h\"\n"
"  #include \"src/Core/arch/AltiVec/TypeCasting.h\"\n"
"  #include \"src/Core/arch/AltiVec/MathFunctions.h\"\n"
"  #include \"src/Core/arch/AltiVec/Complex.h\"\n"
"#elif defined EIGEN_VECTORIZE_NEON\n"
"  #include \"src/Core/arch/NEON/PacketMath.h\"\n"
"  #include \"src/Core/arch/NEON/TypeCasting.h\"\n"
"  #include \"src/Core/arch/NEON/MathFunctions.h\"\n"
"  #include \"src/Core/arch/NEON/Complex.h\"\n"
"#elif defined EIGEN_VECTORIZE_SVE\n"
"  #include \"src/Core/arch/SVE/PacketMath.h\"\n"
"  #include \"src/Core/arch/SVE/TypeCasting.h\"\n"
"  #include \"src/Core/arch/SVE/MathFunctions.h\"\n"
"#elif defined EIGEN_VECTORIZE_ZVECTOR\n"
"  #include \"src/Core/arch/ZVector/PacketMath.h\"\n"
"  #include \"src/Core/arch/ZVector/MathFunctions.h\"\n"
"  #include \"src/Core/arch/ZVector/Complex.h\"\n"
"#elif defined EIGEN_VECTORIZE_MSA\n"
"  #include \"src/Core/arch/MSA/PacketMath.h\"\n"
"  #include \"src/Core/arch/MSA/MathFunctions.h\"\n"
"  #include \"src/Core/arch/MSA/Complex.h\"\n"
"#endif\n"
"\n"
"#if defined EIGEN_VECTORIZE_GPU\n"
"  #include \"src/Core/arch/GPU/PacketMath.h\"\n"
"  #include \"src/Core/arch/GPU/MathFunctions.h\"\n"
"  #include \"src/Core/arch/GPU/TypeCasting.h\"\n"
"#endif\n"
"\n"
"#if defined(EIGEN_USE_SYCL)\n"
"  #include \"src/Core/arch/SYCL/InteropHeaders.h\"\n"
"#if !defined(EIGEN_DONT_VECTORIZE_SYCL)\n"
"  #include \"src/Core/arch/SYCL/PacketMath.h\"\n"
"  #include \"src/Core/arch/SYCL/MathFunctions.h\"\n"
"  #include \"src/Core/arch/SYCL/TypeCasting.h\"\n"
"#endif\n"
"#endif\n"
"\n"
"#include \"src/Core/arch/Default/Settings.h\"\n"
"// This file provides generic implementations valid for scalar as well\n"
"#include \"src/Core/arch/Default/GenericPacketMathFunctions.h\"\n"
"\n"
"#include \"src/Core/functors/TernaryFunctors.h\"\n"
"#include \"src/Core/functors/BinaryFunctors.h\"\n"
"#include \"src/Core/functors/UnaryFunctors.h\"\n"
"#include \"src/Core/functors/NullaryFunctors.h\"\n"
"#include \"src/Core/functors/StlFunctors.h\"\n"
"#include \"src/Core/functors/AssignmentFunctors.h\"\n"
"\n"
"// Specialized functors for GPU.\n"
"#ifdef EIGEN_GPUCC\n"
"#include \"src/Core/arch/GPU/Complex.h\"\n"
"#endif\n"
"\n"
"// Specializations of vectorized activation functions for NEON.\n"
"#ifdef EIGEN_VECTORIZE_NEON\n"
"#include \"src/Core/arch/NEON/UnaryFunctors.h\"\n"
"#endif\n"
"\n"
"#include \"src/Core/util/IndexedViewHelper.h\"\n"
"#include \"src/Core/util/ReshapedHelper.h\"\n"
"#include \"src/Core/ArithmeticSequence.h\"\n"
"#ifndef EIGEN_NO_IO\n"
"  #include \"src/Core/IO.h\"\n"
"#endif\n"
"#include \"src/Core/DenseCoeffsBase.h\"\n"
"#include \"src/Core/DenseBase.h\"\n"
"#include \"src/Core/MatrixBase.h\"\n"
"#include \"src/Core/EigenBase.h\"\n"
"\n"
"#include \"src/Core/Product.h\"\n"
"#include \"src/Core/CoreEvaluators.h\"\n"
"#include \"src/Core/AssignEvaluator.h\"\n"
"\n"
"#ifndef EIGEN_PARSED_BY_DOXYGEN // work around Doxygen bug triggered by Assign.h r814874\n"
"                                // at least confirmed with Doxygen 1.5.5 and 1.5.6\n"
"  #include \"src/Core/Assign.h\"\n"
"#endif\n"
"\n"
"#include \"src/Core/ArrayBase.h\"\n"
"#include \"src/Core/util/BlasUtil.h\"\n"
"#include \"src/Core/DenseStorage.h\"\n"
"#include \"src/Core/NestByValue.h\"\n"
"\n"
"// #include \"src/Core/ForceAlignedAccess.h\"\n"
"\n"
"#include \"src/Core/ReturnByValue.h\"\n"
"#include \"src/Core/NoAlias.h\"\n"
"#include \"src/Core/PlainObjectBase.h\"\n"
"#include \"src/Core/Matrix.h\"\n"
"#include \"src/Core/Array.h\"\n"
"#include \"src/Core/CwiseTernaryOp.h\"\n"
"#include \"src/Core/CwiseBinaryOp.h\"\n"
"#include \"src/Core/CwiseUnaryOp.h\"\n"
"#include \"src/Core/CwiseNullaryOp.h\"\n"
"#include \"src/Core/CwiseUnaryView.h\"\n"
"#include \"src/Core/SelfCwiseBinaryOp.h\"\n"
"#include \"src/Core/Dot.h\"\n"
"#include \"src/Core/StableNorm.h\"\n"
"#include \"src/Core/Stride.h\"\n"
"#include \"src/Core/MapBase.h\"\n"
"#include \"src/Core/Map.h\"\n"
"#include \"src/Core/Ref.h\"\n"
"#include \"src/Core/Block.h\"\n"
"#include \"src/Core/VectorBlock.h\"\n"
"#include \"src/Core/IndexedView.h\"\n"
"#include \"src/Core/Reshaped.h\"\n"
"#include \"src/Core/Transpose.h\"\n"
"#include \"src/Core/DiagonalMatrix.h\"\n"
"#include \"src/Core/Diagonal.h\"\n"
"#include \"src/Core/DiagonalProduct.h\"\n"
"#include \"src/Core/SkewSymmetricMatrix3.h\"\n"
"#include \"src/Core/Redux.h\"\n"
"#include \"src/Core/Visitor.h\"\n"
"#include \"src/Core/Fuzzy.h\"\n"
"#include \"src/Core/Swap.h\"\n"
"#include \"src/Core/CommaInitializer.h\"\n"
"#include \"src/Core/GeneralProduct.h\"\n"
"#include \"src/Core/Solve.h\"\n"
"#include \"src/Core/Inverse.h\"\n"
"#include \"src/Core/SolverBase.h\"\n"
"#include \"src/Core/PermutationMatrix.h\"\n"
"#include \"src/Core/Transpositions.h\"\n"
"#include \"src/Core/TriangularMatrix.h\"\n"
"#include \"src/Core/SelfAdjointView.h\"\n"
"#include \"src/Core/products/GeneralBlockPanelKernel.h\"\n"
"#include \"src/Core/products/Parallelizer.h\"\n"
"#include \"src/Core/ProductEvaluators.h\"\n"
"#include \"src/Core/products/GeneralMatrixVector.h\"\n"
"#include \"src/Core/products/GeneralMatrixMatrix.h\"\n"
"#include \"src/Core/SolveTriangular.h\"\n"
"#include \"src/Core/products/GeneralMatrixMatrixTriangular.h\"\n"
"#include \"src/Core/products/SelfadjointMatrixVector.h\"\n"
"#include \"src/Core/products/SelfadjointMatrixMatrix.h\"\n"
"#include \"src/Core/products/SelfadjointProduct.h\"\n"
"#include \"src/Core/products/SelfadjointRank2Update.h\"\n"
"#include \"src/Core/products/TriangularMatrixVector.h\"\n"
"#include \"src/Core/products/TriangularMatrixMatrix.h\"\n"
"#include \"src/Core/products/TriangularSolverMatrix.h\"\n"
"#include \"src/Core/products/TriangularSolverVector.h\"\n"
"#include \"src/Core/BandMatrix.h\"\n"
"#include \"src/Core/CoreIterators.h\"\n"
"#include \"src/Core/ConditionEstimator.h\"\n"
"\n"
"#if defined(EIGEN_VECTORIZE_VSX)\n"
"  #include \"src/Core/arch/AltiVec/MatrixProduct.h\"\n"
"#elif defined EIGEN_VECTORIZE_NEON\n"
"  #include \"src/Core/arch/NEON/GeneralBlockPanelKernel.h\"\n"
"#endif\n"
"\n"
"#if defined(EIGEN_VECTORIZE_AVX512)\n"
"  #include \"src/Core/arch/AVX512/GemmKernel.h\"\n"
"#endif\n"
"\n"
"#include \"src/Core/Select.h\"\n"
"#include \"src/Core/VectorwiseOp.h\"\n"
"#include \"src/Core/PartialReduxEvaluator.h\"\n"
"#include \"src/Core/Random.h\"\n"
"#include \"src/Core/Replicate.h\"\n"
"#include \"src/Core/Reverse.h\"\n"
"#include \"src/Core/ArrayWrapper.h\"\n"
"#include \"src/Core/StlIterators.h\"\n"
"\n"
"#ifdef EIGEN_USE_BLAS\n"
"#include \"src/Core/products/GeneralMatrixMatrix_BLAS.h\"\n"
"#include \"src/Core/products/GeneralMatrixVector_BLAS.h\"\n"
"#include \"src/Core/products/GeneralMatrixMatrixTriangular_BLAS.h\"\n"
"#include \"src/Core/products/SelfadjointMatrixMatrix_BLAS.h\"\n"
"#include \"src/Core/products/SelfadjointMatrixVector_BLAS.h\"\n"
"#include \"src/Core/products/TriangularMatrixMatrix_BLAS.h\"\n"
"#include \"src/Core/products/TriangularMatrixVector_BLAS.h\"\n"
"#include \"src/Core/products/TriangularSolverMatrix_BLAS.h\"\n"
"#endif // EIGEN_USE_BLAS\n"
"\n"
"#ifdef EIGEN_USE_MKL_VML\n"
"#include \"src/Core/Assign_MKL.h\"\n"
"#endif\n"
"\n"
"#include \"src/Core/GlobalFunctions.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_CORE_MODULE_H\n";

const char* Core = (const char*) temp_binary_data_4;

//================== Dense ==================
static const unsigned char temp_binary_data_5[] =
"#include \"Core\"\n"
"#include \"LU\"\n"
"#include \"Cholesky\"\n"
"#include \"QR\"\n"
"#include \"SVD\"\n"
"#include \"Geometry\"\n"
"#include \"Eigenvalues\"\n";

const char* Dense = (const char*) temp_binary_data_5;

//================== Eigen ==================
static const unsigned char temp_binary_data_6[] =
"#include \"Dense\"\n"
"#include \"Sparse\"\n";

const char* Eigen = (const char*) temp_binary_data_6;

//================== Eigenvalues ==================
static const unsigned char temp_binary_data_7[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_EIGENVALUES_MODULE_H\n"
"#define EIGEN_EIGENVALUES_MODULE_H\n"
"\n"
"#include \"Core\"\n"
"\n"
"#include \"Cholesky\"\n"
"#include \"Jacobi\"\n"
"#include \"Householder\"\n"
"#include \"LU\"\n"
"#include \"Geometry\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"/** \\defgroup Eigenvalues_Module Eigenvalues module\n"
"  *\n"
"  *\n"
"  *\n"
"  * This module mainly provides various eigenvalue solvers.\n"
"  * This module also provides some MatrixBase methods, including:\n"
"  *  - MatrixBase::eigenvalues(),\n"
"  *  - MatrixBase::operatorNorm()\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/Eigenvalues>\n"
"  * \\endcode\n"
"  */\n"
"\n"
"#include \"src/misc/RealSvd2x2.h\"\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/Eigenvalues/Tridiagonalization.h\"\n"
"#include \"src/Eigenvalues/RealSchur.h\"\n"
"#include \"src/Eigenvalues/EigenSolver.h\"\n"
"#include \"src/Eigenvalues/SelfAdjointEigenSolver.h\"\n"
"#include \"src/Eigenvalues/GeneralizedSelfAdjointEigenSolver.h\"\n"
"#include \"src/Eigenvalues/HessenbergDecomposition.h\"\n"
"#include \"src/Eigenvalues/ComplexSchur.h\"\n"
"#include \"src/Eigenvalues/ComplexEigenSolver.h\"\n"
"#include \"src/Eigenvalues/RealQZ.h\"\n"
"#include \"src/Eigenvalues/GeneralizedEigenSolver.h\"\n"
"#include \"src/Eigenvalues/MatrixBaseEigenvalues.h\"\n"
"#ifdef EIGEN_USE_LAPACKE\n"
"#ifdef EIGEN_USE_MKL\n"
"#include \"mkl_lapacke.h\"\n"
"#else\n"
"#include \"src/misc/lapacke.h\"\n"
"#endif\n"
"#include \"src/Eigenvalues/RealSchur_LAPACKE.h\"\n"
"#include \"src/Eigenvalues/ComplexSchur_LAPACKE.h\"\n"
"#include \"src/Eigenvalues/SelfAdjointEigenSolver_LAPACKE.h\"\n"
"#endif\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_EIGENVALUES_MODULE_H\n";

const char* Eigenvalues = (const char*) temp_binary_data_7;

//================== Geometry ==================
static const unsigned char temp_binary_data_8[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_GEOMETRY_MODULE_H\n"
"#define EIGEN_GEOMETRY_MODULE_H\n"
"\n"
"#include \"Core\"\n"
"\n"
"#include \"SVD\"\n"
"#include \"LU\"\n"
"#include <limits>\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"/** \\defgroup Geometry_Module Geometry module\n"
"  *\n"
"  * This module provides support for:\n"
"  *  - fixed-size homogeneous transformations\n"
"  *  - translation, scaling, 2D and 3D rotations\n"
"  *  - \\link Quaternion quaternions \\endlink\n"
"  *  - cross products (\\ref MatrixBase::cross, \\ref MatrixBase::cross3)\n"
"  *  - orthognal vector generation (\\ref MatrixBase::unitOrthogonal)\n"
"  *  - some linear components: \\link ParametrizedLine parametrized-lines \\endlink and \\link Hyperplane hyperplanes \\endlink\n"
"  *  - \\link AlignedBox axis aligned bounding boxes \\endlink\n"
"  *  - \\link umeyama least-square transformation fitting \\endlink\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/Geometry>\n"
"  * \\endcode\n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/Geometry/OrthoMethods.h\"\n"
"#include \"src/Geometry/EulerAngles.h\"\n"
"#include \"src/Geometry/Homogeneous.h\"\n"
"#include \"src/Geometry/RotationBase.h\"\n"
"#include \"src/Geometry/Rotation2D.h\"\n"
"#include \"src/Geometry/Quaternion.h\"\n"
"#include \"src/Geometry/AngleAxis.h\"\n"
"#include \"src/Geometry/Transform.h\"\n"
"#include \"src/Geometry/Translation.h\"\n"
"#include \"src/Geometry/Scaling.h\"\n"
"#include \"src/Geometry/Hyperplane.h\"\n"
"#include \"src/Geometry/ParametrizedLine.h\"\n"
"#include \"src/Geometry/AlignedBox.h\"\n"
"#include \"src/Geometry/Umeyama.h\"\n"
"\n"
"// Use the SSE optimized version whenever possible.\n"
"#if (defined EIGEN_VECTORIZE_SSE) || (defined EIGEN_VECTORIZE_NEON)\n"
"#include \"src/Geometry/arch/Geometry_SIMD.h\"\n"
"#endif\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_GEOMETRY_MODULE_H\n";

const char* Geometry = (const char*) temp_binary_data_8;

//================== Householder ==================
static const unsigned char temp_binary_data_9[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_HOUSEHOLDER_MODULE_H\n"
"#define EIGEN_HOUSEHOLDER_MODULE_H\n"
"\n"
"#include \"Core\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"/** \\defgroup Householder_Module Householder module\n"
"  * This module provides Householder transformations.\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/Householder>\n"
"  * \\endcode\n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/Householder/Householder.h\"\n"
"#include \"src/Householder/HouseholderSequence.h\"\n"
"#include \"src/Householder/BlockHouseholder.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_HOUSEHOLDER_MODULE_H\n";

const char* Householder = (const char*) temp_binary_data_9;

//================== IterativeLinearSolvers ==================
static const unsigned char temp_binary_data_10[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_ITERATIVELINEARSOLVERS_MODULE_H\n"
"#define EIGEN_ITERATIVELINEARSOLVERS_MODULE_H\n"
"\n"
"#include \"SparseCore\"\n"
"#include \"OrderingMethods\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"/** \n"
"  * \\defgroup IterativeLinearSolvers_Module IterativeLinearSolvers module\n"
"  *\n"
"  * This module currently provides iterative methods to solve problems of the form \\c A \\c x = \\c b, where \\c A is a squared matrix, usually very large and sparse.\n"
"  * Those solvers are accessible via the following classes:\n"
"  *  - ConjugateGradient for selfadjoint (hermitian) matrices,\n"
"  *  - LeastSquaresConjugateGradient for rectangular least-square problems,\n"
"  *  - BiCGSTAB for general square matrices.\n"
"  *\n"
"  * These iterative solvers are associated with some preconditioners:\n"
"  *  - IdentityPreconditioner - not really useful\n"
"  *  - DiagonalPreconditioner - also called Jacobi preconditioner, work very well on diagonal dominant matrices.\n"
"  *  - IncompleteLUT - incomplete LU factorization with dual thresholding\n"
"  *\n"
"  * Such problems can also be solved using the direct sparse decomposition modules: SparseCholesky, CholmodSupport, UmfPackSupport, SuperLUSupport, AccelerateSupport.\n"
"  *\n"
"    \\code\n"
"    #include <Eigen/IterativeLinearSolvers>\n"
"    \\endcode\n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/IterativeLinearSolvers/SolveWithGuess.h\"\n"
"#include \"src/IterativeLinearSolvers/IterativeSolverBase.h\"\n"
"#include \"src/IterativeLinearSolvers/BasicPreconditioners.h\"\n"
"#include \"src/IterativeLinearSolvers/ConjugateGradient.h\"\n"
"#include \"src/IterativeLinearSolvers/LeastSquareConjugateGradient.h\"\n"
"#include \"src/IterativeLinearSolvers/BiCGSTAB.h\"\n"
"#include \"src/IterativeLinearSolvers/IncompleteLUT.h\"\n"
"#include \"src/IterativeLinearSolvers/IncompleteCholesky.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_ITERATIVELINEARSOLVERS_MODULE_H\n";

const char* IterativeLinearSolvers = (const char*) temp_binary_data_10;

//================== Jacobi ==================
static const unsigned char temp_binary_data_11[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_JACOBI_MODULE_H\n"
"#define EIGEN_JACOBI_MODULE_H\n"
"\n"
"#include \"Core\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"/** \\defgroup Jacobi_Module Jacobi module\n"
"  * This module provides Jacobi and Givens rotations.\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/Jacobi>\n"
"  * \\endcode\n"
"  *\n"
"  * In addition to listed classes, it defines the two following MatrixBase methods to apply a Jacobi or Givens rotation:\n"
"  *  - MatrixBase::applyOnTheLeft()\n"
"  *  - MatrixBase::applyOnTheRight().\n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/Jacobi/Jacobi.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_JACOBI_MODULE_H\n"
"\n";

const char* Jacobi = (const char*) temp_binary_data_11;

//================== KLUSupport ==================
static const unsigned char temp_binary_data_12[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_KLUSUPPORT_MODULE_H\n"
"#define EIGEN_KLUSUPPORT_MODULE_H\n"
"\n"
"#include \"SparseCore\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"extern \"C\" {\n"
"#include <btf.h>\n"
"#include <klu.h>\n"
"   }\n"
"\n"
"/** \\ingroup Support_modules\n"
"  * \\defgroup KLUSupport_Module KLUSupport module\n"
"  *\n"
"  * This module provides an interface to the KLU library which is part of the <a href=\"http://www.suitesparse.com\">suitesparse</a> package.\n"
"  * It provides the following factorization class:\n"
"  * - class KLU: a sparse LU factorization, well-suited for circuit simulation.\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/KLUSupport>\n"
"  * \\endcode\n"
"  *\n"
"  * In order to use this module, the klu and btf headers must be accessible from the include paths, and your binary must be linked to the klu library and its dependencies.\n"
"  * The dependencies depend on how umfpack has been compiled.\n"
"  * For a cmake based project, you can use our FindKLU.cmake module to help you in this task.\n"
"  *\n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/KLUSupport/KLUSupport.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_KLUSUPPORT_MODULE_H\n";

const char* KLUSupport = (const char*) temp_binary_data_12;

//================== LU ==================
static const unsigned char temp_binary_data_13[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_LU_MODULE_H\n"
"#define EIGEN_LU_MODULE_H\n"
"\n"
"#include \"Core\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"/** \\defgroup LU_Module LU module\n"
"  * This module includes %LU decomposition and related notions such as matrix inversion and determinant.\n"
"  * This module defines the following MatrixBase methods:\n"
"  *  - MatrixBase::inverse()\n"
"  *  - MatrixBase::determinant()\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/LU>\n"
"  * \\endcode\n"
"  */\n"
"\n"
"#include \"src/misc/Kernel.h\"\n"
"#include \"src/misc/Image.h\"\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/LU/FullPivLU.h\"\n"
"#include \"src/LU/PartialPivLU.h\"\n"
"#ifdef EIGEN_USE_LAPACKE\n"
"#include \"src/misc/lapacke_helpers.h\"\n"
"#include \"src/LU/PartialPivLU_LAPACKE.h\"\n"
"#endif\n"
"#include \"src/LU/Determinant.h\"\n"
"#include \"src/LU/InverseImpl.h\"\n"
"\n"
"#if defined EIGEN_VECTORIZE_SSE || defined EIGEN_VECTORIZE_NEON\n"
"  #include \"src/LU/arch/InverseSize4.h\"\n"
"#endif\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_LU_MODULE_H\n";

const char* LU = (const char*) temp_binary_data_13;

//================== MetisSupport ==================
static const unsigned char temp_binary_data_14[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_METISSUPPORT_MODULE_H\n"
"#define EIGEN_METISSUPPORT_MODULE_H\n"
"\n"
"#include \"SparseCore\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"extern \"C\" {\n"
"#include <metis.h>\n"
"}\n"
"\n"
"\n"
"/** \\ingroup Support_modules\n"
"  * \\defgroup MetisSupport_Module MetisSupport module\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/MetisSupport>\n"
"  * \\endcode\n"
"  * This module defines an interface to the METIS reordering package (http://glaros.dtc.umn.edu/gkhome/views/metis). \n"
"  * It can be used just as any other built-in method as explained in \\link OrderingMethods_Module here. \\endlink\n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/MetisSupport/MetisSupport.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_METISSUPPORT_MODULE_H\n";

const char* MetisSupport = (const char*) temp_binary_data_14;

//================== OrderingMethods ==================
static const unsigned char temp_binary_data_15[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_ORDERINGMETHODS_MODULE_H\n"
"#define EIGEN_ORDERINGMETHODS_MODULE_H\n"
"\n"
"#include \"SparseCore\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"/** \n"
"  * \\defgroup OrderingMethods_Module OrderingMethods module\n"
"  *\n"
"  * This module is currently for internal use only\n"
"  * \n"
"  * It defines various built-in and external ordering methods for sparse matrices. \n"
"  * They are typically used to reduce the number of elements during \n"
"  * the sparse matrix decomposition (LLT, LU, QR).\n"
"  * Precisely, in a preprocessing step, a permutation matrix P is computed using \n"
"  * those ordering methods and applied to the columns of the matrix. \n"
"  * Using for instance the sparse Cholesky decomposition, it is expected that \n"
"  * the nonzeros elements in LLT(A*P) will be much smaller than that in LLT(A).\n"
"  * \n"
"  * \n"
"  * Usage : \n"
"  * \\code\n"
"  * #include <Eigen/OrderingMethods>\n"
"  * \\endcode\n"
"  * \n"
"  * A simple usage is as a template parameter in the sparse decomposition classes : \n"
"  * \n"
"  * \\code \n"
"  * SparseLU<MatrixType, COLAMDOrdering<int> > solver;\n"
"  * \\endcode \n"
"  * \n"
"  * \\code \n"
"  * SparseQR<MatrixType, COLAMDOrdering<int> > solver;\n"
"  * \\endcode\n"
"  * \n"
"  * It is possible as well to call directly a particular ordering method for your own purpose, \n"
"  * \\code \n"
"  * AMDOrdering<int> ordering;\n"
"  * PermutationMatrix<Dynamic, Dynamic, int> perm;\n"
"  * SparseMatrix<double> A; \n"
"  * //Fill the matrix ...\n"
"  * \n"
"  * ordering(A, perm); // Call AMD\n"
"  * \\endcode\n"
"  * \n"
"  * \\note Some of these methods (like AMD or METIS), need the sparsity pattern \n"
"  * of the input matrix to be symmetric. When the matrix is structurally unsymmetric, \n"
"  * Eigen computes internally the pattern of \\f$A^T*A\\f$ before calling the method.\n"
"  * If your matrix is already symmetric (at leat in structure), you can avoid that\n"
"  * by calling the method with a SelfAdjointView type.\n"
"  * \n"
"  * \\code\n"
"  *  // Call the ordering on the pattern of the lower triangular matrix A\n"
"  * ordering(A.selfadjointView<Lower>(), perm);\n"
"  * \\endcode\n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/OrderingMethods/Amd.h\"\n"
"#include \"src/OrderingMethods/Ordering.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_ORDERINGMETHODS_MODULE_H\n";

const char* OrderingMethods = (const char*) temp_binary_data_15;

//================== PardisoSupport ==================
static const unsigned char temp_binary_data_16[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_PARDISOSUPPORT_MODULE_H\n"
"#define EIGEN_PARDISOSUPPORT_MODULE_H\n"
"\n"
"#include \"SparseCore\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"#include <mkl_pardiso.h>\n"
"\n"
"/** \\ingroup Support_modules\n"
"  * \\defgroup PardisoSupport_Module PardisoSupport module\n"
"  *\n"
"  * This module brings support for the Intel(R) MKL PARDISO direct sparse solvers.\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/PardisoSupport>\n"
"  * \\endcode\n"
"  *\n"
"  * In order to use this module, the MKL headers must be accessible from the include paths, and your binary must be linked to the MKL library and its dependencies.\n"
"  * See this \\ref TopicUsingIntelMKL \"page\" for more information on MKL-Eigen integration.\n"
"  * \n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/PardisoSupport/PardisoSupport.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_PARDISOSUPPORT_MODULE_H\n";

const char* PardisoSupport = (const char*) temp_binary_data_16;

//================== PaStiXSupport ==================
static const unsigned char temp_binary_data_17[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_PASTIXSUPPORT_MODULE_H\n"
"#define EIGEN_PASTIXSUPPORT_MODULE_H\n"
"\n"
"#include \"SparseCore\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"extern \"C\" {\n"
"#include <pastix_nompi.h>\n"
"#include <pastix.h>\n"
"}\n"
"\n"
"#ifdef complex\n"
"#undef complex\n"
"#endif\n"
"\n"
"/** \\ingroup Support_modules\n"
"  * \\defgroup PaStiXSupport_Module PaStiXSupport module\n"
"  * \n"
"  * This module provides an interface to the <a href=\"http://pastix.gforge.inria.fr/\">PaSTiX</a> library.\n"
"  * PaSTiX is a general \\b supernodal, \\b parallel and \\b opensource sparse solver.\n"
"  * It provides the two following main factorization classes:\n"
"  * - class PastixLLT : a supernodal, parallel LLt Cholesky factorization.\n"
"  * - class PastixLDLT: a supernodal, parallel LDLt Cholesky factorization.\n"
"  * - class PastixLU : a supernodal, parallel LU factorization (optimized for a symmetric pattern).\n"
"  * \n"
"  * \\code\n"
"  * #include <Eigen/PaStiXSupport>\n"
"  * \\endcode\n"
"  *\n"
"  * In order to use this module, the PaSTiX headers must be accessible from the include paths, and your binary must be linked to the PaSTiX library and its dependencies.\n"
"  * This wrapper resuires PaStiX version 5.x compiled without MPI support.\n"
"  * The dependencies depend on how PaSTiX has been compiled.\n"
"  * For a cmake based project, you can use our FindPaSTiX.cmake module to help you in this task.\n"
"  *\n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/PaStiXSupport/PaStiXSupport.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_PASTIXSUPPORT_MODULE_H\n";

const char* PaStiXSupport = (const char*) temp_binary_data_17;

//================== QR ==================
static const unsigned char temp_binary_data_18[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_QR_MODULE_H\n"
"#define EIGEN_QR_MODULE_H\n"
"\n"
"#include \"Core\"\n"
"\n"
"#include \"Cholesky\"\n"
"#include \"Jacobi\"\n"
"#include \"Householder\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"/** \\defgroup QR_Module QR module\n"
"  *\n"
"  *\n"
"  *\n"
"  * This module provides various QR decompositions\n"
"  * This module also provides some MatrixBase methods, including:\n"
"  *  - MatrixBase::householderQr()\n"
"  *  - MatrixBase::colPivHouseholderQr()\n"
"  *  - MatrixBase::fullPivHouseholderQr()\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/QR>\n"
"  * \\endcode\n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/QR/HouseholderQR.h\"\n"
"#include \"src/QR/FullPivHouseholderQR.h\"\n"
"#include \"src/QR/ColPivHouseholderQR.h\"\n"
"#include \"src/QR/CompleteOrthogonalDecomposition.h\"\n"
"#ifdef EIGEN_USE_LAPACKE\n"
"#include \"src/misc/lapacke_helpers.h\"\n"
"#include \"src/QR/HouseholderQR_LAPACKE.h\"\n"
"#include \"src/QR/ColPivHouseholderQR_LAPACKE.h\"\n"
"#endif\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_QR_MODULE_H\n";

const char* QR = (const char*) temp_binary_data_18;

//================== QtAlignedMalloc ==================
static const unsigned char temp_binary_data_19[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_QTMALLOC_MODULE_H\n"
"#define EIGEN_QTMALLOC_MODULE_H\n"
"\n"
"#include \"Core\"\n"
"\n"
"#if (!EIGEN_MALLOC_ALREADY_ALIGNED)\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"void *qMalloc(std::size_t size)\n"
"{\n"
"  return Eigen::internal::aligned_malloc(size);\n"
"}\n"
"\n"
"void qFree(void *ptr)\n"
"{\n"
"  Eigen::internal::aligned_free(ptr);\n"
"}\n"
"\n"
"void *qRealloc(void *ptr, std::size_t size)\n"
"{\n"
"  void* newPtr = Eigen::internal::aligned_malloc(size);\n"
"  std::memcpy(newPtr, ptr, size);\n"
"  Eigen::internal::aligned_free(ptr);\n"
"  return newPtr;\n"
"}\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif\n"
"\n"
"#endif // EIGEN_QTMALLOC_MODULE_H\n";

const char* QtAlignedMalloc = (const char*) temp_binary_data_19;

//================== Sparse ==================
static const unsigned char temp_binary_data_20[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_SPARSE_MODULE_H\n"
"#define EIGEN_SPARSE_MODULE_H\n"
"\n"
"/** \\defgroup Sparse_Module Sparse meta-module\n"
"  *\n"
"  * Meta-module including all related modules:\n"
"  * - \\ref SparseCore_Module\n"
"  * - \\ref OrderingMethods_Module\n"
"  * - \\ref SparseCholesky_Module\n"
"  * - \\ref SparseLU_Module\n"
"  * - \\ref SparseQR_Module\n"
"  * - \\ref IterativeLinearSolvers_Module\n"
"  *\n"
"    \\code\n"
"    #include <Eigen/Sparse>\n"
"    \\endcode\n"
"  */\n"
"\n"
"#include \"SparseCore\"\n"
"#include \"OrderingMethods\"\n"
"#include \"SparseCholesky\"\n"
"#include \"SparseLU\"\n"
"#include \"SparseQR\"\n"
"#include \"IterativeLinearSolvers\"\n"
"\n"
"#endif // EIGEN_SPARSE_MODULE_H\n"
"\n";

const char* Sparse = (const char*) temp_binary_data_20;

//================== SparseCholesky ==================
static const unsigned char temp_binary_data_21[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// Copyright (C) 2008-2013 Gael Guennebaud <gael.guennebaud@inria.fr>\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_SPARSECHOLESKY_MODULE_H\n"
"#define EIGEN_SPARSECHOLESKY_MODULE_H\n"
"\n"
"#include \"SparseCore\"\n"
"#include \"OrderingMethods\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"/** \n"
"  * \\defgroup SparseCholesky_Module SparseCholesky module\n"
"  *\n"
"  * This module currently provides two variants of the direct sparse Cholesky decomposition for selfadjoint (hermitian) matrices.\n"
"  * Those decompositions are accessible via the following classes:\n"
"  *  - SimplicialLLt,\n"
"  *  - SimplicialLDLt\n"
"  *\n"
"  * Such problems can also be solved using the ConjugateGradient solver from the IterativeLinearSolvers module.\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/SparseCholesky>\n"
"  * \\endcode\n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/SparseCholesky/SimplicialCholesky.h\"\n"
"#include \"src/SparseCholesky/SimplicialCholesky_impl.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_SPARSECHOLESKY_MODULE_H\n";

const char* SparseCholesky = (const char*) temp_binary_data_21;

//================== SparseCore ==================
static const unsigned char temp_binary_data_22[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_SPARSECORE_MODULE_H\n"
"#define EIGEN_SPARSECORE_MODULE_H\n"
"\n"
"#include \"Core\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"#include <vector>\n"
"#include <map>\n"
"#include <cstdlib>\n"
"#include <cstring>\n"
"#include <algorithm>\n"
"#include <numeric>\n"
"\n"
"/** \n"
"  * \\defgroup SparseCore_Module SparseCore module\n"
"  *\n"
"  * This module provides a sparse matrix representation, and basic associated matrix manipulations\n"
"  * and operations.\n"
"  *\n"
"  * See the \\ref TutorialSparse \"Sparse tutorial\"\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/SparseCore>\n"
"  * \\endcode\n"
"  *\n"
"  * This module depends on: Core.\n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/SparseCore/SparseUtil.h\"\n"
"#include \"src/SparseCore/SparseMatrixBase.h\"\n"
"#include \"src/SparseCore/SparseAssign.h\"\n"
"#include \"src/SparseCore/CompressedStorage.h\"\n"
"#include \"src/SparseCore/AmbiVector.h\"\n"
"#include \"src/SparseCore/SparseCompressedBase.h\"\n"
"#include \"src/SparseCore/SparseMatrix.h\"\n"
"#include \"src/SparseCore/SparseMap.h\"\n"
"#include \"src/SparseCore/SparseVector.h\"\n"
"#include \"src/SparseCore/SparseRef.h\"\n"
"#include \"src/SparseCore/SparseCwiseUnaryOp.h\"\n"
"#include \"src/SparseCore/SparseCwiseBinaryOp.h\"\n"
"#include \"src/SparseCore/SparseTranspose.h\"\n"
"#include \"src/SparseCore/SparseBlock.h\"\n"
"#include \"src/SparseCore/SparseDot.h\"\n"
"#include \"src/SparseCore/SparseRedux.h\"\n"
"#include \"src/SparseCore/SparseView.h\"\n"
"#include \"src/SparseCore/SparseDiagonalProduct.h\"\n"
"#include \"src/SparseCore/ConservativeSparseSparseProduct.h\"\n"
"#include \"src/SparseCore/SparseSparseProductWithPruning.h\"\n"
"#include \"src/SparseCore/SparseProduct.h\"\n"
"#include \"src/SparseCore/SparseDenseProduct.h\"\n"
"#include \"src/SparseCore/SparseSelfAdjointView.h\"\n"
"#include \"src/SparseCore/SparseTriangularView.h\"\n"
"#include \"src/SparseCore/TriangularSolver.h\"\n"
"#include \"src/SparseCore/SparsePermutation.h\"\n"
"#include \"src/SparseCore/SparseFuzzy.h\"\n"
"#include \"src/SparseCore/SparseSolverBase.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_SPARSECORE_MODULE_H\n"
"\n";

const char* SparseCore = (const char*) temp_binary_data_22;

//================== SparseLU ==================
static const unsigned char temp_binary_data_23[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// Copyright (C) 2012 D\xc3\xa9sir\xc3\xa9 Nuentsa-Wakam <desire.nuentsa_wakam@inria.fr>\n"
"// Copyright (C) 2012 Gael Guennebaud <gael.guennebaud@inria.fr>\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_SPARSELU_MODULE_H\n"
"#define EIGEN_SPARSELU_MODULE_H\n"
"\n"
"#include \"SparseCore\"\n"
"\n"
"/** \n"
"  * \\defgroup SparseLU_Module SparseLU module\n"
"  * This module defines a supernodal factorization of general sparse matrices.\n"
"  * The code is fully optimized for supernode-panel updates with specialized kernels.\n"
"  * Please, see the documentation of the SparseLU class for more details.\n"
"  */\n"
"\n"
"// Ordering interface\n"
"#include \"OrderingMethods\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/SparseLU/SparseLU_Structs.h\"\n"
"#include \"src/SparseLU/SparseLU_SupernodalMatrix.h\"\n"
"#include \"src/SparseLU/SparseLUImpl.h\"\n"
"#include \"src/SparseCore/SparseColEtree.h\"\n"
"#include \"src/SparseLU/SparseLU_Memory.h\"\n"
"#include \"src/SparseLU/SparseLU_heap_relax_snode.h\"\n"
"#include \"src/SparseLU/SparseLU_relax_snode.h\"\n"
"#include \"src/SparseLU/SparseLU_pivotL.h\"\n"
"#include \"src/SparseLU/SparseLU_panel_dfs.h\"\n"
"#include \"src/SparseLU/SparseLU_kernel_bmod.h\"\n"
"#include \"src/SparseLU/SparseLU_panel_bmod.h\"\n"
"#include \"src/SparseLU/SparseLU_column_dfs.h\"\n"
"#include \"src/SparseLU/SparseLU_column_bmod.h\"\n"
"#include \"src/SparseLU/SparseLU_copy_to_ucol.h\"\n"
"#include \"src/SparseLU/SparseLU_pruneL.h\"\n"
"#include \"src/SparseLU/SparseLU_Utils.h\"\n"
"#include \"src/SparseLU/SparseLU.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_SPARSELU_MODULE_H\n";

const char* SparseLU = (const char*) temp_binary_data_23;

//================== SparseQR ==================
static const unsigned char temp_binary_data_24[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_SPARSEQR_MODULE_H\n"
"#define EIGEN_SPARSEQR_MODULE_H\n"
"\n"
"#include \"SparseCore\"\n"
"#include \"OrderingMethods\"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"/** \\defgroup SparseQR_Module SparseQR module\n"
"  * \\brief Provides QR decomposition for sparse matrices\n"
"  * \n"
"  * This module provides a simplicial version of the left-looking Sparse QR decomposition. \n"
"  * The columns of the input matrix should be reordered to limit the fill-in during the \n"
"  * decomposition. Built-in methods (COLAMD, AMD) or external  methods (METIS) can be used to this end.\n"
"  * See the \\link OrderingMethods_Module OrderingMethods\\endlink module for the list \n"
"  * of built-in and external ordering methods.\n"
"  * \n"
"  * \\code\n"
"  * #include <Eigen/SparseQR>\n"
"  * \\endcode\n"
"  * \n"
"  * \n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/SparseCore/SparseColEtree.h\"\n"
"#include \"src/SparseQR/SparseQR.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif\n";

const char* SparseQR = (const char*) temp_binary_data_24;

//================== SPQRSupport ==================
static const unsigned char temp_binary_data_25[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_SPQRSUPPORT_MODULE_H\n"
"#define EIGEN_SPQRSUPPORT_MODULE_H\n"
"\n"
"#include \"SparseCore\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"#include \"SuiteSparseQR.hpp\"\n"
"\n"
"/** \\ingroup Support_modules\n"
"  * \\defgroup SPQRSupport_Module SuiteSparseQR module\n"
"  * \n"
"  * This module provides an interface to the SPQR library, which is part of the <a href=\"http://www.suitesparse.com\">suitesparse</a> package.\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/SPQRSupport>\n"
"  * \\endcode\n"
"  *\n"
"  * In order to use this module, the SPQR headers must be accessible from the include paths, and your binary must be linked to the SPQR library and its dependencies (Cholmod, AMD, COLAMD,...).\n"
"  * For a cmake based project, you can use our FindSPQR.cmake and FindCholmod.Cmake modules\n"
"  *\n"
"  */\n"
"\n"
"#include \"CholmodSupport\"\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/SPQRSupport/SuiteSparseQRSupport.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif\n";

const char* SPQRSupport = (const char*) temp_binary_data_25;

//================== StdDeque ==================
static const unsigned char temp_binary_data_26[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// Copyright (C) 2009 Gael Guennebaud <gael.guennebaud@inria.fr>\n"
"// Copyright (C) 2009 Hauke Heibel <hauke.heibel@googlemail.com>\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_STDDEQUE_MODULE_H\n"
"#define EIGEN_STDDEQUE_MODULE_H\n"
"\n"
"#include \"Core\"\n"
"#include <deque>\n"
"\n"
"#if EIGEN_COMP_MSVC && EIGEN_OS_WIN64 && (EIGEN_MAX_STATIC_ALIGN_BYTES<=16) /* MSVC auto aligns up to 16 bytes in 64 bit builds */\n"
"\n"
"#define EIGEN_DEFINE_STL_DEQUE_SPECIALIZATION(...)\n"
"\n"
"#else\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/StlSupport/StdDeque.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#endif\n"
"\n"
"#endif // EIGEN_STDDEQUE_MODULE_H\n";

const char* StdDeque = (const char*) temp_binary_data_26;

//================== StdList ==================
static const unsigned char temp_binary_data_27[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// Copyright (C) 2009 Hauke Heibel <hauke.heibel@googlemail.com>\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_STDLIST_MODULE_H\n"
"#define EIGEN_STDLIST_MODULE_H\n"
"\n"
"#include \"Core\"\n"
"#include <list>\n"
"\n"
"#if EIGEN_COMP_MSVC && EIGEN_OS_WIN64 && (EIGEN_MAX_STATIC_ALIGN_BYTES<=16) /* MSVC auto aligns up to 16 bytes in 64 bit builds */\n"
"\n"
"#define EIGEN_DEFINE_STL_LIST_SPECIALIZATION(...)\n"
"\n"
"#else\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/StlSupport/StdList.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#endif\n"
"\n"
"#endif // EIGEN_STDLIST_MODULE_H\n";

const char* StdList = (const char*) temp_binary_data_27;

//================== StdVector ==================
static const unsigned char temp_binary_data_28[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// Copyright (C) 2009 Gael Guennebaud <gael.guennebaud@inria.fr>\n"
"// Copyright (C) 2009 Hauke Heibel <hauke.heibel@googlemail.com>\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_STDVECTOR_MODULE_H\n"
"#define EIGEN_STDVECTOR_MODULE_H\n"
"\n"
"#include \"Core\"\n"
"#include <vector>\n"
"\n"
"#if EIGEN_COMP_MSVC && EIGEN_OS_WIN64 && (EIGEN_MAX_STATIC_ALIGN_BYTES<=16) /* MSVC auto aligns up to 16 bytes in 64 bit builds */\n"
"\n"
"#define EIGEN_DEFINE_STL_VECTOR_SPECIALIZATION(...)\n"
"\n"
"#else\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/StlSupport/StdVector.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#endif\n"
"\n"
"#endif // EIGEN_STDVECTOR_MODULE_H\n";

const char* StdVector = (const char*) temp_binary_data_28;

//================== SuperLUSupport ==================
static const unsigned char temp_binary_data_29[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_SUPERLUSUPPORT_MODULE_H\n"
"#define EIGEN_SUPERLUSUPPORT_MODULE_H\n"
"\n"
"#include \"SparseCore\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"#ifdef EMPTY\n"
"#define EIGEN_EMPTY_WAS_ALREADY_DEFINED\n"
"#endif\n"
"\n"
"typedef int int_t;\n"
"#include <slu_Cnames.h>\n"
"#include <supermatrix.h>\n"
"#include <slu_util.h>\n"
"\n"
"// slu_util.h defines a preprocessor token named EMPTY which is really polluting,\n"
"// so we remove it in favor of a SUPERLU_EMPTY token.\n"
"// If EMPTY was already defined then we don't undef it.\n"
"\n"
"#if defined(EIGEN_EMPTY_WAS_ALREADY_DEFINED)\n"
"# undef EIGEN_EMPTY_WAS_ALREADY_DEFINED\n"
"#elif defined(EMPTY)\n"
"# undef EMPTY\n"
"#endif\n"
"\n"
"#define SUPERLU_EMPTY (-1)\n"
"\n"
"namespace Eigen { struct SluMatrix; }\n"
"\n"
"/** \\ingroup Support_modules\n"
"  * \\defgroup SuperLUSupport_Module SuperLUSupport module\n"
"  *\n"
"  * This module provides an interface to the <a href=\"http://crd-legacy.lbl.gov/~xiaoye/SuperLU/\">SuperLU</a> library.\n"
"  * It provides the following factorization class:\n"
"  * - class SuperLU: a supernodal sequential LU factorization.\n"
"  * - class SuperILU: a supernodal sequential incomplete LU factorization (to be used as a preconditioner for iterative methods).\n"
"  *\n"
"  * \\warning This wrapper requires at least versions 4.0 of SuperLU. The 3.x versions are not supported.\n"
"  *\n"
"  * \\warning When including this module, you have to use SUPERLU_EMPTY instead of EMPTY which is no longer defined because it is too polluting.\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/SuperLUSupport>\n"
"  * \\endcode\n"
"  *\n"
"  * In order to use this module, the superlu headers must be accessible from the include paths, and your binary must be linked to the superlu library and its dependencies.\n"
"  * The dependencies depend on how superlu has been compiled.\n"
"  * For a cmake based project, you can use our FindSuperLU.cmake module to help you in this task.\n"
"  *\n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/SuperLUSupport/SuperLUSupport.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_SUPERLUSUPPORT_MODULE_H\n";

const char* SuperLUSupport = (const char*) temp_binary_data_29;

//================== SVD ==================
static const unsigned char temp_binary_data_30[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_SVD_MODULE_H\n"
"#define EIGEN_SVD_MODULE_H\n"
"\n"
"#include \"QR\"\n"
"#include \"Householder\"\n"
"#include \"Jacobi\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"/** \\defgroup SVD_Module SVD module\n"
"  *\n"
"  *\n"
"  *\n"
"  * This module provides SVD decomposition for matrices (both real and complex).\n"
"  * Two decomposition algorithms are provided:\n"
"  *  - JacobiSVD implementing two-sided Jacobi iterations is numerically very accurate, fast for small matrices, but very slow for larger ones.\n"
"  *  - BDCSVD implementing a recursive divide & conquer strategy on top of an upper-bidiagonalization which remains fast for large problems.\n"
"  * These decompositions are accessible via the respective classes and following MatrixBase methods:\n"
"  *  - MatrixBase::jacobiSvd()\n"
"  *  - MatrixBase::bdcSvd()\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/SVD>\n"
"  * \\endcode\n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/misc/RealSvd2x2.h\"\n"
"#include \"src/SVD/UpperBidiagonalization.h\"\n"
"#include \"src/SVD/SVDBase.h\"\n"
"#include \"src/SVD/JacobiSVD.h\"\n"
"#include \"src/SVD/BDCSVD.h\"\n"
"#ifdef EIGEN_USE_LAPACKE\n"
"#ifdef EIGEN_USE_MKL\n"
"#include \"mkl_lapacke.h\"\n"
"#else\n"
"#include \"src/misc/lapacke.h\"\n"
"#endif\n"
"#ifndef EIGEN_USE_LAPACKE_STRICT\n"
"#include \"src/SVD/JacobiSVD_LAPACKE.h\"\n"
"#endif\n"
"#include \"src/SVD/BDCSVD_LAPACKE.h\"\n"
"#endif\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_SVD_MODULE_H\n";

const char* SVD = (const char*) temp_binary_data_30;

//================== ThreadPool ==================
static const unsigned char temp_binary_data_31[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// Copyright (C) 2016 Benoit Steiner <benoit.steiner.goog@gmail.com>\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_THREADPOOL_MODULE_H\n"
"#define EIGEN_THREADPOOL_MODULE_H\n"
"\n"
"#include \"Core\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"/** \\defgroup ThreadPool_Module ThreadPool Module\n"
"  *\n"
"  * This module provides 2 threadpool implementations\n"
"  *  - a simple reference implementation\n"
"  *  - a faster non blocking implementation\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/ThreadPool>\n"
"  * \\endcode\n"
"  */\n"
"\n"
"#include <cstddef>\n"
"#include <cstring>\n"
"#include <time.h>\n"
"\n"
"#include <vector>\n"
"#include <atomic>\n"
"#include <condition_variable>\n"
"#include <deque>\n"
"#include <mutex>\n"
"#include <thread>\n"
"#include <functional>\n"
"#include <memory>\n"
"#include <utility>\n"
"\n"
"// There are non-parenthesized calls to \"max\" in the  <unordered_map> header,\n"
"// which trigger a check in test/main.h causing compilation to fail.\n"
"// We work around the check here by removing the check for max in\n"
"// the case where we have to emulate thread_local.\n"
"#ifdef max\n"
"#undef max\n"
"#endif\n"
"#include <unordered_map>\n"
"\n"
"#include \"src/Core/util/Meta.h\"\n"
"#include \"src/Core/util/MaxSizeVector.h\"\n"
"\n"
"#ifndef EIGEN_MUTEX\n"
"#define EIGEN_MUTEX std::mutex\n"
"#endif\n"
"#ifndef EIGEN_MUTEX_LOCK\n"
"#define EIGEN_MUTEX_LOCK std::unique_lock<std::mutex>\n"
"#endif\n"
"#ifndef EIGEN_CONDVAR\n"
"#define EIGEN_CONDVAR std::condition_variable\n"
"#endif\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/ThreadPool/ThreadLocal.h\"\n"
"#include \"src/ThreadPool/ThreadYield.h\"\n"
"#include \"src/ThreadPool/ThreadCancel.h\"\n"
"#include \"src/ThreadPool/EventCount.h\"\n"
"#include \"src/ThreadPool/RunQueue.h\"\n"
"#include \"src/ThreadPool/ThreadPoolInterface.h\"\n"
"#include \"src/ThreadPool/ThreadEnvironment.h\"\n"
"#include \"src/ThreadPool/Barrier.h\"\n"
"#include \"src/ThreadPool/NonBlockingThreadPool.h\"\n"
"// IWYU pragma: end_exports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_CXX11_THREADPOOL_MODULE_H\n";

const char* ThreadPool = (const char*) temp_binary_data_31;

//================== UmfPackSupport ==================
static const unsigned char temp_binary_data_32[] =
"// This file is part of Eigen, a lightweight C++ template library\n"
"// for linear algebra.\n"
"//\n"
"// This Source Code Form is subject to the terms of the Mozilla\n"
"// Public License v. 2.0. If a copy of the MPL was not distributed\n"
"// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
"\n"
"#ifndef EIGEN_UMFPACKSUPPORT_MODULE_H\n"
"#define EIGEN_UMFPACKSUPPORT_MODULE_H\n"
"\n"
"#include \"SparseCore\"\n"
"\n"
"#include \"src/Core/util/DisableStupidWarnings.h\"\n"
"\n"
"extern \"C\" {\n"
"#include <umfpack.h>\n"
"}\n"
"\n"
"/** \\ingroup Support_modules\n"
"  * \\defgroup UmfPackSupport_Module UmfPackSupport module\n"
"  *\n"
"  * This module provides an interface to the UmfPack library which is part of the <a href=\"http://www.suitesparse.com\">suitesparse</a> package.\n"
"  * It provides the following factorization class:\n"
"  * - class UmfPackLU: a multifrontal sequential LU factorization.\n"
"  *\n"
"  * \\code\n"
"  * #include <Eigen/UmfPackSupport>\n"
"  * \\endcode\n"
"  *\n"
"  * In order to use this module, the umfpack headers must be accessible from the include paths, and your binary must be linked to the umfpack library and its dependencies.\n"
"  * The dependencies depend on how umfpack has been compiled.\n"
"  * For a cmake based project, you can use our FindUmfPack.cmake module to help you in this task.\n"
"  *\n"
"  */\n"
"\n"
"// IWYU pragma: begin_exports\n"
"#include \"src/UmfPackSupport/UmfPackSupport.h\"\n"
"// IWYU pragma: endexports\n"
"\n"
"#include \"src/Core/util/ReenableStupidWarnings.h\"\n"
"\n"
"#endif // EIGEN_UMFPACKSUPPORT_MODULE_H\n";

const char* UmfPackSupport = (const char*) temp_binary_data_32;

//================== CMakeLists.txt ==================
static const unsigned char temp_binary_data_33[] =
"# cmake_minimum_require must be the first command of the file\n"
"cmake_minimum_required(VERSION 3.10.0)\n"
"\n"
"# NOTE Remove setting the policy once the minimum required CMake version is\n"
"# increased to at least 3.15. Retain enabling the export to package registry.\n"
"if (POLICY CMP0090)\n"
"  # The export command does not populate package registry by default\n"
"  cmake_policy (SET CMP0090 NEW)\n"
"\n"
"  # Unless otherwise specified, always export to package registry to ensure\n"
"  # backwards compatibility.\n"
"  if (NOT DEFINED CMAKE_EXPORT_PACKAGE_REGISTRY)\n"
"    set (CMAKE_EXPORT_PACKAGE_REGISTRY ON)\n"
"  endif (NOT DEFINED CMAKE_EXPORT_PACKAGE_REGISTRY)\n"
"endif (POLICY CMP0090)\n"
"\n"
"project(Eigen3)\n"
"\n"
"# Remove this block after bumping CMake to v3.21.0\n"
"# PROJECT_IS_TOP_LEVEL is defined then by default\n"
"if(CMAKE_VERSION VERSION_LESS 3.21.0)\n"
"  if(CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR)\n"
"    set(PROJECT_IS_TOP_LEVEL TRUE)\n"
"  else()\n"
"    set(PROJECT_IS_TOP_LEVEL FALSE)\n"
"  endif()\n"
"endif()\n"
"\n"
"if(PROJECT_IS_TOP_LEVEL)\n"
"  set(CMAKE_CXX_STANDARD 14 CACHE STRING \"Default C++ standard\")\n"
"  set(CMAKE_CXX_STANDARD_REQUIRED ON CACHE BOOL \"Require C++ standard\")\n"
"  set(CMAKE_CXX_EXTENSIONS OFF CACHE BOOL \"Allow C++ extensions\")\n"
"endif()\n"
"\n"
"# guard against in-source builds\n"
"\n"
"if(${CMAKE_SOURCE_DIR} STREQUAL ${CMAKE_BINARY_DIR})\n"
"  message(FATAL_ERROR \"In-source builds not allowed. Please make a new directory (called a build directory) and run CMake from there. You may need to remove CMakeCache.txt. \")\n"
"endif()\n"
"\n"
"\n"
"# Alias Eigen_*_DIR to Eigen3_*_DIR:\n"
"\n"
"set(Eigen_SOURCE_DIR ${Eigen3_SOURCE_DIR})\n"
"set(Eigen_BINARY_DIR ${Eigen3_BINARY_DIR})\n"
"\n"
"# guard against bad build-type strings\n"
"\n"
"if (NOT CMAKE_BUILD_TYPE)\n"
"  set(CMAKE_BUILD_TYPE \"Release\")\n"
"endif()\n"
"\n"
"\n"
"#############################################################################\n"
"# retrieve version information                                              #\n"
"#############################################################################\n"
"\n"
"# automatically parse the version number\n"
"file(READ \"${PROJECT_SOURCE_DIR}/Eigen/src/Core/util/Macros.h\" _eigen_version_header)\n"
"string(REGEX MATCH \"define[ \\t]+EIGEN_WORLD_VERSION[ \\t]+([0-9]+)\" _eigen_world_version_match \"${_eigen_version_header}\")\n"
"set(EIGEN_WORLD_VERSION \"${CMAKE_MATCH_1}\")\n"
"string(REGEX MATCH \"define[ \\t]+EIGEN_MAJOR_VERSION[ \\t]+([0-9]+)\" _eigen_major_version_match \"${_eigen_version_header}\")\n"
"set(EIGEN_MAJOR_VERSION \"${CMAKE_MATCH_1}\")\n"
"string(REGEX MATCH \"define[ \\t]+EIGEN_MINOR_VERSION[ \\t]+([0-9]+)\" _eigen_minor_version_match \"${_eigen_version_header}\")\n"
"set(EIGEN_MINOR_VERSION \"${CMAKE_MATCH_1}\")\n"
"set(EIGEN_VERSION_NUMBER ${EIGEN_WORLD_VERSION}.${EIGEN_MAJOR_VERSION}.${EIGEN_MINOR_VERSION})\n"
"\n"
"# if we are not in a git clone\n"
"if(IS_DIRECTORY ${CMAKE_SOURCE_DIR}/.git)\n"
"  # if the git program is absent or this will leave the EIGEN_GIT_REVNUM string empty,\n"
"  # but won't stop CMake.\n"
"  execute_process(COMMAND git ls-remote --refs -q ${CMAKE_SOURCE_DIR} HEAD OUTPUT_VARIABLE EIGEN_GIT_OUTPUT)\n"
"endif()\n"
"\n"
"# extract the git rev number from the git output...\n"
"if(EIGEN_GIT_OUTPUT)\n"
"string(REGEX MATCH \"^([0-9;a-f]+).*\" EIGEN_GIT_CHANGESET_MATCH \"${EIGEN_GIT_OUTPUT}\")\n"
"set(EIGEN_GIT_REVNUM \"${CMAKE_MATCH_1}\")\n"
"endif()\n"
"#...and show it next to the version number\n"
"if(EIGEN_GIT_REVNUM)\n"
"  set(EIGEN_VERSION \"${EIGEN_VERSION_NUMBER} (git rev ${EIGEN_GIT_REVNUM})\")\n"
"else()\n"
"  set(EIGEN_VERSION \"${EIGEN_VERSION_NUMBER}\")\n"
"endif()\n"
"\n"
"include(CheckCXXCompilerFlag)\n"
"include(GNUInstallDirs)\n"
"include(CMakeDependentOption)\n"
"\n"
"set(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake)\n"
"\n"
"macro(ei_add_cxx_compiler_flag FLAG)\n"
"  string(REGEX REPLACE \"-\" \"\" SFLAG1 ${FLAG})\n"
"  string(REGEX REPLACE \"\\\\+\" \"p\" SFLAG ${SFLAG1})\n"
"  check_cxx_compiler_flag(${FLAG} COMPILER_SUPPORT_${SFLAG})\n"
"  if(COMPILER_SUPPORT_${SFLAG})\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${FLAG}\")\n"
"  endif()\n"
"endmacro()\n"
"\n"
"function(ei_maybe_separate_arguments variable mode args)\n"
"  # Use separate_arguments if the input is a single string containing a space.\n"
"  # Otherwise, if it is already a list or doesn't have a space, just propagate\n"
"  # the original value.  This is to better support multi-argument lists.\n"
"  list(LENGTH args list_length)\n"
"  if (${list_length} EQUAL 1)\n"
"    string(FIND \"${args}\" \" \" has_space)\n"
"    if (${has_space} GREATER -1)\n"
"      separate_arguments(args ${mode} \"${args}\")\n"
"    endif()\n"
"  endif()\n"
"  set(${variable} ${args} PARENT_SCOPE)\n"
"endfunction(ei_maybe_separate_arguments)\n"
"\n"
"# Determine if we should build shared libraries on this platform.\n"
"get_cmake_property(EIGEN_BUILD_SHARED_LIBS TARGET_SUPPORTS_SHARED_LIBS)\n"
"\n"
"#############################################################################\n"
"# find how to link to the standard libraries                                #\n"
"#############################################################################\n"
"\n"
"find_package(StandardMathLibrary)\n"
"\n"
"\n"
"set(EIGEN_TEST_CUSTOM_LINKER_FLAGS  \"\" CACHE STRING \"Additional linker flags when linking unit tests.\")\n"
"set(EIGEN_TEST_CUSTOM_CXX_FLAGS     \"\" CACHE STRING \"Additional compiler flags when compiling unit tests.\")\n"
"\n"
"# Convert space-separated arguments into CMake lists for downstream consumption.\n"
"ei_maybe_separate_arguments(EIGEN_TEST_CUSTOM_LINKER_FLAGS NATIVE_COMMAND \"${EIGEN_TEST_CUSTOM_LINKER_FLAGS}\")\n"
"ei_maybe_separate_arguments(EIGEN_TEST_CUSTOM_CXX_FLAGS NATIVE_COMMAND \"${EIGEN_TEST_CUSTOM_CXX_FLAGS}\")\n"
"\n"
"\n"
"set(EIGEN_STANDARD_LIBRARIES_TO_LINK_TO \"\")\n"
"\n"
"if(NOT STANDARD_MATH_LIBRARY_FOUND)\n"
"\n"
"  message(FATAL_ERROR\n"
"    \"Can't link to the standard math library. Please report to the Eigen developers, telling them about your platform.\")\n"
"\n"
"else()\n"
"  if(EIGEN_STANDARD_LIBRARIES_TO_LINK_TO)\n"
"    set(EIGEN_STANDARD_LIBRARIES_TO_LINK_TO \"${EIGEN_STANDARD_LIBRARIES_TO_LINK_TO} ${STANDARD_MATH_LIBRARY}\")\n"
"  else()\n"
"    set(EIGEN_STANDARD_LIBRARIES_TO_LINK_TO \"${STANDARD_MATH_LIBRARY}\")\n"
"  endif()\n"
"endif()\n"
"\n"
"if(EIGEN_STANDARD_LIBRARIES_TO_LINK_TO)\n"
"  message(STATUS \"Standard libraries to link to explicitly: ${EIGEN_STANDARD_LIBRARIES_TO_LINK_TO}\")\n"
"else()\n"
"  message(STATUS \"Standard libraries to link to explicitly: none\")\n"
"endif()\n"
"\n"
"option(EIGEN_BUILD_BTL \"Build benchmark suite\" OFF)\n"
"option(EIGEN_BUILD_SPBENCH \"Build sparse benchmark suite\" OFF)\n"
"\n"
"# Disable pkgconfig only for native Windows builds\n"
"if(NOT WIN32 OR NOT CMAKE_HOST_SYSTEM_NAME MATCHES Windows)\n"
"  option(EIGEN_BUILD_PKGCONFIG \"Build pkg-config .pc file for Eigen\" ON)\n"
"endif()\n"
"\n"
"set(CMAKE_INCLUDE_CURRENT_DIR OFF)\n"
"\n"
"option(EIGEN_SPLIT_LARGE_TESTS \"Split large tests into smaller executables\" ON)\n"
"\n"
"option(EIGEN_DEFAULT_TO_ROW_MAJOR \"Use row-major as default matrix storage order\" OFF)\n"
"if(EIGEN_DEFAULT_TO_ROW_MAJOR)\n"
"  add_definitions(\"-DEIGEN_DEFAULT_TO_ROW_MAJOR\")\n"
"endif()\n"
"\n"
"set(EIGEN_TEST_MAX_SIZE \"320\" CACHE STRING \"Maximal matrix/vector size, default is 320\")\n"
"\n"
"if(NOT MSVC)\n"
"  # We assume that other compilers are partly compatible with GNUCC\n"
"\n"
"  # clang outputs some warnings for unknown flags that are not caught by check_cxx_compiler_flag\n"
"  # adding -Werror turns such warnings into errors\n"
"  check_cxx_compiler_flag(\"-Werror\" COMPILER_SUPPORT_WERROR)\n"
"  if(COMPILER_SUPPORT_WERROR)\n"
"    set(CMAKE_REQUIRED_FLAGS \"-Werror\")\n"
"  endif()\n"
"  ei_add_cxx_compiler_flag(\"-pedantic\")\n"
"  ei_add_cxx_compiler_flag(\"-Wall\")\n"
"  ei_add_cxx_compiler_flag(\"-Wextra\")\n"
"  #ei_add_cxx_compiler_flag(\"-Weverything\")              # clang\n"
"\n"
"  ei_add_cxx_compiler_flag(\"-Wundef\")\n"
"  ei_add_cxx_compiler_flag(\"-Wcast-align\")\n"
"  ei_add_cxx_compiler_flag(\"-Wchar-subscripts\")\n"
"  ei_add_cxx_compiler_flag(\"-Wnon-virtual-dtor\")\n"
"  ei_add_cxx_compiler_flag(\"-Wunused-local-typedefs\")\n"
"  ei_add_cxx_compiler_flag(\"-Wpointer-arith\")\n"
"  ei_add_cxx_compiler_flag(\"-Wwrite-strings\")\n"
"  ei_add_cxx_compiler_flag(\"-Wformat-security\")\n"
"  ei_add_cxx_compiler_flag(\"-Wshorten-64-to-32\")\n"
"  ei_add_cxx_compiler_flag(\"-Wlogical-op\")\n"
"  ei_add_cxx_compiler_flag(\"-Wenum-conversion\")\n"
"  ei_add_cxx_compiler_flag(\"-Wc++11-extensions\")\n"
"  ei_add_cxx_compiler_flag(\"-Wdouble-promotion\")\n"
"#  ei_add_cxx_compiler_flag(\"-Wconversion\")\n"
"\n"
"  ei_add_cxx_compiler_flag(\"-Wshadow\")\n"
"\n"
"  ei_add_cxx_compiler_flag(\"-Wno-psabi\")\n"
"  ei_add_cxx_compiler_flag(\"-Wno-variadic-macros\")\n"
"  ei_add_cxx_compiler_flag(\"-Wno-long-long\")\n"
"\n"
"  ei_add_cxx_compiler_flag(\"-fno-check-new\")\n"
"  ei_add_cxx_compiler_flag(\"-fno-common\")\n"
"  ei_add_cxx_compiler_flag(\"-fstrict-aliasing\")\n"
"  ei_add_cxx_compiler_flag(\"-wd981\")                    # disable ICC's \"operands are evaluated in unspecified order\" remark\n"
"  ei_add_cxx_compiler_flag(\"-wd2304\")                   # disable ICC's \"warning #2304: non-explicit constructor with single argument may cause implicit type conversion\" produced by -Wnon-virtual-dtor\n"
"\n"
"  if(ANDROID_NDK)\n"
"    ei_add_cxx_compiler_flag(\"-pie\")\n"
"    ei_add_cxx_compiler_flag(\"-fPIE\")\n"
"  endif()\n"
"\n"
"  set(CMAKE_REQUIRED_FLAGS \"\")\n"
"\n"
"  option(EIGEN_TEST_SSE2 \"Enable/Disable SSE2 in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_SSE2)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -msse2\")\n"
"    message(STATUS \"Enabling SSE2 in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_SSE3 \"Enable/Disable SSE3 in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_SSE3)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -msse3\")\n"
"    message(STATUS \"Enabling SSE3 in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_SSSE3 \"Enable/Disable SSSE3 in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_SSSE3)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mssse3\")\n"
"    message(STATUS \"Enabling SSSE3 in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_SSE4_1 \"Enable/Disable SSE4.1 in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_SSE4_1)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -msse4.1\")\n"
"    message(STATUS \"Enabling SSE4.1 in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_SSE4_2 \"Enable/Disable SSE4.2 in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_SSE4_2)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -msse4.2\")\n"
"    message(STATUS \"Enabling SSE4.2 in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_AVX \"Enable/Disable AVX in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_AVX)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mavx\")\n"
"    message(STATUS \"Enabling AVX in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_FMA \"Enable/Disable FMA in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_FMA AND NOT EIGEN_TEST_NEON)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mfma\")\n"
"    message(STATUS \"Enabling FMA in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_AVX2 \"Enable/Disable AVX2 in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_AVX2)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mavx2 -mfma\")\n"
"    message(STATUS \"Enabling AVX2 in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_AVX512 \"Enable/Disable AVX512 in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_AVX512)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mavx512f -mfma\")\n"
"    message(STATUS \"Enabling AVX512 in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_AVX512DQ \"Enable/Disable AVX512DQ in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_AVX512DQ)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mavx512dq -mfma\")\n"
"    message(STATUS \"Enabling AVX512DQ in tests/examples\")\n"
"  endif()\n"
"  \n"
"  option(EIGEN_TEST_AVX512FP16 \"Enable/Disable AVX512-FP16 in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_AVX512FP16)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mavx512f -mfma -mavx512vl -mavx512fp16\")\n"
"\tmessage(STATUS \"Enabling AVX512-FP16 in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_F16C \"Enable/Disable F16C in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_F16C)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mf16c\")\n"
"    message(STATUS \"Enabling F16C in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_ALTIVEC \"Enable/Disable AltiVec in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_ALTIVEC)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -maltivec -mabi=altivec\")\n"
"    message(STATUS \"Enabling AltiVec in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_VSX \"Enable/Disable VSX in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_VSX)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -m64 -mvsx\")\n"
"    message(STATUS \"Enabling VSX in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_MSA \"Enable/Disable MSA in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_MSA)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mmsa\")\n"
"    message(STATUS \"Enabling MSA in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_NEON \"Enable/Disable Neon in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_NEON)\n"
"    if(EIGEN_TEST_FMA)\n"
"      set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mfpu=neon-vfpv4\")\n"
"    else()\n"
"      set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mfpu=neon\")\n"
"    endif()\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mfloat-abi=hard\")\n"
"    message(STATUS \"Enabling NEON in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_NEON64 \"Enable/Disable Neon in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_NEON64)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\")\n"
"    message(STATUS \"Enabling NEON in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_Z13 \"Enable/Disable S390X(zEC13) ZVECTOR in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_Z13)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -march=z13 -mzvector\")\n"
"    message(STATUS \"Enabling S390X(zEC13) ZVECTOR in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_Z14 \"Enable/Disable S390X(zEC14) ZVECTOR in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_Z14)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -march=z14 -mzvector\")\n"
"    message(STATUS \"Enabling S390X(zEC13) ZVECTOR in tests/examples\")\n"
"  endif()\n"
"\n"
"  check_cxx_compiler_flag(\"-fopenmp\" COMPILER_SUPPORT_OPENMP)\n"
"  if(COMPILER_SUPPORT_OPENMP)\n"
"    option(EIGEN_TEST_OPENMP \"Enable/Disable OpenMP in tests/examples\" OFF)\n"
"    if(EIGEN_TEST_OPENMP)\n"
"      set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fopenmp\")\n"
"      message(STATUS \"Enabling OpenMP in tests/examples\")\n"
"    endif()\n"
"  endif()\n"
"\n"
"else()\n"
"\n"
"  # C4127 - conditional expression is constant\n"
"  # C4714 - marked as __forceinline not inlined (I failed to deactivate it selectively)\n"
"  #         We can disable this warning in the unit tests since it is clear that it occurs\n"
"  #         because we are oftentimes returning objects that have a destructor or may\n"
"  #         throw exceptions - in particular in the unit tests we are throwing extra many\n"
"  #         exceptions to cover indexing errors.\n"
"  # C4505 - unreferenced local function has been removed (impossible to deactivate selectively)\n"
"  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /EHsc /wd4127 /wd4505 /wd4714\")\n"
"\n"
"  # replace all /Wx by /W4\n"
"  string(REGEX REPLACE \"/W[0-9]\" \"/W4\" CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\")\n"
"\n"
"  check_cxx_compiler_flag(\"/openmp\" COMPILER_SUPPORT_OPENMP)\n"
"  if(COMPILER_SUPPORT_OPENMP)\n"
"    option(EIGEN_TEST_OPENMP \"Enable/Disable OpenMP in tests/examples\" OFF)\n"
"    if(EIGEN_TEST_OPENMP)\n"
"      set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /openmp\")\n"
"      message(STATUS \"Enabling OpenMP in tests/examples\")\n"
"    endif()\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_SSE2 \"Enable/Disable SSE2 in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_SSE2)\n"
"    if(NOT CMAKE_CL_64)\n"
"      # arch is not supported on 64 bit systems, SSE is enabled automatically.\n"
"      set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /arch:SSE2\")\n"
"    endif()\n"
"    message(STATUS \"Enabling SSE2 in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_AVX \"Enable/Disable AVX in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_AVX)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /arch:AVX\")\n"
"    message(STATUS \"Enabling AVX in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_FMA \"Enable/Disable FMA/AVX2 in tests/examples\" OFF)\n"
"  option(EIGEN_TEST_AVX2 \"Enable/Disable FMA/AVX2 in tests/examples\" OFF)\n"
"  if((EIGEN_TEST_FMA AND NOT EIGEN_TEST_NEON) OR EIGEN_TEST_AVX2)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /arch:AVX2\")\n"
"    message(STATUS \"Enabling FMA/AVX2 in tests/examples\")\n"
"  endif()\n"
"\n"
"  option(EIGEN_TEST_AVX512 \"Enable/Disable AVX512 in tests/examples\" OFF)\n"
"  option(EIGEN_TEST_AVX512DQ \"Enable/Disable AVX512DQ in tests/examples\" OFF)\n"
"  if(EIGEN_TEST_AVX512 OR EIGEN_TEST_AVX512DQ)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /arch:AVX512\")\n"
"    message(STATUS \"Enabling AVX512 in tests/examples\")\n"
"  endif()\n"
"\n"
"endif()\n"
"\n"
"option(EIGEN_TEST_NO_EXPLICIT_VECTORIZATION \"Disable explicit vectorization in tests/examples\" OFF)\n"
"option(EIGEN_TEST_X87 \"Force using X87 instructions. Implies no vectorization.\" OFF)\n"
"option(EIGEN_TEST_32BIT \"Force generating 32bit code.\" OFF)\n"
"\n"
"if(EIGEN_TEST_X87)\n"
"  set(EIGEN_TEST_NO_EXPLICIT_VECTORIZATION ON)\n"
"  if(CMAKE_COMPILER_IS_GNUCXX)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mfpmath=387\")\n"
"    message(STATUS \"Forcing use of x87 instructions in tests/examples\")\n"
"  else()\n"
"    message(STATUS \"EIGEN_TEST_X87 ignored on your compiler\")\n"
"  endif()\n"
"endif()\n"
"\n"
"if(EIGEN_TEST_32BIT)\n"
"  if(CMAKE_COMPILER_IS_GNUCXX)\n"
"    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -m32\")\n"
"    message(STATUS \"Forcing generation of 32-bit code in tests/examples\")\n"
"  else()\n"
"    message(STATUS \"EIGEN_TEST_32BIT ignored on your compiler\")\n"
"  endif()\n"
"endif()\n"
"\n"
"if(EIGEN_TEST_NO_EXPLICIT_VECTORIZATION)\n"
"  add_definitions(-DEIGEN_DONT_VECTORIZE=1)\n"
"  message(STATUS \"Disabling vectorization in tests/examples\")\n"
"endif()\n"
"\n"
"option(EIGEN_TEST_NO_EXPLICIT_ALIGNMENT \"Disable explicit alignment (hence vectorization) in tests/examples\" OFF)\n"
"if(EIGEN_TEST_NO_EXPLICIT_ALIGNMENT)\n"
"  add_definitions(-DEIGEN_DONT_ALIGN=1)\n"
"  message(STATUS \"Disabling alignment in tests/examples\")\n"
"endif()\n"
"\n"
"option(EIGEN_TEST_NO_EXCEPTIONS \"Disables C++ exceptions\" OFF)\n"
"if(EIGEN_TEST_NO_EXCEPTIONS)\n"
"  ei_add_cxx_compiler_flag(\"-fno-exceptions\")\n"
"  message(STATUS \"Disabling exceptions in tests/examples\")\n"
"endif()\n"
"\n"
"set(EIGEN_CUDA_CXX_FLAGS \"\" CACHE STRING \"Additional flags to pass to the cuda compiler.\")\n"
"set(EIGEN_CUDA_COMPUTE_ARCH 30 CACHE STRING \"The CUDA compute architecture(s) to target when compiling CUDA code\")\n"
"\n"
"include_directories(${CMAKE_CURRENT_SOURCE_DIR})\n"
"\n"
"# Backward compatibility support for EIGEN_INCLUDE_INSTALL_DIR\n"
"if(EIGEN_INCLUDE_INSTALL_DIR)\n"
"  message(WARNING \"EIGEN_INCLUDE_INSTALL_DIR is deprecated. Use INCLUDE_INSTALL_DIR instead.\")\n"
"endif()\n"
"\n"
"if(EIGEN_INCLUDE_INSTALL_DIR AND NOT INCLUDE_INSTALL_DIR)\n"
"  set(INCLUDE_INSTALL_DIR ${EIGEN_INCLUDE_INSTALL_DIR}\n"
"      CACHE PATH \"The directory relative to CMAKE_INSTALL_PREFIX where Eigen header files are installed\")\n"
"else()\n"
"  set(INCLUDE_INSTALL_DIR\n"
"      \"${CMAKE_INSTALL_INCLUDEDIR}/eigen3\"\n"
"      CACHE PATH \"The directory relative to CMAKE_INSTALL_PREFIX where Eigen header files are installed\"\n"
"      )\n"
"endif()\n"
"set(CMAKEPACKAGE_INSTALL_DIR\n"
"    \"${CMAKE_INSTALL_DATADIR}/eigen3/cmake\"\n"
"    CACHE PATH \"The directory relative to CMAKE_INSTALL_PREFIX where Eigen3Config.cmake is installed\"\n"
"    )\n"
"set(PKGCONFIG_INSTALL_DIR\n"
"    \"${CMAKE_INSTALL_DATADIR}/pkgconfig\"\n"
"    CACHE PATH \"The directory relative to CMAKE_INSTALL_PREFIX where eigen3.pc is installed\"\n"
"    )\n"
"\n"
"foreach(var INCLUDE_INSTALL_DIR CMAKEPACKAGE_INSTALL_DIR PKGCONFIG_INSTALL_DIR)\n"
"  # If an absolute path is specified, make it relative to \"{CMAKE_INSTALL_PREFIX}\".\n"
"  if(IS_ABSOLUTE \"${${var}}\")\n"
"    file(RELATIVE_PATH \"${var}\" \"${CMAKE_INSTALL_PREFIX}\" \"${${var}}\")\n"
"  endif()\n"
"endforeach()\n"
"\n"
"install(FILES\n"
"  signature_of_eigen3_matrix_library\n"
"  DESTINATION ${INCLUDE_INSTALL_DIR} COMPONENT Devel\n"
"  )\n"
"\n"
"if(EIGEN_BUILD_PKGCONFIG)\n"
"    configure_file(eigen3.pc.in eigen3.pc @ONLY)\n"
"    install(FILES ${CMAKE_CURRENT_BINARY_DIR}/eigen3.pc\n"
"        DESTINATION ${PKGCONFIG_INSTALL_DIR}\n"
"        )\n"
"endif()\n"
"\n"
"install(DIRECTORY Eigen DESTINATION ${INCLUDE_INSTALL_DIR} COMPONENT Devel)\n"
"\n"
"\n"
"option(EIGEN_BUILD_DOC \"Enable creation of Eigen documentation\" ON)\n"
"if(EIGEN_BUILD_DOC)\n"
"  add_subdirectory(doc EXCLUDE_FROM_ALL)\n"
"endif()\n"
"\n"
"\n"
"cmake_dependent_option(BUILD_TESTING \"Enable creation of tests.\" ON \"PROJECT_IS_TOP_LEVEL\" OFF)\n"
"option(EIGEN_BUILD_TESTING \"Enable creation of Eigen tests.\" ${BUILD_TESTING})\n"
"if(EIGEN_BUILD_TESTING)\n"
"  include(EigenConfigureTesting)\n"
"\n"
"  if(EIGEN_LEAVE_TEST_IN_ALL_TARGET)\n"
"    add_subdirectory(test) # can't do EXCLUDE_FROM_ALL here, breaks CTest\n"
"  else()\n"
"    add_subdirectory(test EXCLUDE_FROM_ALL)\n"
"  endif()\n"
"\n"
"  add_subdirectory(failtest)\n"
"endif()\n"
"\n"
"include(CMakeDetermineFortranCompiler)\n"
"option(EIGEN_BUILD_BLAS \"Toggles the building of the Eigen Blas library\" ${CMAKE_Fortran_COMPILER})\n"
"option(EIGEN_BUILD_LAPACK \"Toggles the building of the included Eigen LAPACK library\" ${CMAKE_Fortran_COMPILER})\n"
"if(EIGEN_LEAVE_TEST_IN_ALL_TARGET)\n"
"  add_subdirectory(blas)\n"
"  add_subdirectory(lapack)\n"
"else()\n"
"  add_subdirectory(blas EXCLUDE_FROM_ALL)\n"
"  add_subdirectory(lapack EXCLUDE_FROM_ALL)\n"
"endif()\n"
"\n"
"# add SYCL\n"
"option(EIGEN_TEST_SYCL \"Add Sycl support.\" OFF)\n"
"if(EIGEN_TEST_SYCL)\n"
"  option(EIGEN_SYCL_DPCPP \"Use the DPCPP Sycl implementation (DPCPP is default SYCL-Compiler).\" ON)\n"
"  option(EIGEN_SYCL_TRISYCL \"Use the triSYCL Sycl implementation.\" OFF)\n"
"  option(EIGEN_SYCL_ComputeCpp \"Use the DPCPP Sycl implementation.\" OFF)\n"
"  set(CMAKE_CXX_STANDARD 17)\n"
"  set(CMAKE_CXX_FLAGS  \"${CMAKE_CXX_FLAGS} -Wno-deprecated-declarations -Wno-shorten-64-to-32 -Wno-cast-align\") \n"
"  set(CMAKE_CXX_FLAGS  \"${CMAKE_CXX_FLAGS} -Wno-deprecated-copy-with-user-provided-copy -Wno-unused-variable\")\n"
"  set (CMAKE_MODULE_PATH \"${CMAKE_ROOT}/Modules\" \"cmake/Modules/\" \"${CMAKE_MODULE_PATH}\")\n"
"  find_package(Threads REQUIRED)\n"
"  if(EIGEN_SYCL_TRISYCL)\n"
"    message(STATUS \"Using triSYCL\")\n"
"    include(FindTriSYCL)\n"
"  elseif(EIGEN_SYCL_ComputeCpp)\n"
"    message(STATUS \"Using ComputeCPP SYCL\")\n"
"    include(FindComputeCpp)\n"
"    set(COMPUTECPP_DRIVER_DEFAULT_VALUE OFF)\n"
"    if (NOT MSVC)\n"
"      set(COMPUTECPP_DRIVER_DEFAULT_VALUE ON)\n"
"    endif()\n"
"    option(COMPUTECPP_USE_COMPILER_DRIVER\n"
"      \"Use ComputeCpp driver instead of a 2 steps compilation\"\n"
"      ${COMPUTECPP_DRIVER_DEFAULT_VALUE}\n"
"    )\n"
"  else() #Default SYCL compiler is DPCPP (EIGEN_SYCL_DPCPP)\n"
"    set(DPCPP_SYCL_TARGET \"spir64\" CACHE STRING \"Defualt target for Intel CPU/GPU\")\n"
"    message(STATUS \"Using DPCPP\")\n"
"    find_package(DPCPP)\n"
"    add_definitions(-DSYCL_COMPILER_IS_DPCPP)\n"
"  endif(EIGEN_SYCL_TRISYCL)\n"
"  if(EIGEN_DONT_VECTORIZE_SYCL)\n"
"    message(STATUS \"Disabling SYCL vectorization in tests/examples\")\n"
"    # When disabling SYCL vectorization, also disable Eigen default vectorization\n"
"    add_definitions(-DEIGEN_DONT_VECTORIZE=1)\n"
"    add_definitions(-DEIGEN_DONT_VECTORIZE_SYCL=1)\n"
"  endif()\n"
"endif()\n"
"\n"
"add_subdirectory(unsupported)\n"
"\n"
"add_subdirectory(demos EXCLUDE_FROM_ALL)\n"
"\n"
"# must be after test and unsupported, for configuring buildtests.in\n"
"add_subdirectory(scripts EXCLUDE_FROM_ALL)\n"
"\n"
"# TODO: consider also replacing EIGEN_BUILD_BTL by a custom target \"make btl\"?\n"
"if(EIGEN_BUILD_BTL)\n"
"  add_subdirectory(bench/btl EXCLUDE_FROM_ALL)\n"
"endif()\n"
"\n"
"find_package(CLANG_FORMAT 9 EXACT)\n"
"if(CLANG_FORMAT_FOUND)\n"
"set(FORMAT_SOURCES)\n"
"list(APPEND FORMAT_SUBDIRS blas bench demos \"doc\" Eigen include lapack scripts share unsupported test failtest)\n"
"foreach(DIR ${FORMAT_SUBDIRS})\n"
"    set(ABS_DIR ${CMAKE_CURRENT_SOURCE_DIR}/${DIR})\n"
"    file(GLOB_RECURSE ${DIR}_SOURCES ${ABS_DIR}/*.cc ${ABS_DIR}/*.h ${ABS_DIR}/*.cpp ${ABS_DIR}/*.hpp ${ABS_DIR}/*.c)\n"
"    list(APPEND FORMAT_SOURCES ${${DIR}_SOURCES})\n"
"  endforeach()\n"
"    file(GLOB FORMAT_SOURCES_WITHOUTENDING LIST_DIRECTORIES false ${CMAKE_CURRENT_SOURCE_DIR}/Eigen/* ${CMAKE_CURRENT_SOURCE_DIR}/Eigen/CXX11/* ${CMAKE_CURRENT_SOURCE_DIR}/unsupported/Eigen/* ${CMAKE_CURRENT_SOURCE_DIR}/unsupported/Eigen/CXX11/*)\n"
"    list(FILTER FORMAT_SOURCES_WITHOUTENDING EXCLUDE REGEX \".*.txt$\")\n"
"    list (APPEND FORMAT_SOURCES ${FORMAT_SOURCES_WITHOUTENDING})\n"
"    add_custom_target(format\n"
"    COMMAND ${CLANG_FORMAT_EXECUTABLE} -i -style=file ${FORMAT_SOURCES}\n"
"    DEPENDS ${FORMAT_SOURCES})\n"
"endif()\n"
"\n"
"if(NOT WIN32 AND EIGEN_BUILD_SPBENCH)\n"
"  add_subdirectory(bench/spbench EXCLUDE_FROM_ALL)\n"
"endif()\n"
"\n"
"configure_file(scripts/cdashtesting.cmake.in cdashtesting.cmake @ONLY)\n"
"\n"
"if(EIGEN_BUILD_TESTING)\n"
"  ei_testing_print_summary()\n"
"endif()\n"
"\n"
"message(STATUS \"\")\n"
"message(STATUS \"Configured Eigen ${EIGEN_VERSION_NUMBER}\")\n"
"message(STATUS \"\")\n"
"\n"
"if(PROJECT_IS_TOP_LEVEL)\n"
"  string(TOLOWER \"${CMAKE_GENERATOR}\" cmake_generator_tolower)\n"
"  if(cmake_generator_tolower MATCHES \"makefile\")\n"
"    message(STATUS \"Available targets (use: make TARGET):\")\n"
"  else()\n"
"    message(STATUS \"Available targets (use: cmake --build . --target TARGET):\")\n"
"  endif()\n"
"  message(STATUS \"---------+--------------------------------------------------------------\")\n"
"  message(STATUS \"Target   |   Description\")\n"
"  message(STATUS \"---------+--------------------------------------------------------------\")\n"
"  message(STATUS \"install  | Install Eigen. Headers will be installed to:\")\n"
"  message(STATUS \"         |     <CMAKE_INSTALL_PREFIX>/<INCLUDE_INSTALL_DIR>\")\n"
"  message(STATUS \"         |   Using the following values:\")\n"
"  message(STATUS \"         |     CMAKE_INSTALL_PREFIX: ${CMAKE_INSTALL_PREFIX}\")\n"
"  message(STATUS \"         |     INCLUDE_INSTALL_DIR:  ${INCLUDE_INSTALL_DIR}\")\n"
"  message(STATUS \"         |   Change the install location of Eigen headers using:\")\n"
"  message(STATUS \"         |     cmake . -DCMAKE_INSTALL_PREFIX=yourprefix\")\n"
"  message(STATUS \"         |   Or:\")\n"
"  message(STATUS \"         |     cmake . -DINCLUDE_INSTALL_DIR=yourdir\")\n"
"  message(STATUS \"doc      | Generate the API documentation, requires Doxygen & LaTeX\")\n"
"  if(EIGEN_BUILD_TESTING)\n"
"    message(STATUS \"check    | Build and run the unit-tests. Read this page:\")\n"
"    message(STATUS \"         |   http://eigen.tuxfamily.org/index.php?title=Tests\")\n"
"  endif()\n"
"  if(CLANG_FORMAT_FOUND)\n"
"    message(STATUS \"format   | Formats the source code according to .clang-format file\")\n"
"  endif()\n"
"  message(STATUS \"blas     | Build BLAS library (not the same thing as Eigen)\")\n"
"  message(STATUS \"uninstall| Remove files installed by the install target\")\n"
"  message(STATUS \"---------+--------------------------------------------------------------\")\n"
"  message(STATUS \"\")\n"
"endif()\n"
"\n"
"set ( EIGEN_VERSION_STRING ${EIGEN_VERSION_NUMBER} )\n"
"set ( EIGEN_VERSION_MAJOR  ${EIGEN_WORLD_VERSION} )\n"
"set ( EIGEN_VERSION_MINOR  ${EIGEN_MAJOR_VERSION} )\n"
"set ( EIGEN_VERSION_PATCH  ${EIGEN_MINOR_VERSION} )\n"
"\n"
"include (CMakePackageConfigHelpers)\n"
"\n"
"# Imported target support\n"
"add_library (eigen INTERFACE)\n"
"add_library (Eigen3::Eigen ALIAS eigen)\n"
"target_include_directories (eigen INTERFACE\n"
"  $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}>\n"
"  $<INSTALL_INTERFACE:${INCLUDE_INSTALL_DIR}>\n"
")\n"
"\n"
"# Export as title case Eigen\n"
"set_target_properties (eigen PROPERTIES EXPORT_NAME Eigen)\n"
"\n"
"install (TARGETS eigen EXPORT Eigen3Targets)\n"
"\n"
"option(EIGEN_BUILD_CMAKE_PACKAGE \"Enables the creation of EigenConfig.cmake and related files\" ON)\n"
"if(EIGEN_BUILD_CMAKE_PACKAGE)\n"
"configure_package_config_file (\n"
"  ${CMAKE_CURRENT_SOURCE_DIR}/cmake/Eigen3Config.cmake.in\n"
"  ${CMAKE_CURRENT_BINARY_DIR}/Eigen3Config.cmake\n"
"  INSTALL_DESTINATION ${CMAKEPACKAGE_INSTALL_DIR}\n"
"  NO_SET_AND_CHECK_MACRO # Eigen does not provide legacy style defines\n"
"  NO_CHECK_REQUIRED_COMPONENTS_MACRO # Eigen does not provide components\n"
")\n"
"\n"
"# NOTE Remove the first code path once the minimum required CMake version is\n"
"# bumped to 3.14 or above.\n"
"if (CMAKE_VERSION VERSION_LESS 3.14)\n"
"  # Remove CMAKE_SIZEOF_VOID_P from Eigen3ConfigVersion.cmake since Eigen does\n"
"  # not depend on architecture specific settings or libraries. More\n"
"  # specifically, an Eigen3Config.cmake generated from a 64 bit target can be\n"
"  # used for 32 bit targets as well (and vice versa).\n"
"  set (_Eigen3_CMAKE_SIZEOF_VOID_P ${CMAKE_SIZEOF_VOID_P})\n"
"  unset (CMAKE_SIZEOF_VOID_P)\n"
"  write_basic_package_version_file (Eigen3ConfigVersion.cmake\n"
"                                    VERSION ${EIGEN_VERSION_NUMBER}\n"
"                                    COMPATIBILITY SameMajorVersion)\n"
"  set (CMAKE_SIZEOF_VOID_P ${_Eigen3_CMAKE_SIZEOF_VOID_P})\n"
"else (CMAKE_VERSION VERSION_LESS 3.14)\n"
"  write_basic_package_version_file (Eigen3ConfigVersion.cmake\n"
"                                    VERSION ${EIGEN_VERSION_NUMBER}\n"
"                                    COMPATIBILITY SameMajorVersion\n"
"                                    ARCH_INDEPENDENT)\n"
"endif (CMAKE_VERSION VERSION_LESS 3.14)\n"
"\n"
"# The Eigen target will be located in the Eigen3 namespace. Other CMake\n"
"# targets can refer to it using Eigen3::Eigen.\n"
"export (TARGETS eigen NAMESPACE Eigen3:: FILE Eigen3Targets.cmake)\n"
"# Export Eigen3 package to CMake registry such that it can be easily found by\n"
"# CMake even if it has not been installed to a standard directory.\n"
"export (PACKAGE Eigen3)\n"
"\n"
"install (EXPORT Eigen3Targets NAMESPACE Eigen3:: DESTINATION ${CMAKEPACKAGE_INSTALL_DIR})\n"
"\n"
"install (FILES ${CMAKE_CURRENT_BINARY_DIR}/Eigen3Config.cmake\n"
"               ${CMAKE_CURRENT_BINARY_DIR}/Eigen3ConfigVersion.cmake\n"
"         DESTINATION ${CMAKEPACKAGE_INSTALL_DIR})\n"
"\n"
"# Add uninstall target\n"
"if(NOT TARGET uninstall)\n"
"  add_custom_target ( uninstall\n"
"      COMMAND ${CMAKE_COMMAND} -P ${CMAKE_CURRENT_SOURCE_DIR}/cmake/EigenUninstall.cmake)\n"
"endif()\n"
"endif()\n"
"\n"
"if (EIGEN_SPLIT_TESTSUITE)\n"
"  ei_split_testsuite(\"${EIGEN_SPLIT_TESTSUITE}\")\n"
"endif()\n";

const char* CMakeLists_txt = (const char*) temp_binary_data_33;

//================== modules.textClipping ==================
static const unsigned char temp_binary_data_34[] =
{ 98,112,108,105,115,116,48,48,209,1,2,88,85,84,73,45,68,97,116,97,211,3,4,5,6,7,8,95,16,36,99,111,109,46,97,112,112,108,101,46,116,114,97,100,105,116,105,111,110,97,108,45,109,97,99,45,112,108,97,105,110,45,116,101,120,116,95,16,22,112,117,98,108,105,
99,46,117,116,102,56,45,112,108,97,105,110,45,116,101,120,116,95,16,23,112,117,98,108,105,99,46,117,116,102,49,54,45,112,108,97,105,110,45,116,101,120,116,71,109,111,100,117,108,101,115,87,109,111,100,117,108,101,115,78,109,0,111,0,100,0,117,0,108,0,
101,0,115,0,8,11,20,27,66,91,117,125,133,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,148,0,0 };

const char* modules_textClipping = (const char*) temp_binary_data_34;

//================== batchnorm.tpp ==================
static const unsigned char temp_binary_data_35[] =
"#include \"batchnorm.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"#if !RTNEURAL_USE_EIGEN && !RTNEURAL_USE_XSIMD\n"
"\n"
"template <typename T>\n"
"BatchNorm1DLayer<T>::BatchNorm1DLayer(int size)\n"
"    : Layer<T>(size, size)\n"
"    , gamma(size, (T)1)\n"
"    , beta(size, (T)0)\n"
"    , running_mean(size, (T)0)\n"
"    , running_var(size, (T)1)\n"
"    , multiplier(size, (T)1)\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm1DLayer<T>::setGamma(const std::vector<T>& gammaVals)\n"
"{\n"
"    std::copy(gammaVals.begin(), gammaVals.end(), gamma.begin());\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm1DLayer<T>::setBeta(const std::vector<T>& betaVals)\n"
"{\n"
"    std::copy(betaVals.begin(), betaVals.end(), beta.begin());\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm1DLayer<T>::setRunningMean(const std::vector<T>& runningMean)\n"
"{\n"
"    std::copy(runningMean.begin(), runningMean.end(), running_mean.begin());\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm1DLayer<T>::setRunningVariance(const std::vector<T>& runningVar)\n"
"{\n"
"    std::copy(runningVar.begin(), runningVar.end(), running_var.begin());\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm1DLayer<T>::setEpsilon(T newEpsilon)\n"
"{\n"
"    epsilon = newEpsilon;\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm1DLayer<T>::updateMultiplier()\n"
"{\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"        multiplier[i] = gamma[i] / std::sqrt(running_var[i] + epsilon);\n"
"}\n"
"\n"
"//============================================================\n"
"template <typename T, int size, bool affine>\n"
"BatchNorm1DT<T, size, affine>::BatchNorm1DT()\n"
"{\n"
"    std::fill(std::begin(outs), std::end(outs), (T)0);\n"
"\n"
"    std::fill(std::begin(gamma), std::end(gamma), (T)1);\n"
"    std::fill(std::begin(beta), std::end(beta), (T)0);\n"
"    std::fill(std::begin(running_mean), std::end(running_mean), (T)0);\n"
"    std::fill(std::begin(running_var), std::end(running_var), (T)1);\n"
"    std::fill(std::begin(multiplier), std::end(multiplier), (T)1);\n"
"}\n"
"\n"
"template <typename T, int size, bool affine>\n"
"template <bool isAffine>\n"
"typename std::enable_if<isAffine, void>::type BatchNorm1DT<T, size, affine>::setGamma(const std::vector<T>& gammaVals)\n"
"{\n"
"    std::copy(gammaVals.begin(), gammaVals.end(), std::begin(gamma));\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T, int size, bool affine>\n"
"template <bool isAffine>\n"
"typename std::enable_if<isAffine, void>::type BatchNorm1DT<T, size, affine>::setBeta(const std::vector<T>& betaVals)\n"
"{\n"
"    std::copy(betaVals.begin(), betaVals.end(), std::begin(beta));\n"
"}\n"
"\n"
"template <typename T, int size, bool affine>\n"
"void BatchNorm1DT<T, size, affine>::setRunningMean(const std::vector<T>& runningMean)\n"
"{\n"
"    std::copy(runningMean.begin(), runningMean.end(), std::begin(running_mean));\n"
"}\n"
"\n"
"template <typename T, int size, bool affine>\n"
"void BatchNorm1DT<T, size, affine>::setRunningVariance(const std::vector<T>& runningVar)\n"
"{\n"
"    std::copy(runningVar.begin(), runningVar.end(), std::begin(running_var));\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T, int size, bool affine>\n"
"void BatchNorm1DT<T, size, affine>::setEpsilon(T newEpsilon)\n"
"{\n"
"    epsilon = newEpsilon;\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T, int size, bool affine>\n"
"void BatchNorm1DT<T, size, affine>::updateMultiplier()\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"        multiplier[i] = gamma[i] / std::sqrt(running_var[i] + epsilon);\n"
"}\n"
"#endif\n"
"}";

const char* batchnorm_tpp = (const char*) temp_binary_data_35;

//================== batchnorm_eigen.tpp ==================
static const unsigned char temp_binary_data_36[] =
"#include \"batchnorm_eigen.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"template <typename T>\n"
"BatchNorm1DLayer<T>::BatchNorm1DLayer(int size)\n"
"    : Layer<T>(size, size)\n"
"{\n"
"    gamma = Eigen::Vector<T, Eigen::Dynamic>::Ones(size);\n"
"    beta = Eigen::Vector<T, Eigen::Dynamic>::Zero(size);\n"
"    running_mean = Eigen::Vector<T, Eigen::Dynamic>::Zero(size);\n"
"    running_var = Eigen::Vector<T, Eigen::Dynamic>::Ones(size);\n"
"    multiplier = Eigen::Vector<T, Eigen::Dynamic>::Ones(size);\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm1DLayer<T>::setGamma(const std::vector<T>& gammaVals)\n"
"{\n"
"    std::copy(gammaVals.begin(), gammaVals.end(), gamma.begin());\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm1DLayer<T>::setBeta(const std::vector<T>& betaVals)\n"
"{\n"
"    std::copy(betaVals.begin(), betaVals.end(), beta.begin());\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm1DLayer<T>::setRunningMean(const std::vector<T>& runningMean)\n"
"{\n"
"    std::copy(runningMean.begin(), runningMean.end(), running_mean.begin());\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm1DLayer<T>::setRunningVariance(const std::vector<T>& runningVar)\n"
"{\n"
"    std::copy(runningVar.begin(), runningVar.end(), running_var.begin());\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm1DLayer<T>::setEpsilon(T newEpsilon)\n"
"{\n"
"    epsilon = newEpsilon;\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm1DLayer<T>::updateMultiplier()\n"
"{\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"        multiplier[i] = gamma[i] / std::sqrt(running_var[i] + epsilon);\n"
"}\n"
"\n"
"//============================================================\n"
"template <typename T, int size, bool affine>\n"
"BatchNorm1DT<T, size, affine>::BatchNorm1DT()\n"
"    : outs(outs_internal)\n"
"{\n"
"    gamma = Eigen::Vector<T, size>::Ones(size);\n"
"    beta = Eigen::Vector<T, size>::Zero(size);\n"
"    running_mean = Eigen::Vector<T, size>::Zero(size);\n"
"    running_var = Eigen::Vector<T, size>::Ones(size);\n"
"    multiplier = Eigen::Vector<T, size>::Ones(size);\n"
"}\n"
"\n"
"template <typename T, int size, bool affine>\n"
"template <bool isAffine>\n"
"typename std::enable_if<isAffine, void>::type BatchNorm1DT<T, size, affine>::setGamma(const std::vector<T>& gammaVals)\n"
"{\n"
"    std::copy(gammaVals.begin(), gammaVals.end(), std::begin(gamma));\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T, int size, bool affine>\n"
"template <bool isAffine>\n"
"typename std::enable_if<isAffine, void>::type BatchNorm1DT<T, size, affine>::setBeta(const std::vector<T>& betaVals)\n"
"{\n"
"    std::copy(betaVals.begin(), betaVals.end(), std::begin(beta));\n"
"}\n"
"\n"
"template <typename T, int size, bool affine>\n"
"void BatchNorm1DT<T, size, affine>::setRunningMean(const std::vector<T>& runningMean)\n"
"{\n"
"    std::copy(runningMean.begin(), runningMean.end(), std::begin(running_mean));\n"
"}\n"
"\n"
"template <typename T, int size, bool affine>\n"
"void BatchNorm1DT<T, size, affine>::setRunningVariance(const std::vector<T>& runningVar)\n"
"{\n"
"    std::copy(runningVar.begin(), runningVar.end(), std::begin(running_var));\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T, int size, bool affine>\n"
"void BatchNorm1DT<T, size, affine>::setEpsilon(T newEpsilon)\n"
"{\n"
"    epsilon = newEpsilon;\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T, int size, bool affine>\n"
"void BatchNorm1DT<T, size, affine>::updateMultiplier()\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"        multiplier[i] = gamma[i] / std::sqrt(running_var[i] + epsilon);\n"
"}\n"
"}\n";

const char* batchnorm_eigen_tpp = (const char*) temp_binary_data_36;

//================== batchnorm_xsimd.tpp ==================
static const unsigned char temp_binary_data_37[] =
"#include \"batchnorm_xsimd.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"template <typename T>\n"
"BatchNorm1DLayer<T>::BatchNorm1DLayer(int size)\n"
"    : Layer<T>(size, size)\n"
"    , gamma(size, (T)1)\n"
"    , beta(size, (T)0)\n"
"    , running_mean(size, (T)0)\n"
"    , running_var(size, (T)1)\n"
"    , multiplier(size, (T)1)\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm1DLayer<T>::setGamma(const std::vector<T>& gammaVals)\n"
"{\n"
"    std::copy(gammaVals.begin(), gammaVals.end(), gamma.begin());\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm1DLayer<T>::setBeta(const std::vector<T>& betaVals)\n"
"{\n"
"    std::copy(betaVals.begin(), betaVals.end(), beta.begin());\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm1DLayer<T>::setRunningMean(const std::vector<T>& runningMean)\n"
"{\n"
"    std::copy(runningMean.begin(), runningMean.end(), running_mean.begin());\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm1DLayer<T>::setRunningVariance(const std::vector<T>& runningVar)\n"
"{\n"
"    std::copy(runningVar.begin(), runningVar.end(), running_var.begin());\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm1DLayer<T>::setEpsilon(T newEpsilon)\n"
"{\n"
"    epsilon = newEpsilon;\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm1DLayer<T>::updateMultiplier()\n"
"{\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"        multiplier[i] = gamma[i] / std::sqrt(running_var[i] + epsilon);\n"
"}\n"
"\n"
"//============================================================\n"
"template <typename T, int size, bool affine>\n"
"BatchNorm1DT<T, size, affine>::BatchNorm1DT()\n"
"{\n"
"    std::fill(std::begin(outs), std::end(outs), (T)0);\n"
"\n"
"    std::fill(std::begin(gamma), std::end(gamma), (T)1);\n"
"    std::fill(std::begin(beta), std::end(beta), (T)0);\n"
"    std::fill(std::begin(running_mean), std::end(running_mean), (T)0);\n"
"    std::fill(std::begin(running_var), std::end(running_var), (T)1);\n"
"    std::fill(std::begin(multiplier), std::end(multiplier), (T)1);\n"
"}\n"
"\n"
"template <typename T, int size, bool affine>\n"
"template <bool isAffine>\n"
"typename std::enable_if<isAffine, void>::type BatchNorm1DT<T, size, affine>::setGamma(const std::vector<T>& gammaVals)\n"
"{\n"
"    std::copy(gammaVals.begin(), gammaVals.end(), reinterpret_cast<T*>(std::begin(gamma)));\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T, int size, bool affine>\n"
"template <bool isAffine>\n"
"typename std::enable_if<isAffine, void>::type BatchNorm1DT<T, size, affine>::setBeta(const std::vector<T>& betaVals)\n"
"{\n"
"    std::copy(betaVals.begin(), betaVals.end(), reinterpret_cast<T*>(std::begin(beta)));\n"
"}\n"
"\n"
"template <typename T, int size, bool affine>\n"
"void BatchNorm1DT<T, size, affine>::setRunningMean(const std::vector<T>& runningMean)\n"
"{\n"
"    std::copy(runningMean.begin(), runningMean.end(), reinterpret_cast<T*>(std::begin(running_mean)));\n"
"}\n"
"\n"
"template <typename T, int size, bool affine>\n"
"void BatchNorm1DT<T, size, affine>::setRunningVariance(const std::vector<T>& runningVar)\n"
"{\n"
"    std::copy(runningVar.begin(), runningVar.end(), reinterpret_cast<T*>(std::begin(running_var)));\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T, int size, bool affine>\n"
"void BatchNorm1DT<T, size, affine>::setEpsilon(T newEpsilon)\n"
"{\n"
"    epsilon = newEpsilon;\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T, int size, bool affine>\n"
"void BatchNorm1DT<T, size, affine>::updateMultiplier()\n"
"{\n"
"    for(int i = 0; i < v_out_size; ++i)\n"
"        multiplier[i] = gamma[i] / xsimd::sqrt(running_var[i] + epsilon);\n"
"}\n"
"}\n";

const char* batchnorm_xsimd_tpp = (const char*) temp_binary_data_37;

//================== batchnorm2d.tpp ==================
static const unsigned char temp_binary_data_38[] =
"#include \"batchnorm2d.h\"\n"
"\n"
"#if !RTNEURAL_USE_EIGEN && !RTNEURAL_USE_XSIMD\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"template <typename T>\n"
"BatchNorm2DLayer<T>::BatchNorm2DLayer(int in_num_filters, int in_num_features)\n"
"    : Layer<T>(in_num_filters * in_num_features, in_num_filters * in_num_features)\n"
"    , num_filters(in_num_filters)\n"
"    , num_features(in_num_features)\n"
"    , gamma(num_filters, (T)1)\n"
"    , beta(num_filters, (T)0)\n"
"    , running_mean(num_filters, (T)0)\n"
"    , running_var(num_filters, (T)1)\n"
"    , multiplier(num_filters, (T)1)\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm2DLayer<T>::setGamma(const std::vector<T>& gammaVals)\n"
"{\n"
"    std::copy(gammaVals.begin(), gammaVals.end(), gamma.begin());\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm2DLayer<T>::setBeta(const std::vector<T>& betaVals)\n"
"{\n"
"    std::copy(betaVals.begin(), betaVals.end(), beta.begin());\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm2DLayer<T>::setRunningMean(const std::vector<T>& runningMean)\n"
"{\n"
"    std::copy(runningMean.begin(), runningMean.end(), running_mean.begin());\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm2DLayer<T>::setRunningVariance(const std::vector<T>& runningVar)\n"
"{\n"
"    std::copy(runningVar.begin(), runningVar.end(), running_var.begin());\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm2DLayer<T>::setEpsilon(T newEpsilon)\n"
"{\n"
"    epsilon = newEpsilon;\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm2DLayer<T>::updateMultiplier()\n"
"{\n"
"    for(int i = 0; i < num_filters; ++i)\n"
"        multiplier[i] = gamma[i] / std::sqrt(running_var[i] + epsilon);\n"
"}\n"
"\n"
"//============================================================\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"BatchNorm2DT<T, num_filters_t, num_features_t, affine>::BatchNorm2DT()\n"
"{\n"
"    std::fill(std::begin(outs), std::end(outs), (T)0);\n"
"\n"
"    std::fill(std::begin(gamma), std::end(gamma), (T)1);\n"
"    std::fill(std::begin(beta), std::end(beta), (T)0);\n"
"    std::fill(std::begin(running_mean), std::end(running_mean), (T)0);\n"
"    std::fill(std::begin(running_var), std::end(running_var), (T)1);\n"
"    std::fill(std::begin(multiplier), std::end(multiplier), (T)1);\n"
"}\n"
"\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"template <bool isAffine>\n"
"typename std::enable_if<isAffine, void>::type BatchNorm2DT<T, num_filters_t, num_features_t, affine>::setGamma(const std::vector<T>& gammaVals)\n"
"{\n"
"    std::copy(gammaVals.begin(), gammaVals.end(), std::begin(gamma));\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"template <bool isAffine>\n"
"typename std::enable_if<isAffine, void>::type BatchNorm2DT<T, num_filters_t, num_features_t, affine>::setBeta(const std::vector<T>& betaVals)\n"
"{\n"
"    std::copy(betaVals.begin(), betaVals.end(), std::begin(beta));\n"
"}\n"
"\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"void BatchNorm2DT<T, num_filters_t, num_features_t, affine>::setRunningMean(const std::vector<T>& runningMean)\n"
"{\n"
"    std::copy(runningMean.begin(), runningMean.end(), std::begin(running_mean));\n"
"}\n"
"\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"void BatchNorm2DT<T, num_filters_t, num_features_t, affine>::setRunningVariance(const std::vector<T>& runningVar)\n"
"{\n"
"    std::copy(runningVar.begin(), runningVar.end(), std::begin(running_var));\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"void BatchNorm2DT<T, num_filters_t, num_features_t, affine>::setEpsilon(T newEpsilon)\n"
"{\n"
"    epsilon = newEpsilon;\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"void BatchNorm2DT<T, num_filters_t, num_features_t, affine>::updateMultiplier()\n"
"{\n"
"    for(int i = 0; i < num_filters_t; ++i)\n"
"        multiplier[i] = gamma[i] / std::sqrt(running_var[i] + epsilon);\n"
"}\n"
"}\n"
"\n"
"#endif // RTNEURAL_USE_STL\n";

const char* batchnorm2d_tpp = (const char*) temp_binary_data_38;

//================== batchnorm2d_eigen.tpp ==================
static const unsigned char temp_binary_data_39[] =
"#include \"batchnorm2d_eigen.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"template <typename T>\n"
"BatchNorm2DLayer<T>::BatchNorm2DLayer(int in_num_filters, int in_num_features)\n"
"    : Layer<T>(in_num_filters * in_num_features, in_num_filters * in_num_features)\n"
"    , num_filters(in_num_filters)\n"
"    , num_features(in_num_features)\n"
"{\n"
"    gamma = Eigen::Vector<T, Eigen::Dynamic>::Ones(num_filters);\n"
"    beta = Eigen::Vector<T, Eigen::Dynamic>::Zero(num_filters);\n"
"    running_mean = Eigen::Vector<T, Eigen::Dynamic>::Zero(num_filters);\n"
"    running_var = Eigen::Vector<T, Eigen::Dynamic>::Ones(num_filters);\n"
"    multiplier = Eigen::Vector<T, Eigen::Dynamic>::Ones(num_filters);\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm2DLayer<T>::setGamma(const std::vector<T>& gammaVals)\n"
"{\n"
"    std::copy(gammaVals.begin(), gammaVals.end(), gamma.begin());\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm2DLayer<T>::setBeta(const std::vector<T>& betaVals)\n"
"{\n"
"    std::copy(betaVals.begin(), betaVals.end(), beta.begin());\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm2DLayer<T>::setRunningMean(const std::vector<T>& runningMean)\n"
"{\n"
"    std::copy(runningMean.begin(), runningMean.end(), running_mean.begin());\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm2DLayer<T>::setRunningVariance(const std::vector<T>& runningVar)\n"
"{\n"
"    std::copy(runningVar.begin(), runningVar.end(), running_var.begin());\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm2DLayer<T>::setEpsilon(T newEpsilon)\n"
"{\n"
"    epsilon = newEpsilon;\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm2DLayer<T>::updateMultiplier()\n"
"{\n"
"    for(int i = 0; i < num_filters; ++i)\n"
"        multiplier[i] = gamma[i] / std::sqrt(running_var[i] + epsilon);\n"
"}\n"
"\n"
"//============================================================\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"BatchNorm2DT<T, num_filters_t, num_features_t, affine>::BatchNorm2DT()\n"
"    : outs(outs_internal)\n"
"{\n"
"    gamma = Eigen::Vector<T, num_filters_t>::Ones(num_filters_t);\n"
"    beta = Eigen::Vector<T, num_filters_t>::Zero(num_filters_t);\n"
"    running_mean = Eigen::Vector<T, num_filters_t>::Zero(num_filters_t);\n"
"    running_var = Eigen::Vector<T, num_filters_t>::Ones(num_filters_t);\n"
"    multiplier = Eigen::Vector<T, num_filters_t>::Ones(num_filters_t);\n"
"}\n"
"\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"template <bool isAffine>\n"
"typename std::enable_if<isAffine, void>::type BatchNorm2DT<T, num_filters_t, num_features_t, affine>::setGamma(const std::vector<T>& gammaVals)\n"
"{\n"
"    std::copy(gammaVals.begin(), gammaVals.end(), std::begin(gamma));\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"template <bool isAffine>\n"
"typename std::enable_if<isAffine, void>::type BatchNorm2DT<T, num_filters_t, num_features_t, affine>::setBeta(const std::vector<T>& betaVals)\n"
"{\n"
"    std::copy(betaVals.begin(), betaVals.end(), std::begin(beta));\n"
"}\n"
"\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"void BatchNorm2DT<T, num_filters_t, num_features_t, affine>::setRunningMean(const std::vector<T>& runningMean)\n"
"{\n"
"    std::copy(runningMean.begin(), runningMean.end(), std::begin(running_mean));\n"
"}\n"
"\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"void BatchNorm2DT<T, num_filters_t, num_features_t, affine>::setRunningVariance(const std::vector<T>& runningVar)\n"
"{\n"
"    std::copy(runningVar.begin(), runningVar.end(), std::begin(running_var));\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"void BatchNorm2DT<T, num_filters_t, num_features_t, affine>::setEpsilon(T newEpsilon)\n"
"{\n"
"    epsilon = newEpsilon;\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"void BatchNorm2DT<T, num_filters_t, num_features_t, affine>::updateMultiplier()\n"
"{\n"
"    for(int i = 0; i < num_filters_t; ++i)\n"
"        multiplier[i] = gamma[i] / std::sqrt(running_var[i] + epsilon);\n"
"}\n"
"}\n";

const char* batchnorm2d_eigen_tpp = (const char*) temp_binary_data_39;

//================== batchnorm2d_xsimd.tpp ==================
static const unsigned char temp_binary_data_40[] =
"#include \"batchnorm2d_xsimd.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"template <typename T>\n"
"BatchNorm2DLayer<T>::BatchNorm2DLayer(int in_num_filters, int in_num_features)\n"
"    : Layer<T>(in_num_filters * in_num_features, in_num_filters * in_num_features)\n"
"    , num_filters(in_num_filters)\n"
"    , num_features(in_num_features)\n"
"    , gamma(num_filters, (T)1)\n"
"    , beta(num_filters, (T)0)\n"
"    , running_mean(num_filters, (T)0)\n"
"    , running_var(num_filters, (T)1)\n"
"    , multiplier(num_filters, (T)1)\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm2DLayer<T>::setGamma(const std::vector<T>& gammaVals)\n"
"{\n"
"    std::copy(gammaVals.begin(), gammaVals.end(), gamma.begin());\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm2DLayer<T>::setBeta(const std::vector<T>& betaVals)\n"
"{\n"
"    std::copy(betaVals.begin(), betaVals.end(), beta.begin());\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm2DLayer<T>::setRunningMean(const std::vector<T>& runningMean)\n"
"{\n"
"    std::copy(runningMean.begin(), runningMean.end(), running_mean.begin());\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm2DLayer<T>::setRunningVariance(const std::vector<T>& runningVar)\n"
"{\n"
"    std::copy(runningVar.begin(), runningVar.end(), running_var.begin());\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm2DLayer<T>::setEpsilon(T newEpsilon)\n"
"{\n"
"    epsilon = newEpsilon;\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T>\n"
"void BatchNorm2DLayer<T>::updateMultiplier()\n"
"{\n"
"    for(int i = 0; i < num_filters; ++i)\n"
"        multiplier[i] = gamma[i] / std::sqrt(running_var[i] + epsilon);\n"
"}\n"
"\n"
"//============================================================\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"BatchNorm2DT<T, num_filters_t, num_features_t, affine>::BatchNorm2DT()\n"
"{\n"
"    std::fill(std::begin(outs), std::end(outs), (T)0);\n"
"\n"
"    std::fill(std::begin(gamma), std::end(gamma), (T)1);\n"
"    std::fill(std::begin(beta), std::end(beta), (T)0);\n"
"    std::fill(std::begin(running_mean), std::end(running_mean), (T)0);\n"
"    std::fill(std::begin(running_var), std::end(running_var), (T)1);\n"
"    std::fill(std::begin(multiplier), std::end(multiplier), (T)1);\n"
"}\n"
"\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"template <bool isAffine>\n"
"typename std::enable_if<isAffine, void>::type BatchNorm2DT<T, num_filters_t, num_features_t, affine>::setGamma(const std::vector<T>& gammaVals)\n"
"{\n"
"    std::copy(gammaVals.begin(), gammaVals.end(), reinterpret_cast<T*>(std::begin(gamma)));\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"template <bool isAffine>\n"
"typename std::enable_if<isAffine, void>::type BatchNorm2DT<T, num_filters_t, num_features_t, affine>::setBeta(const std::vector<T>& betaVals)\n"
"{\n"
"    std::copy(betaVals.begin(), betaVals.end(), reinterpret_cast<T*>(std::begin(beta)));\n"
"}\n"
"\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"void BatchNorm2DT<T, num_filters_t, num_features_t, affine>::setRunningMean(const std::vector<T>& runningMean)\n"
"{\n"
"    std::copy(runningMean.begin(), runningMean.end(), reinterpret_cast<T*>(std::begin(running_mean)));\n"
"}\n"
"\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"void BatchNorm2DT<T, num_filters_t, num_features_t, affine>::setRunningVariance(const std::vector<T>& runningVar)\n"
"{\n"
"    std::copy(runningVar.begin(), runningVar.end(), reinterpret_cast<T*>(std::begin(running_var)));\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"void BatchNorm2DT<T, num_filters_t, num_features_t, affine>::setEpsilon(T newEpsilon)\n"
"{\n"
"    epsilon = newEpsilon;\n"
"    updateMultiplier();\n"
"}\n"
"\n"
"template <typename T, int num_filters_t, int num_features_t, bool affine>\n"
"void BatchNorm2DT<T, num_filters_t, num_features_t, affine>::updateMultiplier()\n"
"{\n"
"    for(int i = 0; i < v_num_filters; ++i)\n"
"        multiplier[i] = gamma[i] / xsimd::sqrt(running_var[i] + epsilon);\n"
"}\n"
"}\n";

const char* batchnorm2d_xsimd_tpp = (const char*) temp_binary_data_40;

//================== conv1d.tpp ==================
static const unsigned char temp_binary_data_41[] =
"#include \"conv1d.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"\n"
"#if !RTNEURAL_USE_EIGEN && !RTNEURAL_USE_XSIMD\n"
"\n"
"template <typename T>\n"
"Conv1D<T>::Conv1D(int in_size, int out_size, int kernel_size, int dilation, int num_groups)\n"
"    : Layer<T>(in_size, out_size)\n"
"    , dilation_rate(dilation)\n"
"    , kernel_size(kernel_size)\n"
"    , state_size((kernel_size - 1) * dilation + 1)\n"
"    , groups(num_groups)\n"
"    , filters_per_group(in_size / groups)\n"
"    , channels_per_group(out_size / groups)\n"
"{\n"
"    weights = new T**[out_size];\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        weights[i] = new T*[kernel_size];\n"
"        for(int k = 0; k < kernel_size; ++k)\n"
"        {\n"
"            weights[i][k] = new T[filters_per_group];\n"
"            std::fill(weights[i][k], weights[i][k] + filters_per_group, (T)0);\n"
"        }\n"
"    }\n"
"\n"
"    bias = new T[out_size];\n"
"\n"
"    state = new T*[state_size];\n"
"    for(int k = 0; k < state_size; ++k)\n"
"        state[k] = new T[in_size];\n"
"\n"
"    state_cols = new T*[kernel_size];\n"
"    for(int k = 0; k < kernel_size; ++k)\n"
"        state_cols[k] = new T[filters_per_group];\n"
"\n"
"    state_ptrs = new int[kernel_size];\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1D<T>::Conv1D(std::initializer_list<int> sizes)\n"
"    : Conv1D<T>(*sizes.begin(), *(sizes.begin() + 1), *(sizes.begin() + 2), *(sizes.begin() + 3))\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1D<T>::Conv1D(const Conv1D<T>& other)\n"
"    : Conv1D<T>(other.in_size, other.out_size, other.kernel_size, other.dilation_rate)\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1D<T>& Conv1D<T>::operator=(const Conv1D<T>& other)\n"
"{\n"
"    if(&other != this)\n"
"        *this = Conv1D<T>(other);\n"
"\n"
"    return *this;\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1D<T>::~Conv1D()\n"
"{\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < kernel_size; ++k)\n"
"            delete[] weights[i][k];\n"
"\n"
"        delete[] weights[i];\n"
"    }\n"
"\n"
"    delete[] weights;\n"
"    delete[] bias;\n"
"\n"
"    for(int k = 0; k < state_size; ++k)\n"
"        delete[] state[k];\n"
"    delete[] state;\n"
"\n"
"    for(int k = 0; k < kernel_size; ++k)\n"
"        delete[] state_cols[k];\n"
"    delete[] state_cols;\n"
"\n"
"    delete[] state_ptrs;\n"
"}\n"
"\n"
"template <typename T>\n"
"void Conv1D<T>::reset()\n"
"{\n"
"    for(int k = 0; k < state_size; ++k)\n"
"        std::fill(state[k], state[k] + Layer<T>::in_size, (T)0);\n"
"\n"
"    for(int k = 0; k < kernel_size; ++k)\n"
"        std::fill(state_cols[k], state_cols[k] + filters_per_group, (T)0);\n"
"\n"
"    for(int k = 0; k < kernel_size; ++k)\n"
"        state_ptrs[k] = 0;\n"
"\n"
"    state_ptr = 0;\n"
"}\n"
"\n"
"template <typename T>\n"
"void Conv1D<T>::setWeights(const std::vector<std::vector<std::vector<T>>>& ws)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"        for(int k = 0; k < filters_per_group; ++k)\n"
"            for(int j = 0; j < kernel_size; ++j)\n"
"                weights[i][j][k] = ws[i][k][j];\n"
"}\n"
"\n"
"template <typename T>\n"
"void Conv1D<T>::setBias(const std::vector<T>& biasVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"        bias[i] = biasVals[i];\n"
"}\n"
"\n"
"//====================================================\n"
"template <typename T, int in_sizet, int out_sizet, int kernel_size, int dilation_rate, int groups, bool dynamic_state>\n"
"Conv1DT<T, in_sizet, out_sizet, kernel_size, dilation_rate, groups, dynamic_state>::Conv1DT()\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"        for(int j = 0; j < kernel_size; ++j)\n"
"            for(int k = 0; k < filters_per_group; ++k)\n"
"                weights[i][j][k] = (T)0.0;\n"
"\n"
"    for(int i = 0; i < out_size; ++i)\n"
"        bias[i] = (T)0.0;\n"
"\n"
"    for(int i = 0; i < out_size; ++i)\n"
"        outs[i] = (T)0.0;\n"
"\n"
"    resize_state();\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, int kernel_size, int dilation_rate, int groups, bool dynamic_state>\n"
"void Conv1DT<T, in_sizet, out_sizet, kernel_size, dilation_rate, groups, dynamic_state>::reset()\n"
"{\n"
"    for(int i = 0; i < state_size; ++i)\n"
"        for(int k = 0; k < in_size; ++k)\n"
"            state[i][k] = (T)0.0;\n"
"\n"
"    for(int i = 0; i < kernel_size; ++i)\n"
"        for(int k = 0; k < filters_per_group; ++k)\n"
"            state_cols[i][k] = (T)0.0;\n"
"\n"
"    state_ptr = 0;\n"
"    for(int i = 0; i < kernel_size; ++i)\n"
"        state_ptrs[i] = 0;\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, int kernel_size, int dilation_rate, int groups, bool dynamic_state>\n"
"void Conv1DT<T, in_sizet, out_sizet, kernel_size, dilation_rate, groups, dynamic_state>::setWeights(const std::vector<std::vector<std::vector<T>>>& ws)\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"        for(int k = 0; k < filters_per_group; ++k)\n"
"            for(int j = 0; j < kernel_size; ++j)\n"
"                weights[i][j][k] = ws[i][k][j];\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, int kernel_size, int dilation_rate, int groups, bool dynamic_state>\n"
"void Conv1DT<T, in_sizet, out_sizet, kernel_size, dilation_rate, groups, dynamic_state>::setBias(const std::vector<T>& biasVals)\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"        bias[i] = biasVals[i];\n"
"}\n"
"\n"
"#endif\n"
"\n"
"} // namespace RTNEURAL_NAMESPACE\n";

const char* conv1d_tpp = (const char*) temp_binary_data_41;

//================== conv1d_eigen.tpp ==================
static const unsigned char temp_binary_data_42[] =
"#include \"conv1d_eigen.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"\n"
"template <typename T>\n"
"Conv1D<T>::Conv1D(int in_size, int out_size, int kernel_size, int dilation, int num_groups)\n"
"    : Layer<T>(in_size, out_size)\n"
"    , dilation_rate(dilation)\n"
"    , kernel_size(kernel_size)\n"
"    , state_size((kernel_size - 1) * dilation + 1)\n"
"    , groups(num_groups)\n"
"    , filters_per_group(in_size / groups)\n"
"    , channels_per_group(out_size / groups)\n"
"{\n"
"    kernelWeights.resize(out_size);\n"
"    for(int i = 0; i < out_size; ++i)\n"
"        kernelWeights[i] = Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>::Zero(filters_per_group, kernel_size);\n"
"\n"
"    bias = Eigen::Vector<T, Eigen::Dynamic>::Zero(out_size);\n"
"    state = Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>::Zero(in_size, state_size);\n"
"    state_cols = Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>::Zero(filters_per_group, kernel_size);\n"
"    state_ptrs = Eigen::Vector<int, Eigen::Dynamic>::Zero(kernel_size);\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1D<T>::Conv1D(std::initializer_list<int> sizes)\n"
"    : Conv1D<T>(*sizes.begin(), *(sizes.begin() + 1), *(sizes.begin() + 2), *(sizes.begin() + 3))\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1D<T>::Conv1D(const Conv1D<T>& other)\n"
"    : Conv1D<T>(other.in_size, other.out_size, other.kernel_size, other.dilation_rate)\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1D<T>& Conv1D<T>::operator=(const Conv1D<T>& other)\n"
"{\n"
"    return *this = Conv1D<T>(other);\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1D<T>::~Conv1D() = default;\n"
"\n"
"template <typename T>\n"
"void Conv1D<T>::reset()\n"
"{\n"
"    state_ptr = 0;\n"
"    state_ptrs.setZero();\n"
"    state_cols.setZero();\n"
"    state.setZero();\n"
"}\n"
"\n"
"template <typename T>\n"
"void Conv1D<T>::setWeights(const std::vector<std::vector<std::vector<T>>>& weights)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"        for(int k = 0; k < filters_per_group; ++k)\n"
"            for(int j = 0; j < kernel_size; ++j)\n"
"                kernelWeights[i](k, j) = weights[i][k][j];\n"
"}\n"
"\n"
"template <typename T>\n"
"void Conv1D<T>::setBias(const std::vector<T>& biasVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"        bias(i) = biasVals[i];\n"
"}\n"
"\n"
"//====================================================\n"
"template <typename T, int in_sizet, int out_sizet, int kernel_size, int dilation_rate, int groups, bool dynamic_state>\n"
"Conv1DT<T, in_sizet, out_sizet, kernel_size, dilation_rate, groups, dynamic_state>::Conv1DT()\n"
"    : outs(outs_internal)\n"
"{\n"
"    for(int k = 0; k < out_size; ++k)\n"
"        weights[k] = weights_type::Zero();\n"
"\n"
"    bias = vec_type::Zero();\n"
"\n"
"    resize_state();\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, int kernel_size, int dilation_rate, int groups, bool dynamic_state>\n"
"void Conv1DT<T, in_sizet, out_sizet, kernel_size, dilation_rate, groups, dynamic_state>::reset()\n"
"{\n"
"    state.setZero();\n"
"    state_cols = weights_type::Zero();\n"
"    state_ptrs = state_ptrs_type::Zero();\n"
"    state_ptr = 0;\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, int kernel_size, int dilation_rate, int groups, bool dynamic_state>\n"
"void Conv1DT<T, in_sizet, out_sizet, kernel_size, dilation_rate, groups, dynamic_state>::setWeights(const std::vector<std::vector<std::vector<T>>>& ws)\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"        for(int k = 0; k < filters_per_group; ++k)\n"
"            for(int j = 0; j < kernel_size; ++j)\n"
"                weights[i](k, j) = ws[i][k][j];\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, int kernel_size, int dilation_rate, int groups, bool dynamic_state>\n"
"void Conv1DT<T, in_sizet, out_sizet, kernel_size, dilation_rate, groups, dynamic_state>::setBias(const std::vector<T>& biasVals)\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"        bias(i) = biasVals[i];\n"
"}\n"
"\n"
"} // namespace RTNEURAL_NAMESPACE\n";

const char* conv1d_eigen_tpp = (const char*) temp_binary_data_42;

//================== conv1d_xsimd.tpp ==================
static const unsigned char temp_binary_data_43[] =
"#include \"conv1d_xsimd.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"\n"
"template <typename T>\n"
"Conv1D<T>::Conv1D(int in_size, int out_size, int kernel_size, int dilation, int num_groups)\n"
"    : Layer<T>(in_size, out_size)\n"
"    , dilation_rate(dilation)\n"
"    , kernel_size(kernel_size)\n"
"    , state_size((kernel_size - 1) * dilation + 1)\n"
"    , groups(num_groups)\n"
"    , filters_per_group(in_size / groups)\n"
"    , channels_per_group(out_size / groups)\n"
"{\n"
"    weights = vec3_type(out_size, vec2_type(kernel_size, vec_type(filters_per_group, (T)0)));\n"
"    bias.resize(out_size, (T)0);\n"
"    state = vec2_type(state_size, vec_type(in_size, (T)0));\n"
"    state_cols = vec2_type(kernel_size, vec_type(filters_per_group, (T)0));\n"
"    state_ptrs.resize(kernel_size);\n"
"    prod_state.resize(filters_per_group);\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1D<T>::Conv1D(std::initializer_list<int> sizes)\n"
"    : Conv1D<T>(*sizes.begin(), *(sizes.begin() + 1), *(sizes.begin() + 2), *(sizes.begin() + 3))\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1D<T>::Conv1D(const Conv1D<T>& other)\n"
"    : Conv1D<T>(other.in_size, other.out_size, other.kernel_size, other.dilation_rate)\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1D<T>& Conv1D<T>::operator=(const Conv1D<T>& other)\n"
"{\n"
"    return *this = Conv1D<T>(other);\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1D<T>::~Conv1D() = default;\n"
"\n"
"template <typename T>\n"
"void Conv1D<T>::reset()\n"
"{\n"
"    for(int k = 0; k < state_size; ++k)\n"
"        std::fill(state[k].begin(), state[k].end(), (T)0);\n"
"\n"
"    for(int k = 0; k < kernel_size; ++k)\n"
"        std::fill(state_cols[k].begin(), state_cols[k].end(), (T)0);\n"
"\n"
"    std::fill(state_ptrs.begin(), state_ptrs.end(), 0);\n"
"    state_ptr = 0;\n"
"}\n"
"\n"
"template <typename T>\n"
"void Conv1D<T>::setWeights(const std::vector<std::vector<std::vector<T>>>& ws)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"        for(int k = 0; k < filters_per_group; ++k)\n"
"            for(int j = 0; j < kernel_size; ++j)\n"
"                weights[i][j][k] = ws[i][k][j];\n"
"}\n"
"\n"
"template <typename T>\n"
"void Conv1D<T>::setBias(const std::vector<T>& biasVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"        bias[i] = biasVals[i];\n"
"}\n"
"\n"
"//====================================================\n"
"template <typename T, int in_sizet, int out_sizet, int kernel_size, int dilation_rate, int groups, bool dynamic_state>\n"
"Conv1DT<T, in_sizet, out_sizet, kernel_size, dilation_rate, groups, dynamic_state>::Conv1DT()\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"        for(int j = 0; j < kernel_size; ++j)\n"
"            for(int k = 0; k < v_filters_per_group; ++k)\n"
"                weights[i][j][k] = v_type((T)0.0);\n"
"\n"
"    for(int i = 0; i < v_out_size; ++i)\n"
"        bias[i] = v_type((T)0.0);\n"
"\n"
"    for(int i = 0; i < v_out_size; ++i)\n"
"        outs[i] = v_type((T)0.0);\n"
"\n"
"    resize_state();\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, int kernel_size, int dilation_rate, int groups, bool dynamic_state>\n"
"void Conv1DT<T, in_sizet, out_sizet, kernel_size, dilation_rate, groups, dynamic_state>::reset()\n"
"{\n"
"    for(int i = 0; i < state_size; ++i)\n"
"        for(int k = 0; k < v_in_size; ++k)\n"
"            state[i][k] = v_type((T)0.0);\n"
"\n"
"    for(int i = 0; i < kernel_size; ++i)\n"
"        for(int k = 0; k < v_filters_per_group; ++k)\n"
"            state_cols[i][k] = v_type((T)0.0);\n"
"\n"
"    state_ptr = 0;\n"
"    for(int i = 0; i < kernel_size; ++i)\n"
"        state_ptrs[i] = 0;\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, int kernel_size, int dilation_rate, int groups, bool dynamic_state>\n"
"void Conv1DT<T, in_sizet, out_sizet, kernel_size, dilation_rate, groups, dynamic_state>::setWeights(const std::vector<std::vector<std::vector<T>>>& ws)\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < filters_per_group; ++k)\n"
"        {\n"
"            for(int j = 0; j < kernel_size; ++j)\n"
"            {\n"
"                auto& w = weights[i][j][k / v_size];\n"
"                w = set_value(w, k % v_size, ws[i][k][j]);\n"
"            }\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, int kernel_size, int dilation_rate, int groups, bool dynamic_state>\n"
"void Conv1DT<T, in_sizet, out_sizet, kernel_size, dilation_rate, groups, dynamic_state>::setBias(const std::vector<T>& biasVals)\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"        bias[i / v_size] = set_value(bias[i / v_size], i % v_size, biasVals[i]);\n"
"}\n"
"\n"
"} // namespace RTNEURAL_NAMESPACE\n";

const char* conv1d_xsimd_tpp = (const char*) temp_binary_data_43;

//================== conv1d_stateless.tpp ==================
static const unsigned char temp_binary_data_44[] =
"#include \"conv1d_stateless.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"template <typename T>\n"
"Conv1DStateless<T>::Conv1DStateless(int in_num_filters_in, int in_num_features_in, int in_num_filters_out, int in_kernel_size, int in_stride, bool in_valid_pad)\n"
"    : num_filters_in(in_num_filters_in)\n"
"    , num_features_in(in_num_features_in)\n"
"    , num_filters_out(in_num_filters_out)\n"
"    , kernel_size(in_kernel_size)\n"
"    , stride(in_stride)\n"
"    , valid_pad(in_valid_pad)\n"
"    , num_features_out(computeNumFeaturesOut(in_num_features_in, in_kernel_size, in_stride, in_valid_pad))\n"
"    , pad_left(computePadLeft(in_num_features_in, in_kernel_size, in_stride, in_valid_pad))\n"
"    , pad_right(computePadRight(in_num_features_in, in_kernel_size, in_stride, in_valid_pad))\n"
"    , Layer<T>(in_num_filters_in * in_num_features_in, in_num_filters_out * computeNumFeaturesOut(in_num_features_in, in_kernel_size, in_stride, in_valid_pad))\n"
"{\n"
"    kernelWeights.resize(num_filters_out);\n"
"    for(auto& kw : kernelWeights)\n"
"    {\n"
"        kw.resize(num_filters_in);\n"
"        for(auto& col : kw)\n"
"            col.resize(kernel_size, (T)0);\n"
"    }\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1DStateless<T>::Conv1DStateless(std::initializer_list<int> sizes)\n"
"    : Conv1DStateless<T>(*sizes.begin(), *(sizes.begin() + 1), *(sizes.begin() + 2), *(sizes.begin() + 3), *(sizes.begin() + 4), *(sizes.begin() + 5))\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1DStateless<T>::Conv1DStateless(const Conv1DStateless& other)\n"
"    : Conv1DStateless(other.num_filters_in, other.num_features_in, other.num_filters_out, other.kernel_size, other.stride, other.valid_pad)\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1DStateless<T>& Conv1DStateless<T>::operator=(const Conv1DStateless<T>& other)\n"
"{\n"
"    return *this = Conv1DStateless<T>(other);\n"
"}\n"
"\n"
"template <typename T>\n"
"void Conv1DStateless<T>::setWeights(const std::vector<std::vector<std::vector<T>>>& inWeights)\n"
"{\n"
"    for(int i = 0; i < num_filters_out; ++i)\n"
"        for(int k = 0; k < num_filters_in; ++k)\n"
"            for(int j = 0; j < kernel_size; ++j)\n"
"                kernelWeights[i][k][j] = inWeights.at(i).at(k).at(j);\n"
"}\n"
"\n"
"//====================================================\n"
"\n"
"template <typename T, int num_filters_in_t, int num_features_in_t, int num_filters_out_t, int kernel_size_t, int stride_t, bool valid_pad_t>\n"
"Conv1DStatelessT<T, num_filters_in_t, num_features_in_t, num_filters_out_t, kernel_size_t, stride_t, valid_pad_t>::Conv1DStatelessT()\n"
"{\n"
"    for(int i = 0; i < num_filters_out_t; ++i)\n"
"        for(int k = 0; k < num_filters_in_t; ++k)\n"
"            std::fill(std::begin(kernelWeights[i][k]), std::end(kernelWeights[i][k]), (T)0);\n"
"}\n"
"\n"
"template <typename T, int num_filters_in_t, int num_features_in_t, int num_filters_out_t, int kernel_size_t, int stride_t, bool valid_pad_t>\n"
"void Conv1DStatelessT<T, num_filters_in_t, num_features_in_t, num_filters_out_t, kernel_size_t, stride_t, valid_pad_t>::setWeights(const std::vector<std::vector<std::vector<T>>>& inWeights)\n"
"{\n"
"    for(int i = 0; i < num_filters_out_t; ++i)\n"
"        for(int k = 0; k < num_filters_in_t; ++k)\n"
"            for(int j = 0; j < kernel_size_t; ++j)\n"
"                kernelWeights[i][k][j] = inWeights.at(i).at(k).at(j);\n"
"}\n"
"\n"
"template <typename T, int num_filters_in_t, int num_features_in_t, int num_filters_out_t, int kernel_size_t, int stride_t, bool valid_pad_t>\n"
"void Conv1DStatelessT<T, num_filters_in_t, num_features_in_t, num_filters_out_t, kernel_size_t, stride_t, valid_pad_t>::setWeightsTransposed(const std::vector<std::vector<std::vector<T>>>& inWeights)\n"
"{\n"
"    for(int i = 0; i < num_filters_out_t; ++i)\n"
"        for(int k = 0; k < num_filters_in_t; ++k)\n"
"            for(int j = 0; j < kernel_size_t; ++j)\n"
"                kernelWeights[i][k][j] = inWeights.at(j).at(k).at(i);\n"
"}\n"
"} // RTNEURAL_NAMESPACE\n";

const char* conv1d_stateless_tpp = (const char*) temp_binary_data_44;

//================== conv1d_stateless_eigen.tpp ==================
static const unsigned char temp_binary_data_45[] =
"#include \"conv1d_stateless_eigen.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"template <typename T>\n"
"Conv1DStateless<T>::Conv1DStateless(int in_num_filters_in, int in_num_features_in, int in_num_filters_out, int in_kernel_size, int in_stride, bool in_valid_pad)\n"
"    : num_filters_in(in_num_filters_in)\n"
"    , num_features_in(in_num_features_in)\n"
"    , num_filters_out(in_num_filters_out)\n"
"    , kernel_size(in_kernel_size)\n"
"    , stride(in_stride)\n"
"    , valid_pad(in_valid_pad)\n"
"    , num_features_out(computeNumFeaturesOut(in_num_features_in, in_kernel_size, in_stride, in_valid_pad))\n"
"    , pad_left(computePadLeft(in_num_features_in, in_kernel_size, in_stride, in_valid_pad))\n"
"    , pad_right(computePadRight(in_num_features_in, in_kernel_size, in_stride, in_valid_pad))\n"
"    , Layer<T>(in_num_filters_in * in_num_features_in, in_num_filters_out * computeNumFeaturesOut(in_num_features_in, in_kernel_size, in_stride, in_valid_pad))\n"
"{\n"
"    kernelWeights.resize(num_filters_out, Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>::Zero(num_filters_in, kernel_size));\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1DStateless<T>::Conv1DStateless(std::initializer_list<int> sizes)\n"
"    : Conv1DStateless<T>(*sizes.begin(), *(sizes.begin() + 1), *(sizes.begin() + 2), *(sizes.begin() + 3), *(sizes.begin() + 4), *(sizes.begin() + 5))\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1DStateless<T>::Conv1DStateless(const Conv1DStateless& other)\n"
"    : Conv1DStateless(other.num_filters_in, other.num_features_in, other.num_filters_out, other.kernel_size, other.stride, other.valid_pad)\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1DStateless<T>& Conv1DStateless<T>::operator=(const Conv1DStateless<T>& other)\n"
"{\n"
"    return *this = Conv1DStateless<T>(other);\n"
"}\n"
"\n"
"template <typename T>\n"
"void Conv1DStateless<T>::setWeights(const std::vector<std::vector<std::vector<T>>>& inWeights)\n"
"{\n"
"    for(int i = 0; i < num_filters_out; ++i)\n"
"        for(int k = 0; k < num_filters_in; ++k)\n"
"            for(int j = 0; j < kernel_size; ++j)\n"
"                kernelWeights[i](k, j) = inWeights.at(i).at(k).at(j);\n"
"}\n"
"\n"
"//====================================================\n"
"\n"
"template <typename T, int num_filters_in_t, int num_features_in_t, int num_filters_out_t, int kernel_size_t, int stride_t, bool valid_pad_t>\n"
"Conv1DStatelessT<T, num_filters_in_t, num_features_in_t, num_filters_out_t, kernel_size_t, stride_t, valid_pad_t>::Conv1DStatelessT()\n"
"    : outs(outs_internal)\n"
"{\n"
"    for(int k = 0; k < num_filters_out_t; ++k)\n"
"        kernelWeights[k] = weights_type::Zero();\n"
"}\n"
"\n"
"template <typename T, int num_filters_in_t, int num_features_in_t, int num_filters_out_t, int kernel_size_t, int stride_t, bool valid_pad_t>\n"
"void Conv1DStatelessT<T, num_filters_in_t, num_features_in_t, num_filters_out_t, kernel_size_t, stride_t, valid_pad_t>::setWeights(const std::vector<std::vector<std::vector<T>>>& inWeights)\n"
"{\n"
"    for(int i = 0; i < num_filters_out_t; ++i)\n"
"        for(int k = 0; k < num_filters_in_t; ++k)\n"
"            for(int j = 0; j < kernel_size_t; ++j)\n"
"                kernelWeights[i](k, j) = inWeights.at(i).at(k).at(j);\n"
"}\n"
"\n"
"template <typename T, int num_filters_in_t, int num_features_in_t, int num_filters_out_t, int kernel_size_t, int stride_t, bool valid_pad_t>\n"
"void Conv1DStatelessT<T, num_filters_in_t, num_features_in_t, num_filters_out_t, kernel_size_t, stride_t, valid_pad_t>::setWeightsTransposed(const std::vector<std::vector<std::vector<T>>>& inWeights)\n"
"{\n"
"    for(int i = 0; i < num_filters_out_t; ++i)\n"
"        for(int k = 0; k < num_filters_in_t; ++k)\n"
"            for(int j = 0; j < kernel_size_t; ++j)\n"
"                kernelWeights[i](k, j) = inWeights.at(j).at(k).at(i);\n"
"}\n"
"} // RTNEURAL_NAMESPACE\n";

const char* conv1d_stateless_eigen_tpp = (const char*) temp_binary_data_45;

//================== conv1d_stateless_xsimd.tpp ==================
static const unsigned char temp_binary_data_46[] =
"#include \"conv1d_stateless_xsimd.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"template <typename T>\n"
"Conv1DStateless<T>::Conv1DStateless(int in_num_filters_in, int in_num_features_in, int in_num_filters_out, int in_kernel_size, int in_stride, bool in_valid_pad)\n"
"    : num_filters_in(in_num_filters_in)\n"
"    , num_features_in(in_num_features_in)\n"
"    , num_filters_out(in_num_filters_out)\n"
"    , kernel_size(in_kernel_size)\n"
"    , stride(in_stride)\n"
"    , valid_pad(in_valid_pad)\n"
"    , num_features_out(computeNumFeaturesOut(in_num_features_in, in_kernel_size, in_stride, in_valid_pad))\n"
"    , pad_left(computePadLeft(in_num_features_in, in_kernel_size, in_stride, in_valid_pad))\n"
"    , pad_right(computePadRight(in_num_features_in, in_kernel_size, in_stride, in_valid_pad))\n"
"    , Layer<T>(in_num_filters_in * in_num_features_in, in_num_filters_out * computeNumFeaturesOut(in_num_features_in, in_kernel_size, in_stride, in_valid_pad))\n"
"{\n"
"    kernelWeights.resize(num_filters_out);\n"
"    for(auto& kw : kernelWeights)\n"
"    {\n"
"        kw.resize(kernel_size);\n"
"        for(auto& col : kw)\n"
"            col.resize(num_filters_in, (T)0);\n"
"    }\n"
"\n"
"    scratch.resize(num_filters_in, (T)0);\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1DStateless<T>::Conv1DStateless(std::initializer_list<int> sizes)\n"
"    : Conv1DStateless<T>(*sizes.begin(), *(sizes.begin() + 1), *(sizes.begin() + 2), *(sizes.begin() + 3), *(sizes.begin() + 4), *(sizes.begin() + 5))\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1DStateless<T>::Conv1DStateless(const Conv1DStateless& other)\n"
"    : Conv1DStateless(other.num_filters_in, other.num_features_in, other.num_filters_out, other.kernel_size, other.stride, other.valid_pad)\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv1DStateless<T>& Conv1DStateless<T>::operator=(const Conv1DStateless<T>& other)\n"
"{\n"
"    return *this = Conv1DStateless<T>(other);\n"
"}\n"
"\n"
"template <typename T>\n"
"void Conv1DStateless<T>::setWeights(const std::vector<std::vector<std::vector<T>>>& inWeights)\n"
"{\n"
"    for(int i = 0; i < num_filters_out; ++i)\n"
"        for(int k = 0; k < num_filters_in; ++k)\n"
"            for(int j = 0; j < kernel_size; ++j)\n"
"                kernelWeights[i][j][k] = inWeights.at(i).at(k).at(j);\n"
"}\n"
"\n"
"//====================================================\n"
"\n"
"template <typename T, int num_filters_in_t, int num_features_in_t, int num_filters_out_t, int kernel_size_t, int stride_t, bool valid_pad_t>\n"
"Conv1DStatelessT<T, num_filters_in_t, num_features_in_t, num_filters_out_t, kernel_size_t, stride_t, valid_pad_t>::Conv1DStatelessT()\n"
"{\n"
"    for(int i = 0; i < num_filters_out_t; ++i)\n"
"        for(int k = 0; k < num_filters_in_t; ++k)\n"
"            std::fill(std::begin(kernelWeights[i][k]), std::end(kernelWeights[i][k]), (T)0);\n"
"}\n"
"\n"
"template <typename T, int num_filters_in_t, int num_features_in_t, int num_filters_out_t, int kernel_size_t, int stride_t, bool valid_pad_t>\n"
"void Conv1DStatelessT<T, num_filters_in_t, num_features_in_t, num_filters_out_t, kernel_size_t, stride_t, valid_pad_t>::setWeights(const std::vector<std::vector<std::vector<T>>>& inWeights)\n"
"{\n"
"    for(int i = 0; i < num_filters_out_t; ++i)\n"
"        for(int k = 0; k < num_filters_in_t; ++k)\n"
"            for(int j = 0; j < kernel_size_t; ++j)\n"
"                kernelWeights[i][j][k / v_size] = set_value(kernelWeights[i][j][k / v_size], k % v_size, inWeights.at(i).at(k).at(j));\n"
"}\n"
"} // RTNEURAL_NAMESPACE\n";

const char* conv1d_stateless_xsimd_tpp = (const char*) temp_binary_data_46;

//================== conv2d.tpp ==================
static const unsigned char temp_binary_data_47[] =
"#include \"conv2d.h\"\n"
"\n"
"#if !RTNEURAL_USE_EIGEN && !RTNEURAL_USE_XSIMD\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"template <typename T>\n"
"Conv2D<T>::Conv2D(int in_num_filters_in, int in_num_filters_out, int in_num_features_in, int in_kernel_size_time, int in_kernel_size_feature,\n"
"    int in_dilation_rate, int in_stride, bool in_valid_pad)\n"
"    : num_filters_in(in_num_filters_in)\n"
"    , num_filters_out(in_num_filters_out)\n"
"    , num_features_in(in_num_features_in)\n"
"    , kernel_size_time(in_kernel_size_time)\n"
"    , kernel_size_feature(in_kernel_size_feature)\n"
"    , dilation_rate(in_dilation_rate)\n"
"    , stride(in_stride)\n"
"    , num_features_out(Conv1DStateless<T>::computeNumFeaturesOut(in_num_features_in, in_kernel_size_feature, in_stride, in_valid_pad))\n"
"    , receptive_field(1 + (in_kernel_size_time - 1) * in_dilation_rate) // See \"Dilated (atrous) convolution\" note here: https://distill.pub/2019/computing-receptive-fields/\n"
"    , valid_pad(in_valid_pad)\n"
"    , Layer<T>(in_num_features_in * in_num_filters_in, Conv1DStateless<T>::computeNumFeaturesOut(in_num_features_in, in_kernel_size_feature, in_stride, in_valid_pad) * in_num_filters_out)\n"
"{\n"
"    conv1dLayers.resize(kernel_size_time, Conv1DStateless<T>(num_filters_in, num_features_in, num_filters_out, kernel_size_feature, stride, valid_pad));\n"
"    bias.resize(num_filters_out, (T)0);\n"
"\n"
"    state.resize(receptive_field);\n"
"    for(auto& stateMat : state)\n"
"    {\n"
"        stateMat.resize(num_filters_out * num_features_out, (T)0);\n"
"    }\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv2D<T>::Conv2D(std::initializer_list<int> sizes)\n"
"    : Conv2D<T>(*sizes.begin(), *(sizes.begin() + 1), *(sizes.begin() + 2), *(sizes.begin() + 3), *(sizes.begin() + 4),\n"
"        *(sizes.begin() + 5), *(sizes.begin() + 6), *(sizes.begin() + 7))\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv2D<T>::Conv2D(const Conv2D& other)\n"
"    : Conv2D<T>(other.num_filters_in, other.num_filters_out, other.num_features_in, other.kernel_size_time, other.kernel_size_feature,\n"
"        other.dilation_rate, other.stride, other.valid_pad)\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv2D<T>& Conv2D<T>::operator=(const Conv2D& other)\n"
"{\n"
"    return *this = Conv2D<T>(other);\n"
"}\n"
"\n"
"template <typename T>\n"
"void Conv2D<T>::setWeights(const std::vector<std::vector<std::vector<std::vector<T>>>>& inWeights)\n"
"{\n"
"    for(int i = 0; i < kernel_size_time; i++)\n"
"    {\n"
"        conv1dLayers[i].setWeights(inWeights[i]);\n"
"    }\n"
"}\n"
"\n"
"template <typename T>\n"
"void Conv2D<T>::setBias(const std::vector<T>& inBias)\n"
"{\n"
"    std::copy(inBias.begin(), inBias.end(), bias.begin());\n"
"}\n"
"\n"
"template <typename T, int num_filters_in_t, int num_filters_out_t, int num_features_in_t, int kernel_size_time_t, int kernel_size_feature_t, int dilation_rate_t, int stride_t, bool valid_pad_t>\n"
"Conv2DT<T, num_filters_in_t, num_filters_out_t, num_features_in_t, kernel_size_time_t, kernel_size_feature_t, dilation_rate_t, stride_t, valid_pad_t>::Conv2DT()\n"
"{\n"
"}\n"
"\n"
"template <typename T, int num_filters_in_t, int num_filters_out_t, int num_features_in_t, int kernel_size_time_t,\n"
"    int kernel_size_feature_t, int dilation_rate_t, int stride_t, bool valid_pad_t>\n"
"void Conv2DT<T, num_filters_in_t, num_filters_out_t, num_features_in_t, kernel_size_time_t, kernel_size_feature_t,\n"
"    dilation_rate_t, stride_t, valid_pad_t>::setWeights(const std::vector<std::vector<std::vector<std::vector<T>>>>& inWeights)\n"
"{\n"
"    for(int i = 0; i < kernel_size_time_t; i++)\n"
"    {\n"
"        conv1dLayers[i].setWeights(inWeights[i]);\n"
"    }\n"
"}\n"
"\n"
"template <typename T, int num_filters_in_t, int num_filters_out_t, int num_features_in_t, int kernel_size_time_t,\n"
"    int kernel_size_feature_t, int dilation_rate_t, int stride_t, bool valid_pad_t>\n"
"void Conv2DT<T, num_filters_in_t, num_filters_out_t, num_features_in_t, kernel_size_time_t,\n"
"    kernel_size_feature_t, dilation_rate_t, stride_t, valid_pad_t>::setBias(const std::vector<T>& inBias)\n"
"{\n"
"    std::copy(inBias.begin(), inBias.end(), bias.begin());\n"
"}\n"
"} // RTNEURAL_NAMESPACE\n"
"\n"
"#endif // RTNEURAL_USE_STL\n";

const char* conv2d_tpp = (const char*) temp_binary_data_47;

//================== conv2d_eigen.tpp ==================
static const unsigned char temp_binary_data_48[] =
"#include \"conv2d_eigen.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"template <typename T>\n"
"Conv2D<T>::Conv2D(int in_num_filters_in, int in_num_filters_out, int in_num_features_in, int in_kernel_size_time, int in_kernel_size_feature,\n"
"    int in_dilation_rate, int in_stride, bool in_valid_pad)\n"
"    : num_filters_in(in_num_filters_in)\n"
"    , num_filters_out(in_num_filters_out)\n"
"    , num_features_in(in_num_features_in)\n"
"    , kernel_size_time(in_kernel_size_time)\n"
"    , kernel_size_feature(in_kernel_size_feature)\n"
"    , dilation_rate(in_dilation_rate)\n"
"    , stride(in_stride)\n"
"    , num_features_out(Conv1DStateless<T>::computeNumFeaturesOut(in_num_features_in, in_kernel_size_feature, in_stride, in_valid_pad))\n"
"    , receptive_field(1 + (in_kernel_size_time - 1) * in_dilation_rate) // See \"Dilated (atrous) convolution\" note here: https://distill.pub/2019/computing-receptive-fields/\n"
"    , valid_pad(in_valid_pad)\n"
"    , Layer<T>(in_num_features_in * in_num_filters_in, Conv1DStateless<T>::computeNumFeaturesOut(in_num_features_in, in_kernel_size_feature, in_stride, in_valid_pad) * in_num_filters_out)\n"
"{\n"
"    conv1dLayers.resize(kernel_size_time, Conv1DStateless<T>(num_filters_in, num_features_in, num_filters_out, kernel_size_feature, stride, valid_pad));\n"
"    bias = Eigen::Vector<T, Eigen::Dynamic>::Zero(num_filters_out);\n"
"\n"
"    state.resize(receptive_field, Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>::Zero(num_filters_out, num_features_out));\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv2D<T>::Conv2D(std::initializer_list<int> sizes)\n"
"    : Conv2D<T>(*sizes.begin(), *(sizes.begin() + 1), *(sizes.begin() + 2), *(sizes.begin() + 3), *(sizes.begin() + 4),\n"
"        *(sizes.begin() + 5), *(sizes.begin() + 6), *(sizes.begin() + 7))\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv2D<T>::Conv2D(const Conv2D& other)\n"
"    : Conv2D<T>(other.num_filters_in, other.num_filters_out, other.num_features_in, other.kernel_size_time, other.kernel_size_feature,\n"
"        other.dilation_rate, other.stride, other.valid_pad)\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv2D<T>& Conv2D<T>::operator=(const Conv2D& other)\n"
"{\n"
"    return *this = Conv2D<T>(other);\n"
"}\n"
"\n"
"template <typename T>\n"
"void Conv2D<T>::setWeights(const std::vector<std::vector<std::vector<std::vector<T>>>>& inWeights)\n"
"{\n"
"    for(int i = 0; i < kernel_size_time; i++)\n"
"    {\n"
"        conv1dLayers[i].setWeights(inWeights[i]);\n"
"    }\n"
"}\n"
"\n"
"template <typename T>\n"
"void Conv2D<T>::setBias(const std::vector<T>& inBias)\n"
"{\n"
"    for(int i = 0; i < num_filters_out; i++)\n"
"    {\n"
"        bias(i) = inBias[i];\n"
"    }\n"
"}\n"
"\n"
"template <typename T, int num_filters_in_t, int num_filters_out_t, int num_features_in_t, int kernel_size_time_t, int kernel_size_feature_t, int dilation_rate_t, int stride_t, bool valid_pad_t>\n"
"Conv2DT<T, num_filters_in_t, num_filters_out_t, num_features_in_t, kernel_size_time_t, kernel_size_feature_t, dilation_rate_t, stride_t, valid_pad_t>::Conv2DT()\n"
"    : outs(outs_internal)\n"
"{\n"
"}\n"
"\n"
"template <typename T, int num_filters_in_t, int num_filters_out_t, int num_features_in_t, int kernel_size_time_t,\n"
"    int kernel_size_feature_t, int dilation_rate_t, int stride_t, bool valid_pad_t>\n"
"void Conv2DT<T, num_filters_in_t, num_filters_out_t, num_features_in_t, kernel_size_time_t, kernel_size_feature_t,\n"
"    dilation_rate_t, stride_t, valid_pad_t>::setWeights(const std::vector<std::vector<std::vector<std::vector<T>>>>& inWeights)\n"
"{\n"
"    for(int i = 0; i < kernel_size_time_t; i++)\n"
"    {\n"
"        conv1dLayers[i].setWeights(inWeights[i]);\n"
"    }\n"
"}\n"
"\n"
"template <typename T, int num_filters_in_t, int num_filters_out_t, int num_features_in_t, int kernel_size_time_t,\n"
"    int kernel_size_feature_t, int dilation_rate_t, int stride_t, bool valid_pad_t>\n"
"void Conv2DT<T, num_filters_in_t, num_filters_out_t, num_features_in_t, kernel_size_time_t,\n"
"    kernel_size_feature_t, dilation_rate_t, stride_t, valid_pad_t>::setBias(const std::vector<T>& inBias)\n"
"{\n"
"    for(int i = 0; i < num_filters_out_t; i++)\n"
"    {\n"
"        bias(i) = inBias[i];\n"
"    }\n"
"}\n"
"} // RTNEURAL_NAMESPACE";

const char* conv2d_eigen_tpp = (const char*) temp_binary_data_48;

//================== conv2d_xsimd.tpp ==================
static const unsigned char temp_binary_data_49[] =
"#include \"conv2d_xsimd.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"template <typename T>\n"
"Conv2D<T>::Conv2D(int in_num_filters_in, int in_num_filters_out, int in_num_features_in, int in_kernel_size_time, int in_kernel_size_feature,\n"
"    int in_dilation_rate, int in_stride, bool in_valid_pad)\n"
"    : num_filters_in(in_num_filters_in)\n"
"    , num_filters_out(in_num_filters_out)\n"
"    , num_features_in(in_num_features_in)\n"
"    , kernel_size_time(in_kernel_size_time)\n"
"    , kernel_size_feature(in_kernel_size_feature)\n"
"    , dilation_rate(in_dilation_rate)\n"
"    , stride(in_stride)\n"
"    , num_features_out(Conv1DStateless<T>::computeNumFeaturesOut(in_num_features_in, in_kernel_size_feature, in_stride, in_valid_pad))\n"
"    , receptive_field(1 + (in_kernel_size_time - 1) * in_dilation_rate) // See \"Dilated (atrous) convolution\" note here: https://distill.pub/2019/computing-receptive-fields/\n"
"    , valid_pad(in_valid_pad)\n"
"    , Layer<T>(in_num_features_in * in_num_filters_in, Conv1DStateless<T>::computeNumFeaturesOut(in_num_features_in, in_kernel_size_feature, in_stride, in_valid_pad) * in_num_filters_out)\n"
"{\n"
"    conv1dLayers.resize(kernel_size_time, Conv1DStateless<T>(num_filters_in, num_features_in, num_filters_out, kernel_size_feature, stride, valid_pad));\n"
"    bias.resize(num_filters_out, (T)0);\n"
"\n"
"    state.resize(receptive_field);\n"
"    for(auto& stateMat : state)\n"
"    {\n"
"        stateMat.resize(num_filters_out * num_features_out, (T)0);\n"
"    }\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv2D<T>::Conv2D(std::initializer_list<int> sizes)\n"
"    : Conv2D<T>(*sizes.begin(), *(sizes.begin() + 1), *(sizes.begin() + 2), *(sizes.begin() + 3), *(sizes.begin() + 4),\n"
"        *(sizes.begin() + 5), *(sizes.begin() + 6), *(sizes.begin() + 7))\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv2D<T>::Conv2D(const Conv2D& other)\n"
"    : Conv2D<T>(other.num_filters_in, other.num_filters_out, other.num_features_in, other.kernel_size_time, other.kernel_size_feature,\n"
"        other.dilation_rate, other.stride, other.valid_pad)\n"
"{\n"
"}\n"
"\n"
"template <typename T>\n"
"Conv2D<T>& Conv2D<T>::operator=(const Conv2D& other)\n"
"{\n"
"    return *this = Conv2D<T>(other);\n"
"}\n"
"\n"
"template <typename T>\n"
"void Conv2D<T>::setWeights(const std::vector<std::vector<std::vector<std::vector<T>>>>& inWeights)\n"
"{\n"
"    for(int i = 0; i < kernel_size_time; i++)\n"
"    {\n"
"        conv1dLayers[i].setWeights(inWeights[i]);\n"
"    }\n"
"}\n"
"\n"
"template <typename T>\n"
"void Conv2D<T>::setBias(const std::vector<T>& inBias)\n"
"{\n"
"    std::copy(inBias.begin(), inBias.end(), bias.begin());\n"
"}\n"
"\n"
"template <typename T, int num_filters_in_t, int num_filters_out_t, int num_features_in_t, int kernel_size_time_t, int kernel_size_feature_t, int dilation_rate_t, int stride_t, bool valid_pad_t>\n"
"Conv2DT<T, num_filters_in_t, num_filters_out_t, num_features_in_t, kernel_size_time_t, kernel_size_feature_t, dilation_rate_t, stride_t, valid_pad_t>::Conv2DT()\n"
"{\n"
"}\n"
"\n"
"template <typename T, int num_filters_in_t, int num_filters_out_t, int num_features_in_t, int kernel_size_time_t,\n"
"    int kernel_size_feature_t, int dilation_rate_t, int stride_t, bool valid_pad_t>\n"
"void Conv2DT<T, num_filters_in_t, num_filters_out_t, num_features_in_t, kernel_size_time_t, kernel_size_feature_t,\n"
"    dilation_rate_t, stride_t, valid_pad_t>::setWeights(const std::vector<std::vector<std::vector<std::vector<T>>>>& inWeights)\n"
"{\n"
"    for(int i = 0; i < kernel_size_time_t; i++)\n"
"    {\n"
"        conv1dLayers[i].setWeights(inWeights[i]);\n"
"    }\n"
"}\n"
"\n"
"template <typename T, int num_filters_in_t, int num_filters_out_t, int num_features_in_t, int kernel_size_time_t,\n"
"    int kernel_size_feature_t, int dilation_rate_t, int stride_t, bool valid_pad_t>\n"
"void Conv2DT<T, num_filters_in_t, num_filters_out_t, num_features_in_t, kernel_size_time_t,\n"
"    kernel_size_feature_t, dilation_rate_t, stride_t, valid_pad_t>::setBias(const std::vector<T>& inBias)\n"
"{\n"
"    std::copy(inBias.begin(), inBias.end(), reinterpret_cast<T*>(std::begin(bias)));\n"
"}\n"
"} // RTNEURAL_NAMESPACE\n";

const char* conv2d_xsimd_tpp = (const char*) temp_binary_data_49;

//================== gru.tpp ==================
static const unsigned char temp_binary_data_50[] =
"#include \"gru.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"\n"
"#if !RTNEURAL_USE_EIGEN && !RTNEURAL_USE_XSIMD\n"
"template <typename T, typename MathsProvider>\n"
"GRULayer<T, MathsProvider>::GRULayer(int in_size, int out_size)\n"
"    : Layer<T>(in_size, out_size)\n"
"    , zWeights(in_size, out_size)\n"
"    , rWeights(in_size, out_size)\n"
"    , cWeights(in_size, out_size)\n"
"{\n"
"    ht1 = new T[out_size];\n"
"    zVec = new T[out_size];\n"
"    rVec = new T[out_size];\n"
"    cVec = new T[out_size];\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"GRULayer<T, MathsProvider>::GRULayer(std::initializer_list<int> sizes)\n"
"    : GRULayer<T, MathsProvider>(*sizes.begin(), *(sizes.begin() + 1))\n"
"{\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"GRULayer<T, MathsProvider>::GRULayer(const GRULayer<T, MathsProvider>& other)\n"
"    : GRULayer<T, MathsProvider>(other.in_size, other.out_size)\n"
"{\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"GRULayer<T, MathsProvider>& GRULayer<T, MathsProvider>::operator=(const GRULayer<T, MathsProvider>& other)\n"
"{\n"
"    if(&other != this)\n"
"        *this = GRULayer<T, MathsProvider>(other);\n"
"    return *this;\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"GRULayer<T, MathsProvider>::~GRULayer()\n"
"{\n"
"    delete[] ht1;\n"
"    delete[] zVec;\n"
"    delete[] rVec;\n"
"    delete[] cVec;\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"GRULayer<T, MathsProvider>::WeightSet::WeightSet(int in_size, int out_size)\n"
"    : out_size(out_size)\n"
"{\n"
"    W = new T*[out_size];\n"
"    U = new T*[out_size];\n"
"    b = new T*[kNumBiasLayers];\n"
"\n"
"    for(int i = 0; i < kNumBiasLayers; ++i)\n"
"    {\n"
"        b[i] = new T[out_size];\n"
"    }\n"
"\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        W[i] = new T[in_size];\n"
"        U[i] = new T[out_size];\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"GRULayer<T, MathsProvider>::WeightSet::~WeightSet()\n"
"{\n"
"    for(int i = 0; i < kNumBiasLayers; ++i)\n"
"    {\n"
"        delete[] b[i];\n"
"    }\n"
"\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        delete[] W[i];\n"
"        delete[] U[i];\n"
"    }\n"
"\n"
"    delete[] b;\n"
"    delete[] W;\n"
"    delete[] U;\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void GRULayer<T, MathsProvider>::setWVals(const std::vector<std::vector<T>>& wVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::in_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"        {\n"
"            zWeights.W[k][i] = wVals[i][k];\n"
"            rWeights.W[k][i] = wVals[i][k + Layer<T>::out_size];\n"
"            cWeights.W[k][i] = wVals[i][k + Layer<T>::out_size * 2];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void GRULayer<T, MathsProvider>::setWVals(T** wVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::in_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"        {\n"
"            zWeights.W[k][i] = wVals[i][k];\n"
"            rWeights.W[k][i] = wVals[i][k + Layer<T>::out_size];\n"
"            cWeights.W[k][i] = wVals[i][k + Layer<T>::out_size * 2];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void GRULayer<T, MathsProvider>::setUVals(const std::vector<std::vector<T>>& uVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"        {\n"
"            zWeights.U[k][i] = uVals[i][k];\n"
"            rWeights.U[k][i] = uVals[i][k + Layer<T>::out_size];\n"
"            cWeights.U[k][i] = uVals[i][k + Layer<T>::out_size * 2];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void GRULayer<T, MathsProvider>::setUVals(T** uVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"        {\n"
"            zWeights.U[k][i] = uVals[i][k];\n"
"            rWeights.U[k][i] = uVals[i][k + Layer<T>::out_size];\n"
"            cWeights.U[k][i] = uVals[i][k + Layer<T>::out_size * 2];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void GRULayer<T, MathsProvider>::setBVals(const std::vector<std::vector<T>>& bVals)\n"
"{\n"
"    for(int i = 0; i < 2; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"        {\n"
"            zWeights.b[i][k] = bVals[i][k];\n"
"            rWeights.b[i][k] = bVals[i][k + Layer<T>::out_size];\n"
"            cWeights.b[i][k] = bVals[i][k + Layer<T>::out_size * 2];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void GRULayer<T, MathsProvider>::setBVals(T** bVals)\n"
"{\n"
"    for(int i = 0; i < 2; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"        {\n"
"            zWeights.b[i][k] = bVals[i][k];\n"
"            rWeights.b[i][k] = bVals[i][k + Layer<T>::out_size];\n"
"            cWeights.b[i][k] = bVals[i][k + Layer<T>::out_size * 2];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"T GRULayer<T, MathsProvider>::getWVal(int i, int k) const noexcept\n"
"{\n"
"    T** set = zWeights.W;\n"
"    if(k > 2 * Layer<T>::out_size)\n"
"    {\n"
"        k -= 2 * Layer<T>::out_size;\n"
"        set = cWeights.W;\n"
"    }\n"
"    else if(k > Layer<T>::out_size)\n"
"    {\n"
"        k -= Layer<T>::out_size;\n"
"        set = rWeights.W;\n"
"    }\n"
"\n"
"    return set[i][k];\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"T GRULayer<T, MathsProvider>::getUVal(int i, int k) const noexcept\n"
"{\n"
"    T** set = zWeights.U;\n"
"    if(k > 2 * Layer<T>::out_size)\n"
"    {\n"
"        k -= 2 * Layer<T>::out_size;\n"
"        set = cWeights.U;\n"
"    }\n"
"    else if(k > Layer<T>::out_size)\n"
"    {\n"
"        k -= Layer<T>::out_size;\n"
"        set = rWeights.U;\n"
"    }\n"
"\n"
"    return set[i][k];\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"T GRULayer<T, MathsProvider>::getBVal(int i, int k) const noexcept\n"
"{\n"
"    T** set = zWeights.b;\n"
"    if(k > 2 * Layer<T>::out_size)\n"
"    {\n"
"        k -= 2 * Layer<T>::out_size;\n"
"        set = cWeights.b;\n"
"    }\n"
"    else if(k > Layer<T>::out_size)\n"
"    {\n"
"        k -= Layer<T>::out_size;\n"
"        set = rWeights.b;\n"
"    }\n"
"\n"
"    return set[i][k];\n"
"}\n"
"\n"
"//====================================================\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::GRULayerT()\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        // single-input kernel weights\n"
"        Wz_1[i] = (T)0;\n"
"        Wr_1[i] = (T)0;\n"
"        Wh_1[i] = (T)0;\n"
"\n"
"        // biases\n"
"        bz[i] = (T)0;\n"
"        br[i] = (T)0;\n"
"        bh0[i] = (T)0;\n"
"        bh1[i] = (T)0;\n"
"\n"
"        // intermediate vars\n"
"        zt[i] = (T)0;\n"
"        rt[i] = (T)0;\n"
"        ct[i] = (T)0;\n"
"        ht[i] = (T)0;\n"
"    }\n"
"\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        // recurrent weights\n"
"        for(int k = 0; k < out_size; ++k)\n"
"        {\n"
"            Uz[i][k] = (T)0;\n"
"            Ur[i][k] = (T)0;\n"
"            Uh[i][k] = (T)0;\n"
"        }\n"
"\n"
"        // kernel weights\n"
"        for(int k = 0; k < in_size; ++k)\n"
"        {\n"
"            Wz[i][k] = (T)0;\n"
"            Wr[i][k] = (T)0;\n"
"            Wh[i][k] = (T)0;\n"
"        }\n"
"    }\n"
"\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"template <SampleRateCorrectionMode srCorr>\n"
"std::enable_if_t<srCorr == SampleRateCorrectionMode::NoInterp, void>\n"
"GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::prepare(int delaySamples)\n"
"{\n"
"    delayWriteIdx = delaySamples - 1;\n"
"    outs_delayed.resize(delayWriteIdx + 1, {});\n"
"\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"template <SampleRateCorrectionMode srCorr>\n"
"std::enable_if_t<srCorr == SampleRateCorrectionMode::LinInterp, void>\n"
"GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::prepare(T delaySamples)\n"
"{\n"
"    const auto delayOffFactor = delaySamples - std::floor(delaySamples);\n"
"    delayMult = (T)1 - delayOffFactor;\n"
"    delayPlus1Mult = delayOffFactor;\n"
"\n"
"    delayWriteIdx = (int)std::ceil(delaySamples) - (int)std::ceil(delayOffFactor);\n"
"    outs_delayed.resize(delayWriteIdx + 1, {});\n"
"\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::reset()\n"
"{\n"
"    if(sampleRateCorr != SampleRateCorrectionMode::None)\n"
"    {\n"
"        for(auto& vec : outs_delayed)\n"
"            std::fill(vec.begin(), vec.end(), T {});\n"
"    }\n"
"\n"
"    // reset output state\n"
"    for(int i = 0; i < out_size; ++i)\n"
"        outs[i] = (T)0;\n"
"}\n"
"\n"
"// kernel weights\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::setWVals(const std::vector<std::vector<T>>& wVals)\n"
"{\n"
"    for(int i = 0; i < in_size; ++i)\n"
"    {\n"
"        for(int j = 0; j < out_size; ++j)\n"
"        {\n"
"            Wz[j][i] = wVals[i][j];\n"
"            Wr[j][i] = wVals[i][j + out_size];\n"
"            Wh[j][i] = wVals[i][j + 2 * out_size];\n"
"        }\n"
"    }\n"
"\n"
"    for(int j = 0; j < out_size; ++j)\n"
"    {\n"
"        Wz_1[j] = wVals[0][j];\n"
"        Wr_1[j] = wVals[0][j + out_size];\n"
"        Wh_1[j] = wVals[0][j + 2 * out_size];\n"
"    }\n"
"}\n"
"\n"
"// recurrent weights\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::setUVals(const std::vector<std::vector<T>>& uVals)\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        for(int j = 0; j < out_size; ++j)\n"
"        {\n"
"            Uz[j][i] = uVals[i][j];\n"
"            Ur[j][i] = uVals[i][j + out_size];\n"
"            Uh[j][i] = uVals[i][j + 2 * out_size];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"// biases\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::setBVals(const std::vector<std::vector<T>>& bVals)\n"
"{\n"
"    for(int k = 0; k < out_size; ++k)\n"
"    {\n"
"        bz[k] = bVals[0][k] + bVals[1][k];\n"
"        br[k] = bVals[0][k + out_size] + bVals[1][k + out_size];\n"
"        bh0[k] = bVals[0][k + 2 * out_size];\n"
"        bh1[k] = bVals[1][k + 2 * out_size];\n"
"    }\n"
"}\n"
"\n"
"#endif // !RTNEURAL_USE_EIGEN && !RTNEURAL_USE_XSIMD\n"
"\n"
"} // namespace RTNEURAL_NAMESPACE\n";

const char* gru_tpp = (const char*) temp_binary_data_50;

//================== gru_eigen.tpp ==================
static const unsigned char temp_binary_data_51[] =
"#if RTNEURAL_USE_EIGEN\n"
"\n"
"#include \"gru_eigen.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"GRULayer<T, MathsProvider>::GRULayer(int in_size, int out_size)\n"
"    : Layer<T>(in_size, out_size)\n"
"{\n"
"    wCombinedWeights = Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>::Zero(3 * out_size, in_size + 1);\n"
"    uCombinedWeights = Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>::Zero(3 * out_size, out_size + 1);\n"
"    extendedInVec = Eigen::Matrix<T, Eigen::Dynamic, 1>::Zero(in_size + 1);\n"
"    extendedHt1 = Eigen::Matrix<T, Eigen::Dynamic, 1>::Zero(out_size + 1);\n"
"    extendedInVec(Layer<T>::in_size) = (T)1;\n"
"    extendedHt1(Layer<T>::out_size) = (T)1;\n"
"\n"
"    alphaVec = Eigen::Matrix<T, Eigen::Dynamic, 1>::Zero(3 * out_size);\n"
"    betaVec = Eigen::Matrix<T, Eigen::Dynamic, 1>::Zero(3 * out_size);\n"
"    gammaVec = Eigen::Matrix<T, Eigen::Dynamic, 1>::Zero(2 * out_size);\n"
"    cVec = Eigen::Matrix<T, Eigen::Dynamic, 1>::Zero(out_size);\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"GRULayer<T, MathsProvider>::GRULayer(std::initializer_list<int> sizes)\n"
"    : GRULayer<T, MathsProvider>(*sizes.begin(), *(sizes.begin() + 1))\n"
"{\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"GRULayer<T, MathsProvider>::GRULayer(const GRULayer<T, MathsProvider>& other)\n"
"    : GRULayer<T, MathsProvider>(other.in_size, other.out_size)\n"
"{\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"GRULayer<T, MathsProvider>& GRULayer<T, MathsProvider>::operator=(const GRULayer<T, MathsProvider>& other)\n"
"{\n"
"    return *this = GRULayer<T, MathsProvider>(other);\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void GRULayer<T, MathsProvider>::setWVals(const std::vector<std::vector<T>>& wVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::in_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size * 3; ++k)\n"
"        {\n"
"            wCombinedWeights(k, i) = wVals[i][k];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void GRULayer<T, MathsProvider>::setWVals(T** wVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::in_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size * 3; ++k)\n"
"        {\n"
"            wCombinedWeights(k, i) = wVals[i][k];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void GRULayer<T, MathsProvider>::setUVals(const std::vector<std::vector<T>>& uVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size * 3; ++k)\n"
"        {\n"
"            uCombinedWeights(k, i) = uVals[i][k];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void GRULayer<T, MathsProvider>::setUVals(T** uVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size * 3; ++k)\n"
"        {\n"
"            uCombinedWeights(k, i) = uVals[i][k];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void GRULayer<T, MathsProvider>::setBVals(const std::vector<std::vector<T>>& bVals)\n"
"{\n"
"    for(int k = 0; k < Layer<T>::out_size * 3; ++k)\n"
"    {\n"
"        wCombinedWeights(k, Layer<T>::in_size) = bVals[0][k];\n"
"        uCombinedWeights(k, Layer<T>::out_size) = bVals[1][k];\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void GRULayer<T, MathsProvider>::setBVals(T** bVals)\n"
"{\n"
"    for(int k = 0; k < Layer<T>::out_size * 3; ++k)\n"
"    {\n"
"        wCombinedWeights(k, Layer<T>::in_size) = bVals[0][k];\n"
"        uCombinedWeights(k, Layer<T>::out_size) = bVals[1][k];\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"T GRULayer<T, MathsProvider>::getWVal(int i, int k) const noexcept\n"
"{\n"
"    return wCombinedWeights[k][i];\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"T GRULayer<T, MathsProvider>::getUVal(int i, int k) const noexcept\n"
"{\n"
"    return uCombinedWeights[k][i];\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"T GRULayer<T, MathsProvider>::getBVal(int i, int k) const noexcept\n"
"{\n"
"    T val;\n"
"    if(i == 0)\n"
"    {\n"
"        val = wCombinedWeights[k][Layer<T>::in_size];\n"
"    }\n"
"    else\n"
"    {\n"
"        val = uCombinedWeights[k][Layer<T>::out_size];\n"
"    }\n"
"    return val;\n"
"}\n"
"\n"
"//====================================================\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::GRULayerT()\n"
"    : outs(outs_internal)\n"
"{\n"
"    wCombinedWeights = w_k_type::Zero();\n"
"    uCombinedWeights = u_k_type::Zero();\n"
"    alphaVec = three_out_type::Zero();\n"
"    betaVec = three_out_type::Zero();\n"
"    gammaVec = two_out_type::Zero();\n"
"    cVec = out_type::Zero();\n"
"    extendedInVec = extended_in_type::Zero();\n"
"    extendedHt1 = extended_out_type::Zero();\n"
"\n"
"    extendedInVec(in_sizet) = (T)1;\n"
"    extendedHt1(out_sizet) = (T)1;\n"
"\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"template <SampleRateCorrectionMode srCorr>\n"
"std::enable_if_t<srCorr == SampleRateCorrectionMode::NoInterp, void>\n"
"GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::prepare(int delaySamples)\n"
"{\n"
"    delayWriteIdx = delaySamples - 1;\n"
"    outs_delayed.resize(delayWriteIdx + 1, {});\n"
"\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"template <SampleRateCorrectionMode srCorr>\n"
"std::enable_if_t<srCorr == SampleRateCorrectionMode::LinInterp, void>\n"
"GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::prepare(T delaySamples)\n"
"{\n"
"    const auto delayOffFactor = delaySamples - std::floor(delaySamples);\n"
"    delayMult = (T)1 - delayOffFactor;\n"
"    delayPlus1Mult = delayOffFactor;\n"
"\n"
"    delayWriteIdx = (int)std::ceil(delaySamples) - (int)std::ceil(delayOffFactor);\n"
"    outs_delayed.resize(delayWriteIdx + 1, {});\n"
"\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::reset()\n"
"{\n"
"    if(sampleRateCorr != SampleRateCorrectionMode::None)\n"
"    {\n"
"        for(auto& vec : outs_delayed)\n"
"            vec = out_type::Zero();\n"
"    }\n"
"\n"
"    // reset output state\n"
"    outs = out_type::Zero();\n"
"\n"
"    // reset extended internal state\n"
"    extendedHt1.setZero();\n"
"    extendedHt1(out_sizet) = (T)1;\n"
"}\n"
"\n"
"// kernel weights\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::setWVals(const std::vector<std::vector<T>>& wVals)\n"
"{\n"
"    for(int i = 0; i < in_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < out_size * 3; ++k)\n"
"        {\n"
"            wCombinedWeights(k, i) = wVals[i][k];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"// recurrent weights\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::setUVals(const std::vector<std::vector<T>>& uVals)\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < out_size * 3; ++k)\n"
"        {\n"
"            uCombinedWeights(k, i) = uVals[i][k];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"// biases\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::setBVals(const std::vector<std::vector<T>>& bVals)\n"
"{\n"
"    for(int k = 0; k < out_size * 3; ++k)\n"
"    {\n"
"        wCombinedWeights(k, in_sizet) = bVals[0][k];\n"
"        uCombinedWeights(k, out_sizet) = bVals[1][k];\n"
"    }\n"
"}\n"
"\n"
"} // namespace RTNEURAL_NAMESPACE\n"
"\n"
"#endif // RTNEURAL_USE_EIGEN\n";

const char* gru_eigen_tpp = (const char*) temp_binary_data_51;

//================== gru_xsimd.tpp ==================
static const unsigned char temp_binary_data_52[] =
"#include \"gru_xsimd.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"GRULayer<T, MathsProvider>::GRULayer(int in_size, int out_size)\n"
"    : Layer<T>(in_size, out_size)\n"
"    , zWeights(in_size, out_size)\n"
"    , rWeights(in_size, out_size)\n"
"    , cWeights(in_size, out_size)\n"
"{\n"
"    ht1.resize(out_size, (T)0);\n"
"    zVec.resize(out_size, (T)0);\n"
"    rVec.resize(out_size, (T)0);\n"
"    cVec.resize(out_size, (T)0);\n"
"    cTmp.resize(out_size, (T)0);\n"
"\n"
"    prod_in.resize(in_size, (T)0);\n"
"    prod_out.resize(out_size, (T)0);\n"
"\n"
"    ones.resize(out_size, (T)1);\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"GRULayer<T, MathsProvider>::GRULayer(std::initializer_list<int> sizes)\n"
"    : GRULayer<T, MathsProvider>(*sizes.begin(), *(sizes.begin() + 1))\n"
"{\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"GRULayer<T, MathsProvider>::GRULayer(const GRULayer<T, MathsProvider>& other)\n"
"    : GRULayer<T, MathsProvider>(other.in_size, other.out_size)\n"
"{\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"GRULayer<T, MathsProvider>& GRULayer<T, MathsProvider>::operator=(const GRULayer<T, MathsProvider>& other)\n"
"{\n"
"    return *this = GRULayer<T, MathsProvider>(other);\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"GRULayer<T, MathsProvider>::~GRULayer() = default;\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"GRULayer<T, MathsProvider>::WeightSet::WeightSet(int in_size, int out_size)\n"
"    : out_size(out_size)\n"
"{\n"
"    W = vec2_type(out_size, vec_type(in_size, (T)0));\n"
"    U = vec2_type(out_size, vec_type(out_size, (T)0));\n"
"\n"
"    b[0].resize(out_size, (T)0);\n"
"    b[1].resize(out_size, (T)0);\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"GRULayer<T, MathsProvider>::WeightSet::~WeightSet() = default;\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void GRULayer<T, MathsProvider>::setWVals(const std::vector<std::vector<T>>& wVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::in_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"        {\n"
"            zWeights.W[k][i] = wVals[i][k];\n"
"            rWeights.W[k][i] = wVals[i][k + Layer<T>::out_size];\n"
"            cWeights.W[k][i] = wVals[i][k + Layer<T>::out_size * 2];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void GRULayer<T, MathsProvider>::setWVals(T** wVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::in_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"        {\n"
"            zWeights.W[k][i] = wVals[i][k];\n"
"            rWeights.W[k][i] = wVals[i][k + Layer<T>::out_size];\n"
"            cWeights.W[k][i] = wVals[i][k + Layer<T>::out_size * 2];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void GRULayer<T, MathsProvider>::setUVals(const std::vector<std::vector<T>>& uVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"        {\n"
"            zWeights.U[k][i] = uVals[i][k];\n"
"            rWeights.U[k][i] = uVals[i][k + Layer<T>::out_size];\n"
"            cWeights.U[k][i] = uVals[i][k + Layer<T>::out_size * 2];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void GRULayer<T, MathsProvider>::setUVals(T** uVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"        {\n"
"            zWeights.U[k][i] = uVals[i][k];\n"
"            rWeights.U[k][i] = uVals[i][k + Layer<T>::out_size];\n"
"            cWeights.U[k][i] = uVals[i][k + Layer<T>::out_size * 2];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void GRULayer<T, MathsProvider>::setBVals(const std::vector<std::vector<T>>& bVals)\n"
"{\n"
"    for(int i = 0; i < 2; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"        {\n"
"            zWeights.b[i][k] = bVals[i][k];\n"
"            rWeights.b[i][k] = bVals[i][k + Layer<T>::out_size];\n"
"            cWeights.b[i][k] = bVals[i][k + Layer<T>::out_size * 2];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void GRULayer<T, MathsProvider>::setBVals(T** bVals)\n"
"{\n"
"    for(int i = 0; i < 2; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"        {\n"
"            zWeights.b[i][k] = bVals[i][k];\n"
"            rWeights.b[i][k] = bVals[i][k + Layer<T>::out_size];\n"
"            cWeights.b[i][k] = bVals[i][k + Layer<T>::out_size * 2];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"T GRULayer<T, MathsProvider>::getWVal(int i, int k) const noexcept\n"
"{\n"
"    T** set = zWeights.W;\n"
"    if(k > 2 * Layer<T>::out_size)\n"
"    {\n"
"        k -= 2 * Layer<T>::out_size;\n"
"        set = cWeights.W;\n"
"    }\n"
"    else if(k > Layer<T>::out_size)\n"
"    {\n"
"        k -= Layer<T>::out_size;\n"
"        set = rWeights.W;\n"
"    }\n"
"\n"
"    return set[i][k];\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"T GRULayer<T, MathsProvider>::getUVal(int i, int k) const noexcept\n"
"{\n"
"    T** set = zWeights.U;\n"
"    if(k > 2 * Layer<T>::out_size)\n"
"    {\n"
"        k -= 2 * Layer<T>::out_size;\n"
"        set = cWeights.U;\n"
"    }\n"
"    else if(k > Layer<T>::out_size)\n"
"    {\n"
"        k -= Layer<T>::out_size;\n"
"        set = rWeights.U;\n"
"    }\n"
"\n"
"    return set[i][k];\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"T GRULayer<T, MathsProvider>::getBVal(int i, int k) const noexcept\n"
"{\n"
"    T** set = zWeights.b;\n"
"    if(k > 2 * Layer<T>::out_size)\n"
"    {\n"
"        k -= 2 * Layer<T>::out_size;\n"
"        set = cWeights.b;\n"
"    }\n"
"    else if(k > Layer<T>::out_size)\n"
"    {\n"
"        k -= Layer<T>::out_size;\n"
"        set = rWeights.b;\n"
"    }\n"
"\n"
"    return set[i][k];\n"
"}\n"
"\n"
"//====================================================\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::GRULayerT()\n"
"{\n"
"    for(int i = 0; i < v_out_size; ++i)\n"
"    {\n"
"        // single-input kernel weights\n"
"        Wz_1[i] = v_type((T)0);\n"
"        Wr_1[i] = v_type((T)0);\n"
"        Wh_1[i] = v_type((T)0);\n"
"\n"
"        // biases\n"
"        bz[i] = v_type((T)0);\n"
"        br[i] = v_type((T)0);\n"
"        bh0[i] = v_type((T)0);\n"
"        bh1[i] = v_type((T)0);\n"
"\n"
"        // intermediate vars\n"
"        zt[i] = v_type((T)0);\n"
"        rt[i] = v_type((T)0);\n"
"        ct[i] = v_type((T)0);\n"
"        ht[i] = v_type((T)0);\n"
"    }\n"
"\n"
"    // kernel weights\n"
"    for(int k = 0; k < in_size; ++k)\n"
"    {\n"
"        for(int i = 0; i < v_out_size; ++i)\n"
"        {\n"
"            Wz[k][i] = v_type((T)0);\n"
"            Wr[k][i] = v_type((T)0);\n"
"            Wh[k][i] = v_type((T)0);\n"
"        }\n"
"    }\n"
"\n"
"    // recurrent weights\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < v_out_size; ++k)\n"
"        {\n"
"            Uz[i][k] = v_type((T)0);\n"
"            Ur[i][k] = v_type((T)0);\n"
"            Uh[i][k] = v_type((T)0);\n"
"        }\n"
"    }\n"
"\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"template <SampleRateCorrectionMode srCorr>\n"
"std::enable_if_t<srCorr == SampleRateCorrectionMode::NoInterp, void>\n"
"GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::prepare(int delaySamples)\n"
"{\n"
"    delayWriteIdx = delaySamples - 1;\n"
"    outs_delayed.resize(delayWriteIdx + 1, {});\n"
"\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"template <SampleRateCorrectionMode srCorr>\n"
"std::enable_if_t<srCorr == SampleRateCorrectionMode::LinInterp, void>\n"
"GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::prepare(T delaySamples)\n"
"{\n"
"    const auto delayOffFactor = delaySamples - std::floor(delaySamples);\n"
"    delayMult = (T)1 - delayOffFactor;\n"
"    delayPlus1Mult = delayOffFactor;\n"
"\n"
"    delayWriteIdx = (int)std::ceil(delaySamples) - (int)std::ceil(delayOffFactor);\n"
"    outs_delayed.resize(delayWriteIdx + 1, {});\n"
"\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::reset()\n"
"{\n"
"    if(sampleRateCorr != SampleRateCorrectionMode::None)\n"
"    {\n"
"        for(auto& vec : outs_delayed)\n"
"            std::fill(vec.begin(), vec.end(), v_type {});\n"
"    }\n"
"\n"
"    // reset output state\n"
"    for(int i = 0; i < v_out_size; ++i)\n"
"        outs[i] = v_type((T)0);\n"
"}\n"
"\n"
"// kernel weights\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::setWVals(const std::vector<std::vector<T>>& wVals)\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < in_size; ++k)\n"
"        {\n"
"            Wz[k][i / v_size] = set_value(Wz[k][i / v_size], i % v_size, wVals[k][i]);\n"
"            Wr[k][i / v_size] = set_value(Wr[k][i / v_size], i % v_size, wVals[k][i + out_size]);\n"
"            Wh[k][i / v_size] = set_value(Wh[k][i / v_size], i % v_size, wVals[k][i + 2 * out_size]);\n"
"        }\n"
"    }\n"
"\n"
"    for(int j = 0; j < out_size; ++j)\n"
"    {\n"
"        Wz_1[j / v_size] = set_value(Wz_1[j / v_size], j % v_size, wVals[0][j]);\n"
"        Wr_1[j / v_size] = set_value(Wr_1[j / v_size], j % v_size, wVals[0][j + out_size]);\n"
"        Wh_1[j / v_size] = set_value(Wh_1[j / v_size], j % v_size, wVals[0][j + 2 * out_size]);\n"
"    }\n"
"}\n"
"\n"
"// recurrent weights\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::setUVals(const std::vector<std::vector<T>>& uVals)\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < out_size; ++k)\n"
"        {\n"
"            Uz[k][i / v_size] = set_value(Uz[k][i / v_size], i % v_size, uVals[k][i]);\n"
"            Ur[k][i / v_size] = set_value(Ur[k][i / v_size], i % v_size, uVals[k][i + out_size]);\n"
"            Uh[k][i / v_size] = set_value(Uh[k][i / v_size], i % v_size, uVals[k][i + 2 * out_size]);\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"// biases\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void GRULayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::setBVals(const std::vector<std::vector<T>>& bVals)\n"
"{\n"
"    for(int k = 0; k < out_size; ++k)\n"
"    {\n"
"        bz[k / v_size] = set_value(bz[k / v_size], k % v_size, bVals[0][k] + bVals[1][k]);\n"
"        br[k / v_size] = set_value(br[k / v_size], k % v_size, bVals[0][k + out_size] + bVals[1][k + out_size]);\n"
"        bh0[k / v_size] = set_value(bh0[k / v_size], k % v_size, bVals[0][k + 2 * out_size]);\n"
"        bh1[k / v_size] = set_value(bh1[k / v_size], k % v_size, bVals[1][k + 2 * out_size]);\n"
"    }\n"
"}\n"
"\n"
"} // namespace RTNEURAL_NAMESPACE\n";

const char* gru_xsimd_tpp = (const char*) temp_binary_data_52;

//================== lstm.tpp ==================
static const unsigned char temp_binary_data_53[] =
"#include \"lstm.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"\n"
"#if !RTNEURAL_USE_EIGEN && !RTNEURAL_USE_XSIMD\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"LSTMLayer<T, MathsProvider>::LSTMLayer(int in_size, int out_size)\n"
"    : Layer<T>(in_size, out_size)\n"
"    , fWeights(in_size, out_size)\n"
"    , iWeights(in_size, out_size)\n"
"    , oWeights(in_size, out_size)\n"
"    , cWeights(in_size, out_size)\n"
"{\n"
"    ht1 = new T[out_size];\n"
"    ct1 = new T[out_size];\n"
"\n"
"    fVec = new T[out_size];\n"
"    iVec = new T[out_size];\n"
"    oVec = new T[out_size];\n"
"    ctVec = new T[out_size];\n"
"    cVec = new T[out_size];\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"LSTMLayer<T, MathsProvider>::LSTMLayer(std::initializer_list<int> sizes)\n"
"    : LSTMLayer<T, MathsProvider>(*sizes.begin(), *(sizes.begin() + 1))\n"
"{\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"LSTMLayer<T, MathsProvider>::LSTMLayer(const LSTMLayer& other)\n"
"    : LSTMLayer<T, MathsProvider>(other.in_size, other.out_size)\n"
"{\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"LSTMLayer<T, MathsProvider>& LSTMLayer<T, MathsProvider>::operator=(const LSTMLayer<T, MathsProvider>& other)\n"
"{\n"
"    if(&other != this)\n"
"        *this = LSTMLayer<T, MathsProvider>(other);\n"
"    return *this;\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"LSTMLayer<T, MathsProvider>::~LSTMLayer()\n"
"{\n"
"    delete[] ht1;\n"
"    delete[] ct1;\n"
"\n"
"    delete[] fVec;\n"
"    delete[] iVec;\n"
"    delete[] oVec;\n"
"    delete[] ctVec;\n"
"    delete[] cVec;\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void LSTMLayer<T, MathsProvider>::reset()\n"
"{\n"
"    std::fill(ht1, ht1 + Layer<T>::out_size, (T)0);\n"
"    std::fill(ct1, ct1 + Layer<T>::out_size, (T)0);\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"LSTMLayer<T, MathsProvider>::WeightSet::WeightSet(int in_size, int out_size)\n"
"    : out_size(out_size)\n"
"{\n"
"    W = new T*[out_size];\n"
"    U = new T*[out_size];\n"
"    b = new T[out_size];\n"
"\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        W[i] = new T[in_size];\n"
"        U[i] = new T[out_size];\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"LSTMLayer<T, MathsProvider>::WeightSet::~WeightSet()\n"
"{\n"
"    delete[] b;\n"
"\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        delete[] W[i];\n"
"        delete[] U[i];\n"
"    }\n"
"\n"
"    delete[] W;\n"
"    delete[] U;\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void LSTMLayer<T, MathsProvider>::setWVals(const std::vector<std::vector<T>>& wVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::in_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"        {\n"
"            iWeights.W[k][i] = wVals[i][k];\n"
"            fWeights.W[k][i] = wVals[i][k + Layer<T>::out_size];\n"
"            cWeights.W[k][i] = wVals[i][k + Layer<T>::out_size * 2];\n"
"            oWeights.W[k][i] = wVals[i][k + Layer<T>::out_size * 3];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void LSTMLayer<T, MathsProvider>::setUVals(const std::vector<std::vector<T>>& uVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"        {\n"
"            iWeights.U[k][i] = uVals[i][k];\n"
"            fWeights.U[k][i] = uVals[i][k + Layer<T>::out_size];\n"
"            cWeights.U[k][i] = uVals[i][k + Layer<T>::out_size * 2];\n"
"            oWeights.U[k][i] = uVals[i][k + Layer<T>::out_size * 3];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void LSTMLayer<T, MathsProvider>::setBVals(const std::vector<T>& bVals)\n"
"{\n"
"    for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"    {\n"
"        iWeights.b[k] = bVals[k];\n"
"        fWeights.b[k] = bVals[k + Layer<T>::out_size];\n"
"        cWeights.b[k] = bVals[k + Layer<T>::out_size * 2];\n"
"        oWeights.b[k] = bVals[k + Layer<T>::out_size * 3];\n"
"    }\n"
"}\n"
"\n"
"//====================================================\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::LSTMLayerT()\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        // single-input kernel weights\n"
"        Wf_1[i] = (T)0;\n"
"        Wi_1[i] = (T)0;\n"
"        Wo_1[i] = (T)0;\n"
"        Wc_1[i] = (T)0;\n"
"\n"
"        // biases\n"
"        bf[i] = (T)0;\n"
"        bi[i] = (T)0;\n"
"        bo[i] = (T)0;\n"
"        bc[i] = (T)0;\n"
"\n"
"        // intermediate vars\n"
"        ft[i] = (T)0;\n"
"        it[i] = (T)0;\n"
"        ot[i] = (T)0;\n"
"        ht[i] = (T)0;\n"
"    }\n"
"\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        // recurrent weights\n"
"        for(int k = 0; k < out_size; ++k)\n"
"        {\n"
"            Uf[i][k] = (T)0;\n"
"            Ui[i][k] = (T)0;\n"
"            Uo[i][k] = (T)0;\n"
"            Uc[i][k] = (T)0;\n"
"        }\n"
"\n"
"        // kernel weights\n"
"        for(int k = 0; k < in_size; ++k)\n"
"        {\n"
"            Wf[i][k] = (T)0;\n"
"            Wi[i][k] = (T)0;\n"
"            Wo[i][k] = (T)0;\n"
"            Wc[i][k] = (T)0;\n"
"        }\n"
"    }\n"
"\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"template <SampleRateCorrectionMode srCorr>\n"
"std::enable_if_t<srCorr == SampleRateCorrectionMode::NoInterp, void>\n"
"LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::prepare(int delaySamples)\n"
"{\n"
"    delayWriteIdx = delaySamples - 1;\n"
"    ct_delayed.resize(delayWriteIdx + 1, {});\n"
"    outs_delayed.resize(delayWriteIdx + 1, {});\n"
"\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"template <SampleRateCorrectionMode srCorr>\n"
"std::enable_if_t<srCorr == SampleRateCorrectionMode::LinInterp, void>\n"
"LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::prepare(T delaySamples)\n"
"{\n"
"    const auto delayOffFactor = delaySamples - std::floor(delaySamples);\n"
"    delayMult = (T)1 - delayOffFactor;\n"
"    delayPlus1Mult = delayOffFactor;\n"
"\n"
"    delayWriteIdx = (int)std::ceil(delaySamples) - (int)std::ceil(delayOffFactor);\n"
"    ct_delayed.resize(delayWriteIdx + 1, {});\n"
"    outs_delayed.resize(delayWriteIdx + 1, {});\n"
"\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::reset()\n"
"{\n"
"    if(sampleRateCorr != SampleRateCorrectionMode::None)\n"
"    {\n"
"        for(auto& x : ct_delayed)\n"
"            std::fill(x.begin(), x.end(), T {});\n"
"\n"
"        for(auto& x : outs_delayed)\n"
"            std::fill(x.begin(), x.end(), T {});\n"
"    }\n"
"\n"
"    // reset output state\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        ct[i] = (T)0;\n"
"        outs[i] = (T)0;\n"
"    }\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::setWVals(const std::vector<std::vector<T>>& wVals)\n"
"{\n"
"    for(int i = 0; i < in_size; ++i)\n"
"    {\n"
"        for(int j = 0; j < out_size; ++j)\n"
"        {\n"
"            Wi[j][i] = wVals[i][j];\n"
"            Wf[j][i] = wVals[i][j + out_size];\n"
"            Wc[j][i] = wVals[i][j + 2 * out_size];\n"
"            Wo[j][i] = wVals[i][j + 3 * out_size];\n"
"        }\n"
"    }\n"
"\n"
"    for(int j = 0; j < out_size; ++j)\n"
"    {\n"
"        Wi_1[j] = wVals[0][j];\n"
"        Wf_1[j] = wVals[0][j + out_size];\n"
"        Wc_1[j] = wVals[0][j + 2 * out_size];\n"
"        Wo_1[j] = wVals[0][j + 3 * out_size];\n"
"    }\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::setUVals(const std::vector<std::vector<T>>& uVals)\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        for(int j = 0; j < out_size; ++j)\n"
"        {\n"
"            Ui[j][i] = uVals[i][j];\n"
"            Uf[j][i] = uVals[i][j + out_size];\n"
"            Uc[j][i] = uVals[i][j + 2 * out_size];\n"
"            Uo[j][i] = uVals[i][j + 3 * out_size];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::setBVals(const std::vector<T>& bVals)\n"
"{\n"
"    for(int k = 0; k < out_size; ++k)\n"
"    {\n"
"        bi[k] = bVals[k];\n"
"        bf[k] = bVals[k + out_size];\n"
"        bc[k] = bVals[k + 2 * out_size];\n"
"        bo[k] = bVals[k + 3 * out_size];\n"
"    }\n"
"}\n"
"\n"
"#endif // !RTNEURAL_USE_EIGEN && !RTNEURAL_USE_XSIMD\n"
"\n"
"} // namespace RTNEURAL_NAMESPACE\n";

const char* lstm_tpp = (const char*) temp_binary_data_53;

//================== lstm_eigen.tpp ==================
static const unsigned char temp_binary_data_54[] =
"#include \"lstm_eigen.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"LSTMLayer<T, MathsProvider>::LSTMLayer(int in_size, int out_size)\n"
"    : Layer<T>(in_size, out_size)\n"
"{\n"
"    combinedWeights = Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>::Zero(4 * out_size, in_size + out_size + 1);\n"
"    extendedInVecHt1 = Eigen::Matrix<T, Eigen::Dynamic, 1>::Zero(in_size + out_size + 1);\n"
"    extendedInVecHt1(in_size + out_size) = (T)1;\n"
"\n"
"    fioctVecs = Eigen::Matrix<T, Eigen::Dynamic, 1>::Zero(4 * out_size);\n"
"    fioVecs = Eigen::Matrix<T, Eigen::Dynamic, 1>::Zero(3 * out_size);\n"
"    ctVec = Eigen::Matrix<T, Eigen::Dynamic, 1>::Zero(out_size);\n"
"\n"
"    cTanhVec = Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>::Zero(out_size, 1);\n"
"\n"
"    ht1 = Eigen::Matrix<T, Eigen::Dynamic, 1>::Zero(out_size);\n"
"    ct1 = Eigen::Matrix<T, Eigen::Dynamic, 1>::Zero(out_size);\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"LSTMLayer<T, MathsProvider>::LSTMLayer(std::initializer_list<int> sizes)\n"
"    : LSTMLayer<T, MathsProvider>(*sizes.begin(), *(sizes.begin() + 1))\n"
"{\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"LSTMLayer<T, MathsProvider>::LSTMLayer(const LSTMLayer& other)\n"
"    : LSTMLayer<T, MathsProvider>(other.in_size, other.out_size)\n"
"{\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"LSTMLayer<T, MathsProvider>& LSTMLayer<T, MathsProvider>::operator=(const LSTMLayer<T, MathsProvider>& other)\n"
"{\n"
"    return *this = LSTMLayer<T, MathsProvider>(other);\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void LSTMLayer<T, MathsProvider>::reset()\n"
"{\n"
"    ht1.setZero();\n"
"    ct1.setZero();\n"
"    extendedInVecHt1.setZero();\n"
"    extendedInVecHt1(Layer<T>::in_size + Layer<T>::out_size) = (T)1;\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void LSTMLayer<T, MathsProvider>::setWVals(const std::vector<std::vector<T>>& wVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::in_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"        {\n"
"            combinedWeights(k, i) = wVals[i][k + Layer<T>::out_size]; // Wf\n"
"            combinedWeights(k + Layer<T>::out_size, i) = wVals[i][k]; // Wi\n"
"            combinedWeights(k + Layer<T>::out_size * 2, i) = wVals[i][k + Layer<T>::out_size * 3]; // Wo\n"
"            combinedWeights(k + Layer<T>::out_size * 3, i) = wVals[i][k + Layer<T>::out_size * 2]; // Wc\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void LSTMLayer<T, MathsProvider>::setUVals(const std::vector<std::vector<T>>& uVals)\n"
"{\n"
"    int col;\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"    {\n"
"        col = i + Layer<T>::in_size;\n"
"        for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"        {\n"
"            combinedWeights(k, col) = uVals[i][k + Layer<T>::out_size]; // Uf\n"
"            combinedWeights(k + Layer<T>::out_size, col) = uVals[i][k]; // Ui\n"
"            combinedWeights(k + Layer<T>::out_size * 2, col) = uVals[i][k + Layer<T>::out_size * 3]; // Uo\n"
"            combinedWeights(k + Layer<T>::out_size * 3, col) = uVals[i][k + Layer<T>::out_size * 2]; // Uc\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void LSTMLayer<T, MathsProvider>::setBVals(const std::vector<T>& bVals)\n"
"{\n"
"    int col = Layer<T>::in_size + Layer<T>::out_size;\n"
"    for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"    {\n"
"        combinedWeights(k, col) = bVals[k + Layer<T>::out_size]; // Bf\n"
"        combinedWeights(k + Layer<T>::out_size, col) = bVals[k]; // Bi\n"
"        combinedWeights(k + Layer<T>::out_size * 2, col) = bVals[k + Layer<T>::out_size * 3]; // Bo\n"
"        combinedWeights(k + Layer<T>::out_size * 3, col) = bVals[k + Layer<T>::out_size * 2]; // Bc\n"
"    }\n"
"}\n"
"\n"
"//====================================================\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::LSTMLayerT()\n"
"    : outs(outs_internal)\n"
"{\n"
"    combinedWeights = weights_combined_type::Zero();\n"
"    extendedInHt1Vec = extended_in_out_type::Zero();\n"
"    fioctsVecs = four_out_type::Zero();\n"
"    fioVecs = three_out_type::Zero();\n"
"\n"
"    ctVec = out_type::Zero();\n"
"    cTanhVec = out_type::Zero();\n"
"\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"template <SampleRateCorrectionMode srCorr>\n"
"std::enable_if_t<srCorr == SampleRateCorrectionMode::NoInterp, void>\n"
"LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::prepare(int delaySamples)\n"
"{\n"
"    delayWriteIdx = delaySamples - 1;\n"
"    ct_delayed.resize(delayWriteIdx + 1, {});\n"
"    outs_delayed.resize(delayWriteIdx + 1, {});\n"
"\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"template <SampleRateCorrectionMode srCorr>\n"
"std::enable_if_t<srCorr == SampleRateCorrectionMode::LinInterp, void>\n"
"LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::prepare(T delaySamples)\n"
"{\n"
"    const auto delayOffFactor = delaySamples - std::floor(delaySamples);\n"
"    delayMult = (T)1 - delayOffFactor;\n"
"    delayPlus1Mult = delayOffFactor;\n"
"\n"
"    delayWriteIdx = (int)std::ceil(delaySamples) - (int)std::ceil(delayOffFactor);\n"
"    ct_delayed.resize(delayWriteIdx + 1, {});\n"
"    outs_delayed.resize(delayWriteIdx + 1, {});\n"
"\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::reset()\n"
"{\n"
"    if(sampleRateCorr != SampleRateCorrectionMode::None)\n"
"    {\n"
"        for(auto& x : ct_delayed)\n"
"            x = out_type::Zero();\n"
"\n"
"        for(auto& x : outs_delayed)\n"
"            x = out_type::Zero();\n"
"    }\n"
"\n"
"    // reset output state\n"
"    extendedInHt1Vec.setZero();\n"
"    extendedInHt1Vec(in_sizet + out_sizet) = (T)1;\n"
"    outs = out_type::Zero();\n"
"    cVec = out_type::Zero();\n"
"    ctVec.setZero();\n"
"}\n"
"\n"
"// kernel weights\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::setWVals(const std::vector<std::vector<T>>& wVals)\n"
"{\n"
"    for(int i = 0; i < in_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < out_size; ++k)\n"
"        {\n"
"            combinedWeights(k, i) = wVals[i][k + out_sizet]; // Wf\n"
"            combinedWeights(k + out_sizet, i) = wVals[i][k]; // Wi\n"
"            combinedWeights(k + out_sizet * 2, i) = wVals[i][k + out_sizet * 3]; // Wo\n"
"            combinedWeights(k + out_sizet * 3, i) = wVals[i][k + out_sizet * 2]; // Wc\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"// recurrent weights\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::setUVals(const std::vector<std::vector<T>>& uVals)\n"
"{\n"
"    int col;\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        col = i + in_sizet;\n"
"        for(int k = 0; k < out_size; ++k)\n"
"        {\n"
"            combinedWeights(k, col) = uVals[i][k + out_sizet]; // Uf\n"
"            combinedWeights(k + out_sizet, col) = uVals[i][k]; // Ui\n"
"            combinedWeights(k + out_sizet * 2, col) = uVals[i][k + out_sizet * 3]; // Uo\n"
"            combinedWeights(k + out_sizet * 3, col) = uVals[i][k + out_sizet * 2]; // Uc\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"// biases\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::setBVals(const std::vector<T>& bVals)\n"
"{\n"
"    int col = in_size + out_size;\n"
"    for(int k = 0; k < out_size; ++k)\n"
"    {\n"
"        combinedWeights(k, col) = bVals[k + out_sizet]; // Bf\n"
"        combinedWeights(k + out_sizet, col) = bVals[k]; // Bi\n"
"        combinedWeights(k + out_sizet * 2, col) = bVals[k + out_sizet * 3]; // Bo\n"
"        combinedWeights(k + out_sizet * 3, col) = bVals[k + out_sizet * 2]; // Bc\n"
"    }\n"
"}\n"
"\n"
"} // namespace RTNEURAL_NAMESPACE\n";

const char* lstm_eigen_tpp = (const char*) temp_binary_data_54;

//================== lstm_xsimd.tpp ==================
static const unsigned char temp_binary_data_55[] =
"#include \"lstm_xsimd.h\"\n"
"\n"
"namespace RTNEURAL_NAMESPACE\n"
"{\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"LSTMLayer<T, MathsProvider>::LSTMLayer(int in_size, int out_size)\n"
"    : Layer<T>(in_size, out_size)\n"
"    , fWeights(in_size, out_size)\n"
"    , iWeights(in_size, out_size)\n"
"    , oWeights(in_size, out_size)\n"
"    , cWeights(in_size, out_size)\n"
"{\n"
"    ht1.resize(out_size, (T)0);\n"
"    ct1.resize(out_size, (T)0);\n"
"\n"
"    fVec.resize(out_size, (T)0);\n"
"    iVec.resize(out_size, (T)0);\n"
"    oVec.resize(out_size, (T)0);\n"
"    ctVec.resize(out_size, (T)0);\n"
"    cVec.resize(out_size, (T)0);\n"
"\n"
"    prod_in.resize(in_size, (T)0);\n"
"    prod_out.resize(out_size, (T)0);\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"LSTMLayer<T, MathsProvider>::LSTMLayer(std::initializer_list<int> sizes)\n"
"    : LSTMLayer<T, MathsProvider>(*sizes.begin(), *(sizes.begin() + 1))\n"
"{\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"LSTMLayer<T, MathsProvider>::LSTMLayer(const LSTMLayer& other)\n"
"    : LSTMLayer<T, MathsProvider>(other.in_size, other.out_size)\n"
"{\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"LSTMLayer<T, MathsProvider>& LSTMLayer<T, MathsProvider>::operator=(const LSTMLayer<T, MathsProvider>& other)\n"
"{\n"
"    return *this = LSTMLayer<T, MathsProvider>(other);\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"LSTMLayer<T, MathsProvider>::~LSTMLayer() = default;\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void LSTMLayer<T, MathsProvider>::reset()\n"
"{\n"
"    std::fill(ht1.begin(), ht1.end(), (T)0);\n"
"    std::fill(ct1.begin(), ct1.end(), (T)0);\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"LSTMLayer<T, MathsProvider>::WeightSet::WeightSet(int in_size, int out_size)\n"
"    : out_size(out_size)\n"
"{\n"
"    W = vec2_type(out_size, vec_type(in_size, (T)0));\n"
"    U = vec2_type(out_size, vec_type(out_size, (T)0));\n"
"    b.resize(out_size, (T)0);\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"LSTMLayer<T, MathsProvider>::WeightSet::~WeightSet() = default;\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void LSTMLayer<T, MathsProvider>::setWVals(const std::vector<std::vector<T>>& wVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::in_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"        {\n"
"            iWeights.W[k][i] = wVals[i][k];\n"
"            fWeights.W[k][i] = wVals[i][k + Layer<T>::out_size];\n"
"            cWeights.W[k][i] = wVals[i][k + Layer<T>::out_size * 2];\n"
"            oWeights.W[k][i] = wVals[i][k + Layer<T>::out_size * 3];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void LSTMLayer<T, MathsProvider>::setUVals(const std::vector<std::vector<T>>& uVals)\n"
"{\n"
"    for(int i = 0; i < Layer<T>::out_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"        {\n"
"            iWeights.U[k][i] = uVals[i][k];\n"
"            fWeights.U[k][i] = uVals[i][k + Layer<T>::out_size];\n"
"            cWeights.U[k][i] = uVals[i][k + Layer<T>::out_size * 2];\n"
"            oWeights.U[k][i] = uVals[i][k + Layer<T>::out_size * 3];\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, typename MathsProvider>\n"
"void LSTMLayer<T, MathsProvider>::setBVals(const std::vector<T>& bVals)\n"
"{\n"
"    for(int k = 0; k < Layer<T>::out_size; ++k)\n"
"    {\n"
"        iWeights.b[k] = bVals[k];\n"
"        fWeights.b[k] = bVals[k + Layer<T>::out_size];\n"
"        cWeights.b[k] = bVals[k + Layer<T>::out_size * 2];\n"
"        oWeights.b[k] = bVals[k + Layer<T>::out_size * 3];\n"
"    }\n"
"}\n"
"\n"
"//====================================================\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::LSTMLayerT()\n"
"{\n"
"    for(int i = 0; i < v_out_size; ++i)\n"
"    {\n"
"        // single-input kernel weights\n"
"        Wf_1[i] = v_type((T)0);\n"
"        Wi_1[i] = v_type((T)0);\n"
"        Wo_1[i] = v_type((T)0);\n"
"        Wc_1[i] = v_type((T)0);\n"
"\n"
"        // biases\n"
"        bf[i] = v_type((T)0);\n"
"        bi[i] = v_type((T)0);\n"
"        bo[i] = v_type((T)0);\n"
"        bc[i] = v_type((T)0);\n"
"\n"
"        // intermediate vars\n"
"        ft[i] = v_type((T)0);\n"
"        it[i] = v_type((T)0);\n"
"        ot[i] = v_type((T)0);\n"
"        ht[i] = v_type((T)0);\n"
"    }\n"
"\n"
"    // kernel weights\n"
"    for(int k = 0; k < in_size; ++k)\n"
"    {\n"
"        for(int i = 0; i < v_out_size; ++i)\n"
"        {\n"
"            Wf[k][i] = v_type((T)0);\n"
"            Wi[k][i] = v_type((T)0);\n"
"            Wo[k][i] = v_type((T)0);\n"
"            Wc[k][i] = v_type((T)0);\n"
"        }\n"
"    }\n"
"\n"
"    // recurrent weights\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < v_out_size; ++k)\n"
"        {\n"
"            Uf[i][k] = v_type((T)0);\n"
"            Ui[i][k] = v_type((T)0);\n"
"            Uo[i][k] = v_type((T)0);\n"
"            Uc[i][k] = v_type((T)0);\n"
"        }\n"
"    }\n"
"\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"template <SampleRateCorrectionMode srCorr>\n"
"std::enable_if_t<srCorr == SampleRateCorrectionMode::NoInterp, void>\n"
"LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::prepare(int delaySamples)\n"
"{\n"
"    delayWriteIdx = delaySamples - 1;\n"
"    ct_delayed.resize(delayWriteIdx + 1, {});\n"
"    outs_delayed.resize(delayWriteIdx + 1, {});\n"
"\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"template <SampleRateCorrectionMode srCorr>\n"
"std::enable_if_t<srCorr == SampleRateCorrectionMode::LinInterp, void>\n"
"LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::prepare(T delaySamples)\n"
"{\n"
"    const auto delayOffFactor = delaySamples - std::floor(delaySamples);\n"
"    delayMult = (T)1 - delayOffFactor;\n"
"    delayPlus1Mult = delayOffFactor;\n"
"\n"
"    delayWriteIdx = (int)std::ceil(delaySamples) - (int)std::ceil(delayOffFactor);\n"
"    ct_delayed.resize(delayWriteIdx + 1, {});\n"
"    outs_delayed.resize(delayWriteIdx + 1, {});\n"
"\n"
"    reset();\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::reset()\n"
"{\n"
"    if constexpr(sampleRateCorr != SampleRateCorrectionMode::None)\n"
"    {\n"
"        for(auto& x : ct_delayed)\n"
"            std::fill(x.begin(), x.end(), v_type {});\n"
"\n"
"        for(auto& x : outs_delayed)\n"
"            std::fill(x.begin(), x.end(), v_type {});\n"
"    }\n"
"\n"
"    // reset output state\n"
"    for(int i = 0; i < v_out_size; ++i)\n"
"    {\n"
"        ct[i] = v_type((T)0);\n"
"        outs[i] = v_type((T)0);\n"
"    }\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::setWVals(const std::vector<std::vector<T>>& wVals)\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < in_size; ++k)\n"
"        {\n"
"            Wi[k][i / v_size] = set_value(Wi[k][i / v_size], i % v_size, wVals[k][i]);\n"
"            Wf[k][i / v_size] = set_value(Wf[k][i / v_size], i % v_size, wVals[k][i + out_size]);\n"
"            Wc[k][i / v_size] = set_value(Wc[k][i / v_size], i % v_size, wVals[k][i + 2 * out_size]);\n"
"            Wo[k][i / v_size] = set_value(Wo[k][i / v_size], i % v_size, wVals[k][i + 3 * out_size]);\n"
"        }\n"
"    }\n"
"\n"
"    for(int j = 0; j < out_size; ++j)\n"
"    {\n"
"        Wi_1[j / v_size] = set_value(Wi_1[j / v_size], j % v_size, wVals[0][j]);\n"
"        Wf_1[j / v_size] = set_value(Wf_1[j / v_size], j % v_size, wVals[0][j + out_size]);\n"
"        Wc_1[j / v_size] = set_value(Wc_1[j / v_size], j % v_size, wVals[0][j + 2 * out_size]);\n"
"        Wo_1[j / v_size] = set_value(Wo_1[j / v_size], j % v_size, wVals[0][j + 3 * out_size]);\n"
"    }\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::setUVals(const std::vector<std::vector<T>>& uVals)\n"
"{\n"
"    for(int i = 0; i < out_size; ++i)\n"
"    {\n"
"        for(int k = 0; k < out_size; ++k)\n"
"        {\n"
"            Ui[k][i / v_size] = set_value(Ui[k][i / v_size], i % v_size, uVals[k][i]);\n"
"            Uf[k][i / v_size] = set_value(Uf[k][i / v_size], i % v_size, uVals[k][i + out_size]);\n"
"            Uc[k][i / v_size] = set_value(Uc[k][i / v_size], i % v_size, uVals[k][i + 2 * out_size]);\n"
"            Uo[k][i / v_size] = set_value(Uo[k][i / v_size], i % v_size, uVals[k][i + 3 * out_size]);\n"
"        }\n"
"    }\n"
"}\n"
"\n"
"template <typename T, int in_sizet, int out_sizet, SampleRateCorrectionMode sampleRateCorr, typename MathsProvider>\n"
"void LSTMLayerT<T, in_sizet, out_sizet, sampleRateCorr, MathsProvider>::setBVals(const std::vector<T>& bVals)\n"
"{\n"
"    for(int k = 0; k < out_size; ++k)\n"
"    {\n"
"        bi[k / v_size] = set_value(bi[k / v_size], k % v_size, bVals[k]);\n"
"        bf[k / v_size] = set_value(bf[k / v_size], k % v_size, bVals[k + out_size]);\n"
"        bc[k / v_size] = set_value(bc[k / v_size], k % v_size, bVals[k + 2 * out_size]);\n"
"        bo[k / v_size] = set_value(bo[k / v_size], k % v_size, bVals[k + 3 * out_size]);\n"
"    }\n"
"}\n"
"\n"
"} // namespace RTNEURAL_NAMESPACE\n";

const char* lstm_xsimd_tpp = (const char*) temp_binary_data_55;

//================== README.md ==================
static const unsigned char temp_binary_data_56[] =
"algorithms.hpp used to exist, and some functions are used some functions\n"
"in RTNeural. Later on, xsimd removed this file from the public API as\n"
"they were alternative implementation for C++17/20 standard APIs.\n"
"We are not replacing those functions to maintain compatibility with\n"
"older C++ versions. Hence importing this file here.\n"
"\n"
"For more details, see: https://github.com/jatinchowdhury18/RTNeural/pull/81\n";

const char* README_md = (const char*) temp_binary_data_56;

//================== CMakeLists.txt ==================
static const unsigned char temp_binary_data_57[] =
"add_library(RTNeural STATIC\r\n"
"    activation/activation.h\r\n"
"    activation/activation_eigen.h\r\n"
"    activation/activation_xsimd.h\r\n"
"    Model.h\r\n"
"    Layer.h\r\n"
"    conv1d/conv1d.h\r\n"
"    conv1d/conv1d.tpp\r\n"
"    conv1d_stateless/conv1d_stateless.h\r\n"
"    conv1d_stateless/conv1d_stateless.tpp\r\n"
"    conv1d_stateless/conv1d_stateless_eigen.h\r\n"
"    conv1d_stateless/conv1d_stateless_eigen.h\r\n"
"    conv2d/conv2d.h\r\n"
"    conv2d/conv2d.tpp\r\n"
"    conv2d/conv2d_eigen.h\r\n"
"    conv2d/conv2d_eigen.tpp\r\n"
"    dense/dense.h\r\n"
"    dense/dense_eigen.h\r\n"
"    dense/dense_xsimd.h\r\n"
"    gru/gru.h\r\n"
"    gru/gru.tpp\r\n"
"    gru/gru_eigen.h\r\n"
"    gru/gru_eigen.tpp\r\n"
"    gru/gru_xsimd.h\r\n"
"    gru/gru_xsimd.tpp\r\n"
"    lstm/lstm.h\r\n"
"    lstm/lstm.tpp\r\n"
"    lstm/lstm_eigen.h\r\n"
"    lstm/lstm_eigen.tpp\r\n"
"    lstm/lstm_xsimd.h\r\n"
"    lstm/lstm_xsimd.tpp\r\n"
"    batchnorm/batchnorm2d.h\r\n"
"    batchnorm/batchnorm2d.tpp\r\n"
"    batchnorm/batchnorm2d_eigen.h\r\n"
"    batchnorm/batchnorm2d_eigen.tpp\r\n"
"    model_loader.h\r\n"
"    RTNeural.h\r\n"
"    RTNeural.cpp\r\n"
")\r\n"
"\r\n"
"set_property(TARGET RTNeural PROPERTY POSITION_INDEPENDENT_CODE ON)\r\n"
"set_target_properties(RTNeural PROPERTIES LINKER_LANGUAGE CXX)\r\n"
"target_include_directories(RTNeural\r\n"
"    PUBLIC\r\n"
"        ../modules/json\r\n"
"    INTERFACE\r\n"
"        ..\r\n"
")\r\n"
"set(RTNEURAL_NAMESPACE \"RTNeural\" CACHE STRING \"Namespace to use for RTNeural code\")\r\n"
"target_compile_definitions(RTNeural\r\n"
"    PUBLIC\r\n"
"        RTNEURAL_NAMESPACE=${RTNEURAL_NAMESPACE}\r\n"
")\r\n"
"\r\n"
"if(RTNEURAL_ENABLE_RADSAN)\r\n"
"    rtneural_radsan_configure(RTNeural)\r\n"
"endif()\r\n";

const char* CMakeLists_txt2 = (const char*) temp_binary_data_57;


const char* getNamedResource (const char* resourceNameUTF8, int& numBytes);
const char* getNamedResource (const char* resourceNameUTF8, int& numBytes)
{
    unsigned int hash = 0;

    if (resourceNameUTF8 != nullptr)
        while (*resourceNameUTF8 != 0)
            hash = 31 * hash + (unsigned int) *resourceNameUTF8++;

    switch (hash)
    {
        case 0x04c98ca4:  numBytes = 56496; return TrsmUnrolls_inc;
        case 0x38715a12:  numBytes = 2116; return AccelerateSupport;
        case 0x71024ebe:  numBytes = 1168; return Cholesky;
        case 0xfe5a710f:  numBytes = 1958; return CholmodSupport;
        case 0x002023bf:  numBytes = 13420; return Core;
        case 0x03edd2ff:  numBytes = 122; return Dense;
        case 0x03fda00c:  numBytes = 35; return Eigen;
        case 0x80f6042e:  numBytes = 1836; return Eigenvalues;
        case 0x71e26c92:  numBytes = 1997; return Geometry;
        case 0xd03464ac:  numBytes = 887; return Householder;
        case 0xcb4ed54e:  numBytes = 2160; return IterativeLinearSolvers;
        case 0x83cc338a:  numBytes = 952; return Jacobi;
        case 0x0844161b:  numBytes = 1429; return KLUSupport;
        case 0x00000989:  numBytes = 1276; return LU;
        case 0x2ed44d09:  numBytes = 1048; return MetisSupport;
        case 0x20e2ff1e:  numBytes = 2510; return OrderingMethods;
        case 0x5790bfcd:  numBytes = 1174; return PardisoSupport;
        case 0xe4d2f3ae:  numBytes = 1809; return PaStiXSupport;
        case 0x00000a21:  numBytes = 1279; return QR;
        case 0xd49890e9:  numBytes = 900; return QtAlignedMalloc;
        case 0x93fa5640:  numBytes = 888; return Sparse;
        case 0x796264fe:  numBytes = 1294; return SparseCholesky;
        case 0xc68559ff:  numBytes = 2270; return SparseCore;
        case 0x7ebdcfc9:  numBytes = 1824; return SparseLU;
        case 0x7ebdd061:  numBytes = 1253; return SparseQR;
        case 0xc18be4b1:  numBytes = 1251; return SPQRSupport;
        case 0x54e6e25d:  numBytes = 855; return StdDeque;
        case 0xf23cb141:  numBytes = 784; return StdList;
        case 0x66a64526:  numBytes = 861; return StdVector;
        case 0x9831890b:  numBytes = 2301; return SuperLUSupport;
        case 0x00014241:  numBytes = 1674; return SVD;
        case 0x302ee166:  numBytes = 2137; return ThreadPool;
        case 0x6ac119e8:  numBytes = 1439; return UmfPackSupport;
        case 0x90e15cf5:  numBytes = 28443; return CMakeLists_txt;
        case 0x30871ec7:  numBytes = 189; return modules_textClipping;
        case 0xbf975fab:  numBytes = 3309; return batchnorm_tpp;
        case 0x876f63d8:  numBytes = 3339; return batchnorm_eigen_tpp;
        case 0x0e35b211:  numBytes = 3353; return batchnorm_xsimd_tpp;
        case 0xe98af79d:  numBytes = 3901; return batchnorm2d_tpp;
        case 0xfdb9be4a:  numBytes = 3999; return batchnorm2d_eigen_tpp;
        case 0x84800c83:  numBytes = 3921; return batchnorm2d_xsimd_tpp;
        case 0x203f0dbc:  numBytes = 4899; return conv1d_tpp;
        case 0x8e668129:  numBytes = 3687; return conv1d_eigen_tpp;
        case 0x152ccf62:  numBytes = 4301; return conv1d_xsimd_tpp;
        case 0x363d53a7:  numBytes = 3748; return conv1d_stateless_tpp;
        case 0x4b356ad4:  numBytes = 3619; return conv1d_stateless_eigen_tpp;
        case 0xd1fbb90d:  numBytes = 3298; return conv1d_stateless_xsimd_tpp;
        case 0x21f3e65b:  numBytes = 3928; return conv2d_tpp;
        case 0x96182688:  numBytes = 3947; return conv2d_eigen_tpp;
        case 0x1cde74c1:  numBytes = 3884; return conv2d_xsimd_tpp;
        case 0x11c7fc9f:  numBytes = 10049; return gru_tpp;
        case 0x2d2b79cc:  numBytes = 7564; return gru_eigen_tpp;
        case 0xb3f1c805:  numBytes = 10528; return gru_xsimd_tpp;
        case 0x628aa6f5:  numBytes = 8300; return lstm_tpp;
        case 0x0d1e8ba2:  numBytes = 7946; return lstm_eigen_tpp;
        case 0x93e4d9db:  numBytes = 8989; return lstm_xsimd_tpp;
        case 0x64791dc8:  numBytes = 405; return README_md;
        case 0x8b4a41dd:  numBytes = 1493; return CMakeLists_txt2;
        default: break;
    }

    numBytes = 0;
    return nullptr;
}

const char* namedResourceList[] =
{
    "TrsmUnrolls_inc",
    "AccelerateSupport",
    "Cholesky",
    "CholmodSupport",
    "Core",
    "Dense",
    "Eigen",
    "Eigenvalues",
    "Geometry",
    "Householder",
    "IterativeLinearSolvers",
    "Jacobi",
    "KLUSupport",
    "LU",
    "MetisSupport",
    "OrderingMethods",
    "PardisoSupport",
    "PaStiXSupport",
    "QR",
    "QtAlignedMalloc",
    "Sparse",
    "SparseCholesky",
    "SparseCore",
    "SparseLU",
    "SparseQR",
    "SPQRSupport",
    "StdDeque",
    "StdList",
    "StdVector",
    "SuperLUSupport",
    "SVD",
    "ThreadPool",
    "UmfPackSupport",
    "CMakeLists_txt",
    "modules_textClipping",
    "batchnorm_tpp",
    "batchnorm_eigen_tpp",
    "batchnorm_xsimd_tpp",
    "batchnorm2d_tpp",
    "batchnorm2d_eigen_tpp",
    "batchnorm2d_xsimd_tpp",
    "conv1d_tpp",
    "conv1d_eigen_tpp",
    "conv1d_xsimd_tpp",
    "conv1d_stateless_tpp",
    "conv1d_stateless_eigen_tpp",
    "conv1d_stateless_xsimd_tpp",
    "conv2d_tpp",
    "conv2d_eigen_tpp",
    "conv2d_xsimd_tpp",
    "gru_tpp",
    "gru_eigen_tpp",
    "gru_xsimd_tpp",
    "lstm_tpp",
    "lstm_eigen_tpp",
    "lstm_xsimd_tpp",
    "README_md",
    "CMakeLists_txt2"
};

const char* originalFilenames[] =
{
    "TrsmUnrolls.inc",
    "AccelerateSupport",
    "Cholesky",
    "CholmodSupport",
    "Core",
    "Dense",
    "Eigen",
    "Eigenvalues",
    "Geometry",
    "Householder",
    "IterativeLinearSolvers",
    "Jacobi",
    "KLUSupport",
    "LU",
    "MetisSupport",
    "OrderingMethods",
    "PardisoSupport",
    "PaStiXSupport",
    "QR",
    "QtAlignedMalloc",
    "Sparse",
    "SparseCholesky",
    "SparseCore",
    "SparseLU",
    "SparseQR",
    "SPQRSupport",
    "StdDeque",
    "StdList",
    "StdVector",
    "SuperLUSupport",
    "SVD",
    "ThreadPool",
    "UmfPackSupport",
    "CMakeLists.txt",
    "modules.textClipping",
    "batchnorm.tpp",
    "batchnorm_eigen.tpp",
    "batchnorm_xsimd.tpp",
    "batchnorm2d.tpp",
    "batchnorm2d_eigen.tpp",
    "batchnorm2d_xsimd.tpp",
    "conv1d.tpp",
    "conv1d_eigen.tpp",
    "conv1d_xsimd.tpp",
    "conv1d_stateless.tpp",
    "conv1d_stateless_eigen.tpp",
    "conv1d_stateless_xsimd.tpp",
    "conv2d.tpp",
    "conv2d_eigen.tpp",
    "conv2d_xsimd.tpp",
    "gru.tpp",
    "gru_eigen.tpp",
    "gru_xsimd.tpp",
    "lstm.tpp",
    "lstm_eigen.tpp",
    "lstm_xsimd.tpp",
    "README.md",
    "CMakeLists.txt"
};

const char* getNamedResourceOriginalFilename (const char* resourceNameUTF8);
const char* getNamedResourceOriginalFilename (const char* resourceNameUTF8)
{
    for (unsigned int i = 0; i < (sizeof (namedResourceList) / sizeof (namedResourceList[0])); ++i)
        if (strcmp (namedResourceList[i], resourceNameUTF8) == 0)
            return originalFilenames[i];

    return nullptr;
}

}
